{
  "hash": "4ecf908c4d2631a17ad0be3cdb0cd4c4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Panoramic Image Stitching\ndescription: Explore the complicacies of handling multi-image stitching\nauthor: [\"Guntas Singh Saran\"]\ndate: \"2024-12-07\"\nimage: \"stitcher.png\"\ncategories: [\"Computer Vision\"]\nbibliography: \"references.bib\"\ncsl: \"ieee.csl\"\ntoc: false\nformat: \n    html: \n        code-fold: show\n        code-summary: \"Show the code\"\n        page-layout: full\n---\n\n- **Final source Code** available at [https://github.com/guntas-13/ES666-Assignment3](https://github.com/guntas-13/ES666-Assignment3).\n- Do checkout the other blog at [Notebook Blog](https://guntas-13.github.io/Blog/posts/stitch/stitch.html).\n- Some inspiration from [https://github.com/CorentinBrtx/image-stitching](https://github.com/CorentinBrtx/image-stitching).\n- Special thanks to [\\@Robohrriday](https://github.com/Robohrriday) for his immense help during various stages of execution of this assignment.\n\n# Introduction\n\nGiven a set of $N$ images $I_1, I_2, \\ldots, I_N$ with some overlap of the actual 3-D scene, taken from a static camera with only a rotational degree of freedom, the objective is to warp and stitch the images to form a **Panoramic-Stitched Image**.\n\n![](./images/Result.png)\n<p style=\"text-align: center; color: #5f9ea0;\">A Panoramic-Stitched Image.</p>\n\n# Homography Estimation\n\n### Scale-Invariant Feature Transform (SIFT)\nThe SIFT algorithm [@SIFTarticle] is one of the commonly used algorithms developed for the purpose of interest point detection and description. It detects keypoints in an image that are scale/shift-invariant and provides a 128-dimension feature description for each of the keypoints detected in an image. A variant of this algorithm is implemented in [OpenCV](https://opencv.org/) as ` cv2.SIFT_create()`\n\n<center>\n![](./images/SIFT.png){width=\"60%\"}\n</center>\n\n<p style=\"text-align: center; color: #5f9ea0;\">SIFT-detected Keypoints.</p>\n\n### Finding the Point Correspondances\nNow given the SIFT keypoints in two images, we need to find the matching keypoints in both the images i.e. to compare every 128-dimension feature vector of one image with every other feature vector of the other image to find the closest match. This is usually done with the help of some approximate KNN algorithm (KD Tree here).\n\nCode for SIFT feature detection and Matching:\n\n::: {#f0013e91 .cell execution_count=1}\n``` {.python .cell-code}\ndef detect_and_match_features(img1, img2):\n    sift = cv2.SIFT_create()\n    \n    keypoints1, descriptors1 = sift.detectAndCompute(img1, None)\n    keypoints2, descriptors2 = sift.detectAndCompute(img2, None)\n\n    FLANN_INDEX_KDTREE = 1\n    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n    search_params = dict(checks=50)\n    flann = cv2.FlannBasedMatcher(index_params, search_params)\n\n    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n    \n    good_matches = []\n    for m, n in matches:\n        if m.distance < 0.75 * n.distance:\n            good_matches.append(m)\n\n    points1 = np.zeros((len(good_matches), 2), dtype=np.float32)\n    points2 = np.zeros((len(good_matches), 2), dtype=np.float32)\n    \n    for i, match in enumerate(good_matches):\n        points1[i, :] = keypoints1[match.queryIdx].pt\n        points2[i, :] = keypoints2[match.trainIdx].pt\n\n    return points1, points2, keypoints1, keypoints2, good_matches\n```\n:::\n\n\n<center>\n![](./images/Match.png){width=\"85%\"}\n</center>\n\n<p style=\"text-align: center; color: #5f9ea0;\">Feature Matching in two Images.</p>\n\n## Robust Estimation of Homography Matrix (RANSAC)\nA **homography matrix** $H$ is a $3 \\times 3$ matrix that relates corresponding points between two images in projective space $\\mathbb{P}^2$. Given a point $\\mathbf{x} = [x, y, 1]^T$ in one image, its corresponding point $\\mathbf{x}' = [x', y', 1]^T$ in the second image can be expressed as:\n$$\n\\mathbf{x}' \\sim H \\mathbf{x},\n$$\n\nwhere $\\sim$ indicates equality up to a scale factor.\n\nThe homography matrix $H$ has **8 degrees of freedom (DOF)** hence **atleast 4 point correspondances** are needed (as 1 point correspondance gives 2 equations), as it has 9 entries but is defined up to a scale factor. It is generally represented as:\n\n$$\nH =\n\\begin{bmatrix}\nh_{11} & h_{12} & h_{13} \\\\\nh_{21} & h_{22} & h_{23} \\\\\nh_{31} & h_{32} & h_{33}\n\\end{bmatrix},\n$$\n\nwhere one of the entries is fixed (e.g., $h_{33} = 1$) to remove the scale ambiguity.\n\nThe estimation of the homography matrix involves solving a linear system of equations derived from point correspondences. For a pair of corresponding points $(x, y)$ and $(x', y')$, the following two equations are obtained:\n$$\nx' = \\frac{h_{11}x + h_{12}y + h_{13}}{h_{31}x + h_{32}y + h_{33}}, \\quad y' = \\frac{h_{21}x + h_{22}y + h_{23}}{h_{31}x + h_{32}y + h_{33}}.\n$$\n\nRewriting these equations in matrix form, they contribute two rows to the system $A \\mathbf{h} = 0$, where $\\mathbf{h}$ is the vectorized homography matrix. The homography is obtained by solving this system using singular value decomposition (SVD).\n\nThe provided code implements a RANSAC-based algorithm to estimate the homography matrix robustly in the presence of outliers. The steps are as follows:\n\n- Randomly select 4 point correspondences to compute a candidate homography $H$ using the $\\texttt{compute\\_homography}$ function.\n- Transform all points from the first image using $H$ and compute the reprojection error with respect to the second image points.\n- Identify inliers as points with a reprojection error below a given threshold.\n- Repeat the process for a fixed number of iterations, retaining the homography with the largest number of inliers.\n- Recompute $H$ using all inliers for the final estimate.\n\nCode for Homography estimation using RANSAC\n\n::: {#1aba3cd0 .cell execution_count=2}\n``` {.python .cell-code}\ndef compute_homography(pts1, pts2):\n    A = []\n    for i in range(len(pts1)):\n        x, y = pts1[i][0], pts1[i][1]\n        x_prime, y_prime = pts2[i][0], pts2[i][1]\n        A.append([-x, -y, -1, 0, 0, 0, x * x_prime, y * x_prime, x_prime])\n        A.append([0, 0, 0, -x, -y, -1, x * y_prime, y * y_prime, y_prime])\n    A = np.array(A)\n    _, _, Vt = np.linalg.svd(A)\n    H = Vt[-1].reshape(3, 3)\n    return H / H[2, 2]\n\ndef compute_homography_ransac(points1, points2, iterations=1000, threshold=5.0):\n    max_inliers = []\n    points1_h = np.hstack([points1, np.ones((points1.shape[0], 1))])\n    points2_h = np.hstack([points2, np.ones((points2.shape[0], 1))])\n\n    for _ in range(iterations):\n        idxs = np.random.choice(len(points1), 4, replace=False)\n        pts1_sample = points1[idxs]\n        pts2_sample = points2[idxs]\n\n        H = compute_homography(pts1_sample, pts2_sample)\n\n        projected_points = (H @ points1_h.T).T\n        projected_points /= projected_points[:, 2:3]\n\n        distances = np.linalg.norm(points2_h[:, :2] - projected_points[:, :2], axis=1)\n        inliers = np.where(distances < threshold)[0]\n\n        if len(inliers) > len(max_inliers):\n            max_inliers = inliers\n            \n    inlier_pts1 = points1[max_inliers]\n    inlier_pts2 = points2[max_inliers]\n    best_H = compute_homography(inlier_pts1, inlier_pts2)\n\n    return best_H, len(max_inliers)\n```\n:::\n\n\n# The Naive Approach *[Theoretically Correct]*\nSource: [First Principles of Computer Vision Channel by Shree K. Nayar](https://youtube.com/playlist?list=PL2zRqk16wsdp8KbDfHKvPYNGF2L-zQASc&si=XrWXLeeuW42d67UD)\n\n<center>\n![](./images/Ref.png){width=\"80%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Computing Homographies of all images with respect tot the reference image.</p>\n\nTo create a seamless panorama, the algorithm selects a **reference image** $I_r$ around which all other images are aligned. The choice of the reference image depends on the total number of images $n$:\n$$\nr = \n\\begin{cases} \n\\frac{n}{2} - 1 & \\text{if } n \\text{ is even,} \\\\ \n\\left\\lfloor \\frac{n}{2} \\right\\rfloor & \\text{if } n \\text{ is odd.}\n\\end{cases}\n$$\n\nFor a set of images $\\{I_0, I_1, \\ldots, I_{n-1}\\}$, the goal is to compute the homography of every image $I_i$ with respect to $I_r$, denoted as $H_{ri}$. Since non-consecutive images may not have sufficient overlap, the algorithm leverages the following principle:\n\n- Compute consecutive homographies $H_{i, i+1}$, representing the transformation from image $I_i$ to $I_{i+1}$.\n- For images to the left of the reference, the homography $H_{ri}$ is computed by chaining transformations:\n    $$\n    H_{ri} = H_{r, r-1} H_{r-1, r-2} \\cdots H_{i+1, i}.\n    $$\n\n- For images to the right of the reference, the homography is computed similarly:\n    $$\n    H_{ri} = H_{r, r+1} H_{r+1, r+2} \\cdots H_{i-1, i}.\n    $$\n\n\n### Canvas Size Calculation\nOnce all the homographies $H_{ri}$ are computed, the transformed corners of each image are calculated using the homographies. Let the corners of an image $I_i$ be represented in homogeneous coordinates as:\n$$\n\\mathbf{c}_i = \n\\begin{bmatrix}\n0 & 0 & 1 \\\\\n0 & h_i - 1 & 1 \\\\\nw_i - 1 & h_i - 1 & 1 \\\\\nw_i - 1 & 0 & 1\n\\end{bmatrix}^T,\n$$\n\nwhere $w_i$ and $h_i$ are the width and height of $I_i$, respectively.\n\nThe transformed corners are given by:\n$$\n\\mathbf{c}_i' = H_{ri} \\mathbf{c}_i.\n$$\n\nSince the transformed corners $\\mathbf{c}_i'$ are also in homogeneous coordinates, they are converted back to Cartesian coordinates by normalizing with the third component:\n$$\n\\mathbf{c}_i' = \n\\begin{bmatrix}\n\\frac{x'}{z'} & \\frac{y'}{z'}\n\\end{bmatrix},\n$$\n\nwhere $\\mathbf{c}_i' = [x', y', z']^T$.\n\nFrom the transformed corners of all images, the global bounds of the panorama are determined:\n$$\n\\texttt{min\\_x} = \\min_i (\\mathbf{c}_i'[:, 0]), \\quad\n\\texttt{max\\_x} = \\max_i (\\mathbf{c}_i'[:, 0]),\n$$\n$$\n\\texttt{min\\_y} = \\min_i (\\mathbf{c}_i'[:, 1]), \\quad\n\\texttt{max\\_y} = \\max_i (\\mathbf{c}_i'[:, 1]).\n$$\nThe final canvas size is then computed as:\n$$\n\\texttt{width} = \\texttt{max\\_x} - \\texttt{min\\_x}, \\quad\n\\texttt{height} = \\texttt{max\\_y} - \\texttt{min\\_y}.\n$$\n\n### Image Warping and Final Stitching\nTo align all images within the same canvas, a **shift matrix** is applied to adjust for the minimum $x$ and $y$ coordinates:\n$$\nS = \n\\begin{bmatrix}\n1 & 0 & -\\texttt{min\\_x} \\\\\n0 & 1 & -\\texttt{min\\_y} \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n$$\n\n<center>\n![](./images/Warp.png){width=\"80%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Warp all images using the homographies wrt reference image.</p>\n\nThe final homography for each image $I_i$ with respect to the canvas is:\n$$\nH_{i, \\text{canvas}} = S H_{ri}.\n$$\n\nEach image is warped onto the canvas using OpenCV's $\\texttt{cv2.warpPerspective}$ function, which performs **inverse mapping**:\n$$\n\\mathbf{p}_{\\text{source}} = H_{i, \\text{canvas}}^{-1} \\mathbf{p}_{\\text{destination}},\n$$\n\nwhere $\\mathbf{p}_{\\text{destination}}$ represents a pixel in the canvas, and $\\mathbf{p}_{\\text{source}}$ is the corresponding pixel in the original image. Since $\\mathbf{p}_{\\text{source}}$ may not lie on integer coordinates, interpolation (e.g., bilinear) is used to compute pixel values.\n\n### Final Panorama Composition\nThe warped images are combined into a single panorama. Two strategies are used:\n\n- **First-over-last blending:** Images are combined in reverse order, with earlier images overwriting the later ones.\n- **Accumulative blending:** All non-zero pixel values from each warped image are combined sequentially.\n\nCode for this simple approach:\n\n::: {#77d33e8e .cell execution_count=3}\n``` {.python .cell-code}\ndef panorama_stitcher(images, useOpenCV = False, first_over_last = True):\n    \"\"\"\n    Args:\n        images (List): List of Images to be stitched together\n    \"\"\"\n    \n    assert len(images) >= 2, \"Number of images should be greater than or equal to 2!\"\n    \n    # suppose the consecutive homography matrices are H01, H12, H23, H34, H45 and the reference images is I2\n    # we need H02, H12, H23, H24, H25 (homographies wrt to the reference image)\n    # H02 = H01 * H12, H24 = H23 * H34, H25 = H24 * H45\n    \n    homographies_wrt_reference = [None] * (len(images) - 1)\n    \n    if len(images) % 2 == 0:\n        reference_idx = (len(images) // 2) - 1\n    else:\n        reference_idx = len(images) // 2\n        \n    print(reference_idx)\n    \n    # Homographies of Consecutive Images\n    homographies = []\n    for i in range(1, reference_idx + 1):\n        H = estimate_homography(images[i - 1], images[i], useOpenCV)\n        homographies.append(H)\n    \n    for i in range(reference_idx, len(images) - 1):\n        H = estimate_homography(images[i + 1], images[i], useOpenCV)\n        homographies.append(H)\n    # print(homographies)\n    \n    homographies_wrt_reference[reference_idx] = homographies[reference_idx]\n    if reference_idx >= 1:\n        homographies_wrt_reference[reference_idx - 1] = homographies[reference_idx - 1]\n    \n    # print(homographies_wrt_reference)\n    for i in range(reference_idx - 2, -1, -1):\n        homographies_wrt_reference[i] = np.dot(homographies[i], homographies_wrt_reference[i + 1])\n    \n    for i in range(reference_idx + 1, len(images) - 1):\n        homographies_wrt_reference[i] = np.dot(homographies_wrt_reference[i - 1], homographies[i])\n    \n    # homographies_wrt_reference = [H02, H12, H23, H24, H25] same as the indices of images by inserting a None at the reference index\n    homographies_wrt_reference.insert(reference_idx, None)\n    # now homographies_wrt_reference = [H02, H12, None, H23, H24, H25]\n    # print(homographies_wrt_reference)\n    \n    # computing the transformed corners of all the images\n    min_xs, max_xs, min_ys, max_ys = np.inf, -np.inf, np.inf, -np.inf\n    for i in range(len(images)):\n        if i == reference_idx:\n            continue\n        dest_corners, min_x, min_y, max_x, max_y = transform_corners(images[i], homographies_wrt_reference[i])\n        min_xs = min(min_xs, min_x)\n        max_xs = max(max_xs, max_x)\n        min_ys = min(min_ys, min_y)\n        max_ys = max(max_ys, max_y)\n        print(dest_corners)\n        print(min_x, min_y, max_x, max_y)\n        print(\"=============================\\n\")\n    print(max_xs, min_xs, max_ys, min_ys)\n    \n    # final canvas size\n    final_canvas_width = int(max_xs - min_xs)\n    final_canvas_height = int(max_ys - min_ys)\n    print(f\"(Width, Height) = ({final_canvas_width}, {final_canvas_height})\")\n    \n    # now we need to shift all the images taking into account the min_x and min_y\n    shift_matrix = np.array([[1, 0, -min_xs], [0, 1, -min_ys], [0, 0, 1]])\n    \n    final_homographies_wrt_reference = []\n    for i in range(len(images)):\n        if i == reference_idx:\n            final_homographies_wrt_reference.append(shift_matrix)\n        else:\n            final_homographies_wrt_reference.append(np.dot(shift_matrix, homographies_wrt_reference[i]))\n    \n    visualize_homography_corners(images, final_homographies_wrt_reference, final_canvas_width, final_canvas_height)\n    \n    warped_images = []\n    \n    for i in range(len(images)):\n        warped_images.append(cv2.warpPerspective(images[i], final_homographies_wrt_reference[i], (final_canvas_width, final_canvas_height)))\n    \n    # plotting individual warped images\n    for i in range(len(warped_images)):\n        plt.figure(figsize=(15, 15))\n        plt.title(f\"Image {i + 1}\")\n        plt.imshow(cv2.cvtColor(warped_images[i], cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        plt.show()\n    \n    if first_over_last:\n        final_image = warped_images[-1].copy()\n        for i in range(len(images) - 2, -1, -1):\n            non_zero_indices = np.nonzero(warped_images[i])\n            final_image[non_zero_indices] = warped_images[i][non_zero_indices]\n    else:\n        final_image = np.zeros_like(warped_images[0])\n        for i in range(len(images)):\n            non_zero_indices = np.nonzero(warped_images[i])\n            final_image[non_zero_indices] = warped_images[i][non_zero_indices]\n    \n    plt.figure(figsize=(15, 15))\n    plt.title(\"Final Image\")\n    plt.imshow(cv2.cvtColor(final_image, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.show()\n    \n    return warped_images\n```\n:::\n\n\nOutput:\n\n::: {#6d17e7ea .cell execution_count=4}\n``` {.python .cell-code code-summary=\"Show the output\"}\n# 2\n# [[-5008.472     -611.777   ]\n#  [-5937.4375    3009.8218  ]\n#  [ -587.5613    2644.8523  ]\n#  [ -347.63306     26.290306]]\n# -5937.4375 -611.777 -347.63306 3009.8218\n# =============================\n\n# [[-1964.2637   -226.8197 ]\n#  [-2486.629    2658.986  ]\n#  [ 1540.8529   2553.5437 ]\n#  [ 1683.7982     48.38358]]\n# -2486.629 -226.8197 1683.7982 2658.986\n# =============================\n\n# [[1789.1836     65.615555]\n#  [1860.5791   2376.226   ]\n#  [5154.1025   2485.239   ]\n#  [5083.1216    -43.93821 ]]\n# 1789.1836 -43.93821 5154.1025 2485.239\n# =============================\n\n# [[3141.279      80.706474]\n#  [3192.036    2416.6748  ]\n#  [6938.0474   2743.146   ]\n#  [6869.2407    -80.182846]]\n# 3141.279 -80.182846 6938.0474 2743.146\n# =============================\n\n# [[ 4988.6816      29.972841]\n#  [ 5091.458     2548.468   ]\n#  [10146.665     3168.1335  ]\n#  [ 9960.408     -373.7504  ]]\n# 4988.6816 -373.7504 10146.665 3168.1335\n# =============================\n\n# 10146.665 -5937.4375 3168.1335 -611.777\n# (Width, Height) = (16084, 3779)\n```\n:::\n\n\n![](./images/1.png)\n![](./images/2.png)\n![](./images/3.png)\n![](./images/4.png)\n![](./images/5.png)\n![](./images/6.png)\n![](./images/7.png)\n![](./images/8.png)\n\nBut here's the catch - this approach works just fine considering the camera is **not translated**. Notice the inherent digital lens problem that cause a **brightness change** for the side edges of the images, indicated by the **crease line** between the images. Further all the **projections are independent** of their neighbouring images and only based on the reference image, hence it may not happen that consecutive images, which had a significant overlap, will actually blend well, as seen above. And, the placing of images just above/under - clearly, **overwriting the pixels** of the other image, is not ideal.\n\n# The Robust Approach\nSince, we have noticed the theoretically correct approach **does not give seamless blend**, hence we levarage the **good overlap** of consecutive images to enhance the blend. <br>\nIn this approach, we enhance the blending of consecutive images by leveraging a **distance transform-based weighting**. This ensures smooth transitions in overlapping regions, avoiding visible seams and artifacts. Below is the step-by-step methodology.\n\n### Weight Matrix Construction\nFor a single image, a 2D weight matrix is constructed based on a distance transform. The weights are higher near the center and taper off towards the edges. For an image of size $h \\times w$, the weight matrix is given by:\n\n$$\nW(i, j) = \\frac{d(i, j)}{\\max_{i,j} d(i, j)},\n$$\n\nwhere $d(i, j)$ represents the Euclidean distance from the pixel $(i, j)$ to the nearest edge. This is computed using:\n\n$$\nd(i, j) = \\min(\\text{dist}(i, 0), \\text{dist}(i, h-1), \\text{dist}(j, 0), \\text{dist}(j, w-1)).\n$$\n\n\nThe weights are then normalized across the entire matrix to ensure smooth blending.\n\n<center>\n![](./images/Im2Dist.png){width=\"60%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">A variant of a simple Distance Transform.</p>\n\nCode for a simple distance transform:\n\n::: {#c246309e .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\n\ndef single_weights_array(size: int) -> np.ndarray:\n    \"\"\"\n    Create a 1D weights array.\n\n    Args:\n        size: Size of the array\n\n    Returns:\n        weights: 1D weights array\n    \"\"\"\n    if size % 2 == 1:\n        return np.concatenate(\n            [np.linspace(0, 1, (size + 1) // 2), np.linspace(1, 0, (size + 1) // 2)[1:]]\n        )\n    else:\n        return np.concatenate([np.linspace(0, 1, size // 2), np.linspace(1, 0, size // 2)])\n\n\ndef single_weights_matrix(shape: tuple[int]) -> np.ndarray:\n    \"\"\"\n    Create a 2D weights matrix.\n\n    Args:\n        shape: Shape of the matrix\n\n    Returns:\n        weights: 2D weights matrix\n    \"\"\"\n    return (\n        single_weights_array(shape[0])[:, np.newaxis]\n        @ single_weights_array(shape[1])[:, np.newaxis].T\n    )\n```\n:::\n\n\n### Blending Consecutive Images\nLet $I_1$ and $I_2$ be two consecutive images. Their homography matrix $H_{12}$ is computed such that:\n\n$$\nH_{12} \\mathbf{p}_1 = \\mathbf{p}_2,\n$$\n\nwhere $\\mathbf{p}_1$ and $\\mathbf{p}_2$ are corresponding points in $I_1$ and $I_2$, respectively. \n\n### Steps:\n1. **Warping Images**:\n   - Using the computed homography $H_{12}$, $I_1$ is warped onto the coordinate space of $I_2$:\n   \n   $$\n   I_1' = \\text{warp}(I_1, H_{12}),\n   $$\n\n   where the warp operation involves **inverse mapping** to find the source pixel for each destination pixel and interpolating pixel values.\n\n   Similarly, $I_2$ is translated using a shift matrix $S$ derived from the minimum x and y coordinates of the transformed corners.\n\n2. **Warping Weights**:\n   - The weight matrices for $I_1$ and $I_2$, denoted as $W_1$ and $W_2$, are also warped using the same transformations.\n\n3. **Blending**:\n   - The total weight at each pixel is computed as:\n   \n   $$\n   W_{\\text{total}}(i, j) = W_1'(i, j) + W_2'(i, j).\n   $$\n\n   - The normalized weights are:\n   \n   $$\n   \\hat{W}_1'(i, j) = \\frac{W_1'(i, j)}{W_{\\text{total}}(i, j)}, \\quad \n   \\hat{W}_2'(i, j) = \\frac{W_2'(i, j)}{W_{\\text{total}}(i, j)}.\n   $$\n\n   - The blended image is then obtained as:\n   \n   $$\n   I_{\\text{blend}}(i, j) = \\hat{W}_1'(i, j) I_1'(i, j) + \\hat{W}_2'(i, j) I_2'(i, j).\n   $$\n\n\n### Iterative Blending of Images\nThe way we will approach is from both sides - keeping the reference images still as the middle one.\n\n1. Start in **Forward Direction (Left to Right)**, take Image1 and Image2. Keeping Image2 as the reference, warp Image1 and translate Image2 so that they overlap.\n\n<center>\n![](./images/Im2Wrp.png){width=\"60%\"}\n</center>\n<center>\n![](./images/Im1Tr.png){width=\"60%\"}\n</center>\n\n2. Compute their distance transforms too and apply the same homography and shift matrices to the respective ones.\n\n<center>\n![](./images/Im2Dist.png){width=\"60%\"}\n</center>\n<center>\n![](./images/Im1Dist.png){width=\"60%\"}\n</center>\n\n3. Blend the two images by adding them with their respective distance transforms.\n\n<center>\n![](./images/Blend.png){width=\"60%\"}\n</center>\n\n4. We call this blended image as $\\texttt{Image12}$ and we will also add and normalise their **combined ditance transform**.\n\n<center>\n![](./images/DistComb.png){width=\"60%\"}\n</center>\n\n5. Likewise we will now take this $\\texttt{Image12}$ and $\\texttt{Image3}$, with $\\texttt{Image3}$ as the reference. Notice how we will warp the entire blended $\\texttt{Image12}$ using the Homography $H_{23}$ between $\\texttt{Image2}$ and $\\texttt{Image3}$. Doing the same for their distance transforms too and saving the final distance transform too.\n\n<center>\n![](./images/Im12Wrp.png){width=\"60%\"}\n</center>\n<center>\n![](./images/Im3.png){width=\"60%\"}\n</center>\n<center>\n![](./images/Dist3.png){width=\"60%\"}\n</center>\n<center>\n![](./images/Dist2.png){width=\"60%\"}\n</center>\n<center>\n![](./images/CombineAll.png){width=\"60%\"}\n</center>\n<center>\n![](./images/Dist123.png){width=\"60%\"}\n</center>\n\n1. Repeat the same process but in the **Backward Direction (Right to Left)** and reverse order - $\\texttt{Image6}$ and $\\texttt{Image5}$ $\\to$ $\\texttt{Image65}$, and then $\\texttt{Image65}$ and $\\texttt{Image4}$ $\\to$ $\\texttt{Image654}$.\n\n<center>\n![](./images/Combine654.png){width=\"70%\"}\n</center>\n\n1. Now we need to blend these two sides - the Forward and Backward Segments - again use the Homography $H_{34}$ i.e between $\\texttt{Image3}$ and $\\texttt{Image4}$ to warp/translate the entire blended segments $\\texttt{Image123}$ and $\\texttt{Image654}$.\n\n![](./images/FinForward.png)\n![](./images/FinBackward.png)\n![](./images/DistForward.png)\n![](./images/DistBackward.png)\n\n8. Finally, we get the seamless image - clearly the **ghosting effect** shows that images were **actually not taken from a perfect rotating camera** and there was **some translation** introduced - further the **subjects moved** in the meanwhile of taking those pictures hence we simply shouldn't overwrite one or the other.\n\n![](./images/Result.png)\n\n# A Notorius Case - *Big Panoramas*\nSource: [Ancient Secrets of Computer Vision Channel](https://www.youtube.com/watch?v=taty6lPVcmA)\n\n### Problem with Planes :(\nNotice that if the images are too wide in their scene, then their **projection onto a plane** will be highly elongated\n\n<center>\n![](./images/Plane.png){width=\"60%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Problem with Projecting onto a Plane.</p>\n<center>\n![](./images/Skew1.png){width=\"80%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Wide Panoramas in action (Visualization).</p>\n<center>\n![](./images/skew2.png){width=\"80%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Wide Panoramas in action.</p>\n\n### Use Cylinders!\nRather than a flat plane - the idea is to **project the images onto a cylinder** so that the span of each image is roughly the same\n\n<center>\n![](./images/Cylinder.png){width=\"60%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Projecting onto a cylindrical surface ensures all projections of similar span.</p>\n<center>\n![](./images/CylinderProject.png){width=\"60%\"}\n</center>\n<p style=\"text-align: center; color: #5f9ea0;\">Cylindrical Projection.</p>\n\n::: {#96fcd33b .cell execution_count=6}\n``` {.python .cell-code}\ndef cylindrical_warp(image, focal_length):\n    h, w = image.shape[:2]\n    x_c, y_c = w // 2, h // 2\n\n    u, v = np.meshgrid(np.arange(w), np.arange(h))\n\n    theta = (u - x_c) / focal_length\n    h_cyl = (v - y_c) / focal_length\n\n    x_hat = np.sin(theta)\n    y_hat = h_cyl\n    z_hat = np.cos(theta)\n\n    x_img = (focal_length * x_hat / z_hat + x_c).astype(np.int32)\n    y_img = (focal_length * y_hat / z_hat + y_c).astype(np.int32)\n\n    valid_mask = (x_img >= 0) & (x_img < w) & (y_img >= 0) & (y_img < h)\n\n    cylindrical_img = np.zeros_like(image)\n    cylindrical_img[v[valid_mask], u[valid_mask]] = image[y_img[valid_mask], x_img[valid_mask]]\n\n    cylindrical_img = Image.fromarray(cylindrical_img)\n    cylindrical_img = cylindrical_img.crop((u[valid_mask].min(), v[valid_mask].min(), u[valid_mask].max(), v[valid_mask].max()))\n    cylindrical_img = np.array(cylindrical_img)\n\n    return cylindrical_img\n```\n:::\n\n\n![](./images/Cyl1.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Image Projections with f = 400.</p>\n![](./images/Cyl2.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Image Projections with f = 800.</p>\n\nAll you now need to do is this pre-processing before our usual algorithm. Note that all panoramic set of images **might not require cylindrical warping**. There are ways in which OpenCV's implementation handles the calculation of **focal length** (and that too estimating the **pixel density** since actual focal length is in **mm** and not **pixels**). For this implementation, I had **heuristically figured** out the focal length (although you can look at the image properties in your file manager - that might reveal the camera details that took that picture) and that too only for two set of panoramic images - rest worked just fine without it.\n\n![](./images/Result2.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Final Panoramic stitching using f = 800.</p>\n![](./images/Result3.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Final Panoramic stitching using f = 800.</p>\n\n# Final Results\n\n![](./images/Result.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Panorama 1.</p>\n![](./images/Result2.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Panorama 2 with f = 800.</p>\n![](./images/Result3.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Panorama 3 with f = 800.</p>\n![](./images/Result4.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Panorama 4.</p>\n![](./images/Result5.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Panorama 5.</p>\n![](./images/Result6.png)\n<p style=\"text-align: center; color: #5f9ea0;\">Panorama 6.</p>\n\nHope you had a great time learning and have an even time implementing it on your own!\n\n",
    "supporting": [
      "stitch_files"
    ],
    "filters": [],
    "includes": {}
  }
}