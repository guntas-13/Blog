[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Here’s the compilation of the blog posts written by me in the interest of exploring or learning new concepts!"
  },
  {
    "objectID": "posts/Convolution/ML_Scratch_Convolution.html",
    "href": "posts/Convolution/ML_Scratch_Convolution.html",
    "title": "Convolution operations built from scratch",
    "section": "",
    "text": "Convolution\nFor two functions \\(f(x)\\) and \\(g(x)\\), the convolution function \\(f(x) * g(x)\\) is defined as:\n\\[\\begin{equation}\n(f * g) (t) = \\int_{-\\infty}^{\\infty} f(τ) ⋅ g(t - τ) dτ\n\\end{equation}\\]\nfor discrete samples that we deal with: \\[\\begin{equation}\ny[n] = f[n] * g[n] = \\sum_{k = -∞}^{∞} f[k] ⋅ g[n - k]\n\\end{equation}\\]\nif \\(f\\) has \\(N\\) samples and \\(g\\) has \\(M\\) samples, then the convolved function has \\(N + M - 1\\) samples. A basic rule: “flip any one of the functions, overlap it with the stationary one, multiply and add, and then traverse over.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef triangle(x):\n  if x &gt;= 0 and x &lt;= 5:\n    return 5 - x\n  elif x &lt; 0 and x &gt;= -5:\n    return 5 + x\n  else:\n    return 0\n\ndef rect(x):\n  if -5 &lt;= x &lt;= 5:\n    return 5\n  return 0\n\ndef signal(f, x):\n  return [f(i) for i in x]\n\n\nx = np.arange(-8, 8, 0.1)\nf_x = signal(rect, x)\nplt.plot(x, f_x, color = \"black\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\\(O(N^2)\\) complexity algorithm total \\((N + M - 1) \\cdot M\\) or \\((N + M - 1) \\cdot N\\) computations\n\nA_signal, B_kernel = signal(rect, x), signal(rect, x)\nN_signal, M_kernel = len(A_signal), len(B_kernel)\nY = np.zeros(N_signal + M_kernel - 1)\nfor i in range(len(Y)):\n  for j in range(M_kernel):\n    if i - j &gt;= 0 and i - j &lt; N_signal:\n      Y[i] += A_signal[i - j] * B_kernel[j]\n\n\nConvolution of two rect(\\(x\\))\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"green\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"green\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nprint(f\"Signal Length: {N_signal}, Convolved Signal Lenght: {len(Y)}\")\nplt.plot(Y, color = \"red\")\nplt.xlabel(\"Index Number (k)\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignal Length: 160, Convolved Signal Lenght: 319\n\n\n\n\n\n\n\n\n\n\nA_signal, B_kernel = signal(triangle, x), signal(triangle, x)\nN_signal, M_kernel = len(A_signal), len(B_kernel)\nY = np.zeros(N_signal + M_kernel - 1)\nfor i in range(len(Y)):\n  for j in range(M_kernel):\n    if i - j &gt;= 0 and i - j &lt; N_signal:\n      Y[i] += A_signal[i - j] * B_kernel[j]\n\n\n\nConvolution of two triangle(\\(x\\))\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"cyan\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"cyan\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nprint(f\"Signal Length: {N_signal}, Convolved Signal Lenght: {len(Y)}\")\nplt.plot(Y, color = \"blue\")\nplt.xlabel(\"Index Number (k)\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignal Length: 160, Convolved Signal Lenght: 319"
  },
  {
    "objectID": "posts/Model-Deployment-Raw/FinalTrainingTry2.html",
    "href": "posts/Model-Deployment-Raw/FinalTrainingTry2.html",
    "title": "Model Deployment on Raw Time Series Data",
    "section": "",
    "text": "Yet to decide on Train-Val check or Inner-Outer Fold check\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom latex import latexify, format_axes\nimport numpy as np\nimport tsfel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom MakeDataset import *\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\nTraining data shape:  (108, 500, 3)\nTesting data shape:  (36, 500, 3)\nValidation data shape:  (36, 500, 3)\n\n\n\nX_train, y_train\nX_test, y_test\nX_val, y_val\n\n(180, 500, 3)\n\n\n\n\\((a_x^2 + a_y^2 + a_z^2)\\)\n\nX_train_TS = np.sum(np.square(X_train), axis = -1)\nX_test_TS = np.sum(np.square(X_test), axis = -1)\nX_val_TS = np.sum(np.square(X_val), axis = -1)\nprint(X_train_TS.shape, X_test_TS.shape, X_val_TS.shape)\n\n(108, 500) (36, 500) (36, 500)\n\n\n\nclassesN = {1 : 'WALKING', 2 : 'WALKING_UPSTAIRS', 3 : 'WALKING_DOWNSTAIRS', 4 : 'SITTING', 5 : 'STANDING', 6 : 'LAYING'}\nnamedLabel = [classesN[i] for i in y_train]\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\nhyperparams = {\"max_depth\" : [2, 3, 4, 5, 6, 7, 8, 9, 10], \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\nhyperparams\n\n{'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n 'criterion': ['gini', 'entropy'],\n 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n\n\n\nfrom itertools import product\nfinal, counter = {}, 0\nfor max_depth, criteria, min_sample in product(hyperparams[\"max_depth\"], hyperparams[\"criterion\"], hyperparams[\"min_samples_leaf\"]):\n    model = DecisionTreeClassifier(max_depth = max_depth, criterion = criteria, min_samples_leaf = min_sample, random_state = 42)\n    model.fit(X_train_TS, y_train)\n    val_score = model.score(X_val_TS, y_val)\n    final[counter] = {\"max_depth\" : max_depth, \"criterion\" : criteria, \"min_samples_leaf\" : min_sample, \"val_score\" : val_score}\n    counter += 1\n\n\nhparam_df = pd.DataFrame(final).T\nhparam_df\n\n\n\n\n\n\n\n\nmax_depth\ncriterion\nmin_samples_leaf\nval_score\n\n\n\n\n0\n2\ngini\n1\n0.472222\n\n\n1\n2\ngini\n2\n0.472222\n\n\n2\n2\ngini\n3\n0.472222\n\n\n3\n2\ngini\n4\n0.472222\n\n\n4\n2\ngini\n5\n0.472222\n\n\n...\n...\n...\n...\n...\n\n\n265\n10\nentropy\n11\n0.527778\n\n\n266\n10\nentropy\n12\n0.527778\n\n\n267\n10\nentropy\n13\n0.527778\n\n\n268\n10\nentropy\n14\n0.444444\n\n\n269\n10\nentropy\n15\n0.472222\n\n\n\n\n270 rows × 4 columns\n\n\n\n\nhparam_df.sort_values(by = \"val_score\", ascending = False).head(10)\n\n\n\n\n\n\n\n\nmax_depth\ncriterion\nmin_samples_leaf\nval_score\n\n\n\n\n144\n6\nentropy\n10\n0.583333\n\n\n53\n3\nentropy\n9\n0.583333\n\n\n204\n8\nentropy\n10\n0.583333\n\n\n174\n7\nentropy\n10\n0.583333\n\n\n114\n5\nentropy\n10\n0.583333\n\n\n50\n3\nentropy\n6\n0.583333\n\n\n54\n3\nentropy\n10\n0.583333\n\n\n84\n4\nentropy\n10\n0.583333\n\n\n234\n9\nentropy\n10\n0.583333\n\n\n52\n3\nentropy\n8\n0.583333\n\n\n\n\n\n\n\n\ndfTrain_Val_Test = np.vstack([X_train_TS, X_val_TS, X_test_TS])\ny_train_test_val = np.hstack([y_train, y_val, y_test])\ndfTrain_Val_Test = pd.DataFrame(dfTrain_Val_Test)\ndfTrain_Val_Test\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n\n\n\n\n0\n1.056837\n1.055002\n1.055806\n1.056825\n1.056743\n1.058030\n1.059746\n1.056402\n1.051561\n1.051040\n...\n1.059888\n1.052544\n1.056687\n1.060374\n1.060270\n1.057576\n1.050376\n1.052854\n1.056003\n1.050580\n\n\n1\n1.083240\n1.076504\n1.071849\n1.070542\n1.073735\n1.069331\n1.065576\n1.070615\n1.073486\n1.074425\n...\n1.076160\n1.072783\n1.070026\n1.066329\n1.064303\n1.069655\n1.073976\n1.075890\n1.078382\n1.072455\n\n\n2\n1.138189\n1.118926\n1.010193\n0.908460\n0.877500\n0.799665\n0.755336\n0.604213\n0.398809\n0.387867\n...\n1.131734\n1.211883\n1.395558\n1.574451\n1.786266\n2.000218\n2.163595\n2.539505\n2.744447\n2.195609\n\n\n3\n1.181108\n1.152283\n1.143152\n1.270364\n1.238777\n1.149924\n1.015107\n0.984543\n1.273980\n1.684522\n...\n0.621903\n1.029622\n1.784374\n2.366215\n2.621218\n2.250886\n1.741832\n1.685947\n1.807674\n1.804153\n\n\n4\n1.011227\n1.017584\n1.013233\n1.011926\n1.009752\n1.005219\n1.001461\n1.005883\n1.007562\n1.007073\n...\n1.009191\n1.006528\n1.004264\n1.003962\n1.007311\n1.005560\n0.999966\n0.998143\n1.002371\n1.010588\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n175\n1.012265\n1.010108\n1.011981\n1.011857\n1.013834\n1.014795\n1.014063\n1.014580\n1.009580\n1.004343\n...\n1.010006\n1.004767\n1.001302\n1.003619\n1.011636\n1.014957\n1.012647\n1.015879\n1.018396\n1.015794\n\n\n176\n1.038929\n1.034965\n1.030862\n1.029954\n1.026855\n1.026849\n1.027156\n1.026479\n1.032641\n1.032752\n...\n1.028253\n1.032168\n1.035130\n1.032640\n1.030046\n1.031069\n1.033694\n1.031294\n1.022681\n1.019043\n\n\n177\n1.269837\n1.462317\n1.900056\n1.875284\n1.269108\n0.712168\n0.530259\n0.568774\n0.467038\n0.432422\n...\n0.297068\n0.393685\n0.297214\n0.255879\n0.319893\n0.316965\n0.505662\n0.799774\n1.057363\n1.227443\n\n\n178\n1.170950\n1.442350\n2.105890\n2.921814\n3.152692\n3.007977\n2.318748\n1.582135\n1.562915\n1.680339\n...\n0.783872\n1.191452\n1.071581\n1.027061\n1.272170\n1.171053\n0.952702\n0.696993\n0.730763\n0.944393\n\n\n179\n2.437082\n2.287970\n1.472244\n0.668358\n0.561077\n0.932713\n1.170589\n1.235397\n1.133492\n0.844032\n...\n1.284310\n1.342955\n1.152644\n1.066167\n0.971577\n0.797153\n0.791073\n0.748641\n0.629820\n0.552444\n\n\n\n\n180 rows × 500 columns\n\n\n\n\nmodel = DecisionTreeClassifier(max_depth = 6, min_samples_leaf = 10, criterion = \"entropy\", random_state = 42)\nmodel.fit(dfTrain_Val_Test, y_train_test_val)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=10,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=10,\n                       random_state=42)\n\n\n\ndef getTimeSeries(filename):\n    filePath = f\"./Time Series Data/{filename}\"\n    df = pd.read_csv(filePath)\n    return df\n\n\ndf = getTimeSeries('TS2Walking.csv')\ndf\n\n\n\n\n\n\n\n\ntime\ngFx\ngFy\ngFz\n0\n\n\n\n\n0\n0.004371\n-0.9965\n0.1796\n0.2842\n1.068\n\n\n1\n0.005229\n-1.0007\n0.1845\n0.2910\n1.075\n\n\n2\n0.005670\n-1.0034\n0.1886\n0.2964\n1.080\n\n\n3\n0.006074\n-1.0026\n0.1903\n0.2984\n1.080\n\n\n4\n0.006489\n-0.9985\n0.1920\n0.2954\n1.076\n\n\n...\n...\n...\n...\n...\n...\n\n\n19310\n38.514996\n-0.9303\n-0.0344\n0.4249\n1.032\n\n\n19311\n38.516256\n-0.9301\n-0.0349\n0.4234\n1.031\n\n\n19312\n38.518234\n-0.9315\n-0.0347\n0.4222\n1.032\n\n\n19313\n38.520242\n-0.9335\n-0.0354\n0.4229\n1.034\n\n\n19314\n38.522770\n-0.9354\n-0.0364\n0.4251\n1.037\n\n\n\n\n19315 rows × 5 columns\n\n\n\n\ndef fetchTotTS(dataFrame):\n    return pd.DataFrame(dataFrame.iloc[:, 3]**2)\n\n\ndef PlotTimeSeries(df, flag):\n    latexify()\n    if flag:\n        plt.figure(figsize = (9, 3))\n        plt.title(r\"Time Series of Acceleration $(acc_x, acc_y, acc_z)$\")\n        colors = [\"red\", \"green\", \"blue\"]\n        for k in range(1, 4):\n            plt.plot(df.iloc[:, k], color = colors[k - 1], linewidth = 0.8)\n        plt.xlabel(\"Time Samples\")\n        plt.ylabel(r\"Acceleration in $m/s^2$\")\n        plt.legend([r\"$a_x$\", r\"$a_y$\", r\"$a_z$\"])\n        plt.grid()\n        plt.show()\n    else:\n        plt.figure(figsize = (9, 3))\n        plt.title(r\"Time Series of Total Acceleration $(acc_x^2 + acc_y^2 + acc_z^2)$\")\n        plt.plot(df.iloc[:, 3]**2, color = \"deeppink\", linewidth = 0.8)\n        plt.xlabel(\"Time Samples\")\n        plt.ylabel(r\"Total Acceleration in $m/s^2$\")\n        plt.legend([r\"$(acc_x^2 + acc_y^2 + acc_z^2)$\"])\n        plt.grid()\n        plt.show()\n\n\n\n\\(\\text{Sampling Time} = \\frac{\\text{No. of Samples}}{f_s}\\)\n\n\n\\(f_s = 500 Hz\\)\n\ndf.shape[0] / 500.0\n\n38.63\n\n\n\npd.DataFrame(fetchTotTS(df)).T\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n19305\n19306\n19307\n19308\n19309\n19310\n19311\n19312\n19313\n19314\n\n\n\n\n0\n1.140624\n1.155625\n1.1664\n1.1664\n1.157776\n1.147041\n1.138489\n1.125721\n1.115136\n1.096209\n...\n1.083681\n1.079521\n1.077444\n1.073296\n1.069156\n1.065024\n1.062961\n1.065024\n1.069156\n1.075369\n\n\n\n\n1 rows × 19315 columns\n\n\n\n\nPlotTimeSeries(df, 1)\nPlotTimeSeries(df, 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom scipy.signal import resample\n\n\ndfN = pd.DataFrame(resample(df, 500))\nPlotTimeSeries(dfN, 0)\nPlotTimeSeries(dfN, 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# flag = 1 -&gt; Only display the orginal untrimmed TS and trim-prediction on flag != 1\ndef PredictPlot(filename,  flag = 1):\n    df = getTimeSeries(filename)\n    if flag:\n        print(\"Original Time Series\")\n        PlotTimeSeries(df, 1)\n        PlotTimeSeries(df, 0)\n    else:\n        df.drop(columns = [\"time\"], inplace = True)\n        dfN = pd.DataFrame(resample(df, 500))\n        dfN = fetchTotTS(dfN).T\n        y_pred = model.predict(dfN)\n        print(classesN[y_pred[0]])\n\n\nPredictPlot('TS2Walking.csv', 1)\n\nOriginal Time Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS2Walking.csv', 0)\n\nWALKING_DOWNSTAIRS\n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\nPredictPlot('TS4Walking.csv', 1)\nPredictPlot('TS4Walking.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS5WalkingUpstairs.csv', 1)\nPredictPlot('TS5WalkingUpstairs.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS6WalkingDownstairs.csv', 1)\nPredictPlot('TS6WalkingDownstairs.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS7WalkingDownstairs.csv', 1)\nPredictPlot('TS7WalkingDownstairs.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS8WalkingUpstairs.csv', 1)\nPredictPlot('TS8WalkingUpstairs.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS9Sitting.csv', 1)\nPredictPlot('TS9Sitting.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS10Sitting.csv', 1)\nPredictPlot('TS10Sitting.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS11Standing.csv', 1)\nPredictPlot('TS11Standing.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS12Standing.csv', 1)\nPredictPlot('TS12Standing.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS14WalkingUpstairs.csv', 1)\nPredictPlot('TS14WalkingUpstairs.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS15Walking.csv', 1)\nPredictPlot('TS15Walking.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS16Laying.csv', 1)\nPredictPlot('TS16Laying.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS17Sitting.csv', 1)\nPredictPlot('TS17Sitting.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS18Sitting.csv', 1)\nPredictPlot('TS18Sitting.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS19Sitting.csv', 1)\nPredictPlot('TS19Sitting.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS20Sitting.csv', 1)\nPredictPlot('TS20Sitting.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS21Sitting.csv', 1)\nPredictPlot('TS21Sitting.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS22Standing.csv', 1)\nPredictPlot('TS22Standing.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS23Standing.csv', 1)\nPredictPlot('TS23Standing.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS24Standing.csv', 1)\nPredictPlot('TS24Standing.csv', 0)\n\nOriginal Time Series\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS25Standing.csv', 1)\nPredictPlot('TS25Standing.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS26Standing.csv', 1)\nPredictPlot('TS26Standing.csv', 0)\n\nOriginal Time Series\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS27Laying.csv', 1)\nPredictPlot('TS27Laying.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS28Laying.csv', 1)\nPredictPlot('TS28Laying.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS29Laying.csv', 1)\nPredictPlot('TS29Laying.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS30Laying.csv', 1)\nPredictPlot('TS30Laying.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS31Laying.csv', 1)\nPredictPlot('TS31Laying.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS32Walking.csv', 1)\nPredictPlot('TS32Walking.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS33Walking.csv', 1)\nPredictPlot('TS33Walking.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS34Walking.csv', 1)\nPredictPlot('TS34Walking.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS35Walking.csv', 1)\nPredictPlot('TS35Walking.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS36Walking.csv', 1)\nPredictPlot('TS36Walking.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS37Upstairs.csv', 1)\nPredictPlot('TS37Upstairs.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS38Downstairs.csv', 1)\nPredictPlot('TS38Downstairs.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS39Upstairs.csv', 1)\nPredictPlot('TS39Upstairs.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS40Downstairs.csv', 1)\nPredictPlot('TS40Downstairs.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS41Upstairs.csv', 1)\nPredictPlot('TS41Upstairs.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS42Downstairs.csv', 1)\nPredictPlot('TS42Downstairs.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS43Upstairs.csv', 1)\nPredictPlot('TS43Upstairs.csv', 0)\n\nOriginal Time Series\nSTANDING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS44Downstairs.csv', 1)\nPredictPlot('TS44Downstairs.csv', 0)\n\nOriginal Time Series\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS45Upstairs.csv', 1)\nPredictPlot('TS45Upstairs.csv', 0)\n\nOriginal Time Series\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot('TS46Downstairs.csv', 1)\nPredictPlot('TS46Downstairs.csv', 0)\n\nOriginal Time Series\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny = [1, 1, 2, 3, 3, 2, 4, 4, 5, 5, 2 ,1, 6, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\ny_pred = [3, 3, 2, 1, 3, 3, 4, 4, 4, 4, 3, 3, 2, 6, 4, 6, 6, 6, 6, 4, 4, 6, 6, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 3, 2, 1, 1, 2, 5, 2, 1, 3]\n\n\ncm = confusion_matrix(y, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\ndf_cm\n\n\n\n\n\n\n\n\nWALKING\nWALKING_UPSTAIRS\nWALKING_DOWNSTAIRS\nSITTING\nSTANDING\nLAYING\n\n\n\n\nWALKING\n3\n2\n3\n0\n0\n0\n\n\nWALKING_UPSTAIRS\n3\n2\n2\n0\n1\n0\n\n\nWALKING_DOWNSTAIRS\n2\n2\n3\n0\n0\n0\n\n\nSITTING\n0\n0\n0\n3\n0\n4\n\n\nSTANDING\n0\n0\n0\n4\n0\n3\n\n\nLAYING\n0\n6\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nprint(classification_report(y, y_pred, labels = np.unique(y_pred)))\n\n              precision    recall  f1-score   support\n\n           1       0.38      0.38      0.38         8\n           2       0.17      0.25      0.20         8\n           3       0.38      0.43      0.40         7\n           4       0.43      0.43      0.43         7\n           5       0.00      0.00      0.00         7\n           6       0.00      0.00      0.00         6\n\n    accuracy                           0.26        43\n   macro avg       0.22      0.25      0.23        43\nweighted avg       0.23      0.26      0.24        43\n\n\n\n\ndef confMatrix(dataFrame, flag = 1, accuracies = None):\n    if flag:\n        plt.figure(figsize = (6, 6))\n        ax = sns.heatmap(dataFrame, annot = True, cmap = \"PuBu\")\n        plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 8)\n        plt.setp(ax.get_yticklabels(), fontsize = 8)\n        plt.ylabel(\"True label\", fontsize = 18)\n        plt.xlabel(\"Predicted label\", fontsize = 18)\n        plt.title(f\"Accuracy = {accuracy_score(y, y_pred)*100: .4f}%\", fontweight = \"bold\", fontsize = 13)\n        plt.show()\n    else:\n        fig, axes = plt.subplots(3, 3, figsize = (25, 25))\n        axes = axes.flatten()\n\n        for i, df in enumerate(dataFrame):\n            ax = sns.heatmap(df, annot = True, ax = axes[i], cbar = False, cmap = \"PuBu\")\n            \n            plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 6)\n            plt.setp(ax.get_yticklabels(), fontsize = 8)\n            ax.set_title(f\"Depth = {i + 2}\\nAccuracy = {accuracies[i] * 100: .4f}%\", fontsize = 10)\n            ax.set_ylabel(\"True label\", fontsize = 12)\n            ax.set_xlabel(\"Predicted label\", fontsize = 12)\n            \n        plt.delaxes(axes[7])\n        plt.delaxes(axes[8])\n        plt.tight_layout()\n        plt.subplots_adjust(wspace = 1.1, hspace = 1.1)\n        plt.show()\n\n\nconfMatrix(df_cm, 1)\n\n\n\n\n\n\n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}"
  },
  {
    "objectID": "posts/Model-Deployment/FinalTrainingTry1.html",
    "href": "posts/Model-Deployment/FinalTrainingTry1.html",
    "title": "Model Deployment on Feature Extracted Data",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom latex import latexify, format_axes\nimport numpy as np\nimport tsfel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom MakeDataset import *\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\nTraining data shape:  (108, 500, 3)\nTesting data shape:  (36, 500, 3)\nValidation data shape:  (36, 500, 3)\n\n\n\nX_train, y_train\nX_test, y_test\nX_val, y_val\n\n(180, 500, 3)\n\n\n\n\n\nX_train_TS = np.sum(np.square(X_train), axis = -1)\nX_test_TS = np.sum(np.square(X_test), axis = -1)\nX_val_TS = np.sum(np.square(X_val), axis = -1)\nprint(X_train_TS.shape, X_test_TS.shape, X_val_TS.shape)\n\n(108, 500) (36, 500) (36, 500)\n\n\n\nfeatures_sel = [\"0_Mean\", \"0_Variance\", \"0_Peak to peak distance\", \"0_Mean absolute deviation\"]\n\n\nclassesN = {1 : 'WALKING', 2 : 'WALKING_UPSTAIRS', 3 : 'WALKING_DOWNSTAIRS', 4 : 'SITTING', 5 : 'STANDING', 6 : 'LAYING'}\nnamedLabel = [classesN[i] for i in y_train]\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\ndef Featuriser(XTimeSeries, features):\n    cfg = tsfel.get_features_by_domain()\n    df = pd.DataFrame(XTimeSeries)\n    dataFrames = []\n    for i in df.index:\n        dataFrames.append(tsfel.time_series_features_extractor(cfg, df.iloc[i,:], fs = 50))\n    dfN = pd.concat(dataFrames, axis = 0)\n    dfNFeaturized = dfN[features]\n    return dfNFeaturized\n\n\n\n\n\ndfTrain = Featuriser(X_train_TS, features_sel)\ndfTest = Featuriser(X_test_TS, features_sel)\ndfVal = Featuriser(X_val_TS, features_sel)\n\n\ndfTrain.shape\n\n(108, 4)\n\n\n\ndfTest.shape\n\n(36, 4)\n\n\n\ndfVal.shape\n\n(36, 4)\n\n\n\ndfTrain\n\n\n\n\n\n\n\n\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n\n\n\n\n0\n1.058182\n0.000441\n0.276308\n0.010239\n\n\n0\n1.072680\n0.000439\n0.302652\n0.011554\n\n\n0\n1.141142\n0.281282\n2.951101\n0.382585\n\n\n0\n1.193139\n0.442850\n2.853736\n0.537589\n\n\n0\n1.005901\n0.000026\n0.042222\n0.004003\n\n\n...\n...\n...\n...\n...\n\n\n0\n1.328807\n1.370835\n4.655614\n0.995502\n\n\n0\n1.188371\n0.436127\n3.625210\n0.506932\n\n\n0\n1.066069\n0.000026\n0.031092\n0.004027\n\n\n0\n1.116685\n0.245975\n2.492894\n0.382408\n\n\n0\n1.118337\n0.250313\n2.180257\n0.381846\n\n\n\n\n108 rows × 4 columns\n\n\n\n\nhyperparams = {\"max_depth\" : [2, 3, 4, 5, 6, 7, 8, 9, 10], \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\nhyperparams\n\n{'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n 'criterion': ['gini', 'entropy'],\n 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n\n\n\nfrom itertools import product\nfinal, counter = {}, 0\nfor max_depth, criteria, min_sample in product(hyperparams[\"max_depth\"], hyperparams[\"criterion\"], hyperparams[\"min_samples_leaf\"]):\n    model = DecisionTreeClassifier(max_depth = max_depth, criterion = criteria, min_samples_leaf = min_sample, random_state = 42)\n    model.fit(dfTrain, y_train)\n    val_score = model.score(dfVal, y_val)\n    final[counter] = {\"max_depth\" : max_depth, \"criterion\" : criteria, \"min_samples_leaf\" : min_sample, \"val_score\" : val_score}\n    counter += 1\n\n\nhparam_df = pd.DataFrame(final).T\nhparam_df\n\n\n\n\n\n\n\n\nmax_depth\ncriterion\nmin_samples_leaf\nval_score\n\n\n\n\n0\n2\ngini\n1\n0.638889\n\n\n1\n2\ngini\n2\n0.638889\n\n\n2\n2\ngini\n3\n0.638889\n\n\n3\n2\ngini\n4\n0.638889\n\n\n4\n2\ngini\n5\n0.638889\n\n\n...\n...\n...\n...\n...\n\n\n265\n10\nentropy\n11\n0.694444\n\n\n266\n10\nentropy\n12\n0.666667\n\n\n267\n10\nentropy\n13\n0.666667\n\n\n268\n10\nentropy\n14\n0.666667\n\n\n269\n10\nentropy\n15\n0.666667\n\n\n\n\n270 rows × 4 columns\n\n\n\n\nhparam_df.sort_values(by = \"val_score\", ascending = False).head(10)\n\n\n\n\n\n\n\n\nmax_depth\ncriterion\nmin_samples_leaf\nval_score\n\n\n\n\n136\n6\nentropy\n2\n0.777778\n\n\n256\n10\nentropy\n2\n0.722222\n\n\n255\n10\nentropy\n1\n0.722222\n\n\n115\n5\nentropy\n11\n0.694444\n\n\n130\n6\ngini\n11\n0.694444\n\n\n220\n9\ngini\n11\n0.694444\n\n\n241\n10\ngini\n2\n0.694444\n\n\n100\n5\ngini\n11\n0.694444\n\n\n250\n10\ngini\n11\n0.694444\n\n\n190\n8\ngini\n11\n0.694444\n\n\n\n\n\n\n\n\ndfTrain_Val_Test = pd.concat([dfTrain, dfVal, dfTest], axis = 0)\ny_train_test_val = np.hstack([y_train, y_val, y_test])\ndfTrain_Val_Test\n\n\n\n\n\n\n\n\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n\n\n\n\n0\n1.058182\n0.000441\n0.276308\n0.010239\n\n\n0\n1.072680\n0.000439\n0.302652\n0.011554\n\n\n0\n1.141142\n0.281282\n2.951101\n0.382585\n\n\n0\n1.193139\n0.442850\n2.853736\n0.537589\n\n\n0\n1.005901\n0.000026\n0.042222\n0.004003\n\n\n...\n...\n...\n...\n...\n\n\n0\n1.012756\n0.000036\n0.046293\n0.004625\n\n\n0\n1.029612\n0.000035\n0.055289\n0.004476\n\n\n0\n1.221348\n0.614296\n3.526818\n0.662692\n\n\n0\n1.168859\n0.455832\n3.239159\n0.501238\n\n\n0\n1.159747\n0.317845\n2.934196\n0.436656\n\n\n\n\n180 rows × 4 columns\n\n\n\n\nmodel = DecisionTreeClassifier(max_depth = 6, min_samples_leaf = 2, criterion = \"entropy\", random_state = 42)\nmodel.fit(dfTrain_Val_Test, y_train_test_val)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=2,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=2,\n                       random_state=42)\n\n\n\ndef getTimeSeries(filename):\n    filePath = f\"./Time Series Data/{filename}\"\n    df = pd.read_csv(filePath)\n    return df\n\n\ndf = getTimeSeries('TS2Walking.csv')\ndf\n\n\n\n\n\n\n\n\ntime\ngFx\ngFy\ngFz\n0\n\n\n\n\n0\n0.004371\n-0.9965\n0.1796\n0.2842\n1.068\n\n\n1\n0.005229\n-1.0007\n0.1845\n0.2910\n1.075\n\n\n2\n0.005670\n-1.0034\n0.1886\n0.2964\n1.080\n\n\n3\n0.006074\n-1.0026\n0.1903\n0.2984\n1.080\n\n\n4\n0.006489\n-0.9985\n0.1920\n0.2954\n1.076\n\n\n...\n...\n...\n...\n...\n...\n\n\n19310\n38.514996\n-0.9303\n-0.0344\n0.4249\n1.032\n\n\n19311\n38.516256\n-0.9301\n-0.0349\n0.4234\n1.031\n\n\n19312\n38.518234\n-0.9315\n-0.0347\n0.4222\n1.032\n\n\n19313\n38.520242\n-0.9335\n-0.0354\n0.4229\n1.034\n\n\n19314\n38.522770\n-0.9354\n-0.0364\n0.4251\n1.037\n\n\n\n\n19315 rows × 5 columns\n\n\n\n\ndef fetchTotTS(dataFrame):\n    return pd.DataFrame(dataFrame.iloc[:, 4]**2)\n\n\ndef PlotTimeSeries(df, flag):\n    latexify()\n    if flag:\n        plt.figure(figsize = (9, 3))\n        plt.title(r\"Time Series of Acceleration $(acc_x, acc_y, acc_z)$\")\n        colors = [\"red\", \"green\", \"blue\"]\n        for k in range(1, 4):\n            plt.plot(df.iloc[:, k], color = colors[k - 1], linewidth = 0.8)\n        plt.xlabel(\"Time Samples\")\n        plt.ylabel(r\"Acceleration in $m/s^2$\")\n        plt.legend([r\"$a_x$\", r\"$a_y$\", r\"$a_z$\"])\n        plt.grid()\n        plt.show()\n    else:\n        plt.figure(figsize = (9, 3))\n        plt.title(r\"Time Series of Total Acceleration $(acc_x^2 + acc_y^2 + acc_z^2)$\")\n        plt.plot(df.iloc[:, 4]**2, color = \"deeppink\", linewidth = 0.8)\n        plt.xlabel(\"Time Samples\")\n        plt.ylabel(r\"Total Acceleration in $m/s^2$\")\n        plt.legend([r\"$(acc_x^2 + acc_y^2 + acc_z^2)$\"])\n        plt.grid()\n        plt.show()\n\n\n\n\n\n\n\ndf.shape[0] / 500.0\n\n38.63\n\n\n\ndef FeaturiserN(XTimeSeries, features):\n    model1 = tsfel.get_features_by_domain()\n    df = pd.DataFrame(XTimeSeries).T\n    dfN = tsfel.time_series_features_extractor(model1, signal_windows = list(df.iloc[0, :]), fs = 50)\n    dfNFeaturized = dfN[features]\n    return dfNFeaturized\n\n\npd.DataFrame(fetchTotTS(df.iloc[2500:7500, :])).T\n\n\n\n\n\n\n\n\n2500\n2501\n2502\n2503\n2504\n2505\n2506\n2507\n2508\n2509\n...\n7490\n7491\n7492\n7493\n7494\n7495\n7496\n7497\n7498\n7499\n\n\n\n\n0\n0.751689\n0.755161\n0.758641\n0.765625\n0.776161\n0.786769\n0.799236\n0.808201\n0.817216\n0.8281\n...\n1.646089\n1.565001\n1.517824\n1.485961\n1.452025\n1.420864\n1.3924\n1.364224\n1.331716\n1.301881\n\n\n\n\n1 rows × 5000 columns\n\n\n\n\ndfN1 = FeaturiserN(fetchTotTS(df.iloc[2500:7500, :]), features_sel)\ndfN1\n\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\n\n\n\n\n\n\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n\n\n\n\n0\n1.18036\n0.49156\n3.675776\n0.540058\n\n\n\n\n\n\n\n\nPlotTimeSeries(df.iloc[2500:7500,:], 1)\nPlotTimeSeries(df.iloc[2500:7500,:], 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny_pred = model.predict(dfN1)\ny_pred\n\narray([2])\n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\ndf1 = getTimeSeries('TS4Walking.csv')\nDF = df1.iloc[2500:7500, :]\nPlotTimeSeries(DF, 1)\nPlotTimeSeries(DF, 0)\ndfN2 = FeaturiserN(fetchTotTS(DF), features_sel)\ny_pred = model.predict(dfN2)\nclassesN[y_pred[0]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n'WALKING'\n\n\n\ndf2 = getTimeSeries('TS10Sitting.csv')\nDF = df2.iloc[2500:7500, :]\nPlotTimeSeries(DF, 1)\nPlotTimeSeries(DF, 0)\ndfN3 = FeaturiserN(fetchTotTS(DF), features_sel)\ny_pred = model.predict(dfN3)\nclassesN[y_pred[0]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n'SITTING'\n\n\n\n# flag = 1 -&gt; Only display the orginal untrimmed TS and trim-prediction on flag != 1\ndef PredictPlot(filename,  flag = 1, start = None, end = None):\n    df = getTimeSeries(filename)\n    if flag:\n        print(\"Original Time Series\")\n        PlotTimeSeries(df, 1)\n        PlotTimeSeries(df, 0)\n    else:\n        DF = df.iloc[start : end, :]\n        print(\"Trimmed Time Series\")\n        PlotTimeSeries(DF, 1)\n        PlotTimeSeries(DF, 0)\n        dfN = FeaturiserN(fetchTotTS(DF), features_sel)\n        y_pred = model.predict(dfN)\n        print(classesN[y_pred[0]])\n\n\nPredictPlot(\"TS5WalkingUpstairs.csv\", 0, 6000, 11000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS6WalkingDownstairs.csv\", 0, 6000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS7WalkingDownstairs.csv\", 0, 4000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS8WalkingUpstairs.csv\", 0, 4000, 11000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSTANDING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS9Sitting.csv\", 0, 4000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS11Standing.csv\", 0, 4000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS12Standing.csv\", 0, 4000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 1)\n\nOriginal Time Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 0, 6000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 0, 17000, 22000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 0, 1000, 7800)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS15Walking.csv\", 0, 2600, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS16Laying.csv\", 0, 5000, 17000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\n## flag = 1 for a single plot and 0 for subplots for 2 - 8 depths\ndef confMatrix(dataFrame, flag = 1, accuracies = None):\n    if flag:\n        plt.figure(figsize = (6, 6))\n        ax = sns.heatmap(dataFrame, annot = True, cmap = \"PuBu\")\n        plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 8)\n        plt.setp(ax.get_yticklabels(), fontsize = 8)\n        plt.ylabel(\"True label\", fontsize = 18)\n        plt.xlabel(\"Predicted label\", fontsize = 18)\n        plt.title(f\"Accuracy = {accuracy_score(y, y_pred)*100: .4f}%\", fontweight = \"bold\", fontsize = 13)\n        plt.show()\n    else:\n        fig, axes = plt.subplots(3, 3, figsize = (25, 25))\n        axes = axes.flatten()\n\n        for i, df in enumerate(dataFrame):\n            ax = sns.heatmap(df, annot = True, ax = axes[i], cbar = False, cmap = \"PuBu\")\n            \n            plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 6)\n            plt.setp(ax.get_yticklabels(), fontsize = 8)\n            ax.set_title(f\"Depth = {i + 2}\\nAccuracy = {accuracies[i] * 100: .4f}%\", fontsize = 10)\n            ax.set_ylabel(\"True label\", fontsize = 12)\n            ax.set_xlabel(\"Predicted label\", fontsize = 12)\n            \n        plt.delaxes(axes[7])\n        plt.delaxes(axes[8])\n        plt.tight_layout()\n        plt.subplots_adjust(wspace = 1.1, hspace = 1.1)\n        plt.show()\n\n\nPredictPlot(\"TS17Sitting.csv\", 0, 5000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS18Sitting.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS19Sitting.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS21Sitting.csv\", 0, 5000, 15000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS22Standing.csv\", 0, 5000, 11000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS23Standing.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS24Standing.csv\", 0, 5000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS25Standing.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS26Standing.csv\", 0, 5000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS27Laying.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS28Laying.csv\", 0, 5000, 15000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS29Laying.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS30Laying.csv\", 0, 4000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS31Laying.csv\", 0, 5000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS32Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS33Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS34Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS35Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS36Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS37Upstairs.csv\", 0, 2000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSTANDING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS38Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS39Upstairs.csv\", 0, 2000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS40Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS41Upstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS42Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS43Upstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS44Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS45Upstairs.csv\", 0, 2000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS46Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\ny = [1, 4, 2, 3, 3, 2, 4, 5, 5, 4, 5, 1, 6, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\ny_pred = [1, 4, 2, 3, 3, 5, 4, 4, 4, 4, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 1, 1, 1, 2, 1, 5, 3, 2, 2, 2, 3, 2, 3, 2, 3]\n\n42 42\n\n\n\ncm = confusion_matrix(y, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\ndf_cm\n\n\n\n\n\n\n\n\nWALKING\nWALKING_UPSTAIRS\nWALKING_DOWNSTAIRS\nSITTING\nSTANDING\nLAYING\n\n\n\n\nWALKING\n5\n1\n0\n0\n0\n1\n\n\nWALKING_UPSTAIRS\n0\n5\n0\n0\n2\n0\n\n\nWALKING_DOWNSTAIRS\n0\n1\n6\n0\n0\n0\n\n\nSITTING\n0\n0\n0\n7\n0\n0\n\n\nSTANDING\n0\n0\n0\n7\n0\n1\n\n\nLAYING\n0\n0\n0\n1\n0\n5\n\n\n\n\n\n\n\n\nprint(classification_report(y, y_pred, labels = np.unique(y_pred)))\n\n              precision    recall  f1-score   support\n\n           1       1.00      0.71      0.83         7\n           2       0.71      0.71      0.71         7\n           3       1.00      0.86      0.92         7\n           4       0.47      1.00      0.64         7\n           5       0.00      0.00      0.00         8\n           6       0.71      0.83      0.77         6\n\n    accuracy                           0.67        42\n   macro avg       0.65      0.69      0.65        42\nweighted avg       0.63      0.67      0.63        42\n\n\n\n\nconfMatrix(df_cm, 1)"
  },
  {
    "objectID": "posts/Model-Deployment/FinalTrainingTry1.html#featurising-all-the-x_train_ts-x_test_ts-x_val_ts",
    "href": "posts/Model-Deployment/FinalTrainingTry1.html#featurising-all-the-x_train_ts-x_test_ts-x_val_ts",
    "title": "Model Deployment on Feature Extracted Data",
    "section": "",
    "text": "dfTrain = Featuriser(X_train_TS, features_sel)\ndfTest = Featuriser(X_test_TS, features_sel)\ndfVal = Featuriser(X_val_TS, features_sel)\n\n\ndfTrain.shape\n\n(108, 4)\n\n\n\ndfTest.shape\n\n(36, 4)\n\n\n\ndfVal.shape\n\n(36, 4)\n\n\n\ndfTrain\n\n\n\n\n\n\n\n\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n\n\n\n\n0\n1.058182\n0.000441\n0.276308\n0.010239\n\n\n0\n1.072680\n0.000439\n0.302652\n0.011554\n\n\n0\n1.141142\n0.281282\n2.951101\n0.382585\n\n\n0\n1.193139\n0.442850\n2.853736\n0.537589\n\n\n0\n1.005901\n0.000026\n0.042222\n0.004003\n\n\n...\n...\n...\n...\n...\n\n\n0\n1.328807\n1.370835\n4.655614\n0.995502\n\n\n0\n1.188371\n0.436127\n3.625210\n0.506932\n\n\n0\n1.066069\n0.000026\n0.031092\n0.004027\n\n\n0\n1.116685\n0.245975\n2.492894\n0.382408\n\n\n0\n1.118337\n0.250313\n2.180257\n0.381846\n\n\n\n\n108 rows × 4 columns\n\n\n\n\nhyperparams = {\"max_depth\" : [2, 3, 4, 5, 6, 7, 8, 9, 10], \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\nhyperparams\n\n{'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n 'criterion': ['gini', 'entropy'],\n 'min_samples_leaf': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}\n\n\n\nfrom itertools import product\nfinal, counter = {}, 0\nfor max_depth, criteria, min_sample in product(hyperparams[\"max_depth\"], hyperparams[\"criterion\"], hyperparams[\"min_samples_leaf\"]):\n    model = DecisionTreeClassifier(max_depth = max_depth, criterion = criteria, min_samples_leaf = min_sample, random_state = 42)\n    model.fit(dfTrain, y_train)\n    val_score = model.score(dfVal, y_val)\n    final[counter] = {\"max_depth\" : max_depth, \"criterion\" : criteria, \"min_samples_leaf\" : min_sample, \"val_score\" : val_score}\n    counter += 1\n\n\nhparam_df = pd.DataFrame(final).T\nhparam_df\n\n\n\n\n\n\n\n\nmax_depth\ncriterion\nmin_samples_leaf\nval_score\n\n\n\n\n0\n2\ngini\n1\n0.638889\n\n\n1\n2\ngini\n2\n0.638889\n\n\n2\n2\ngini\n3\n0.638889\n\n\n3\n2\ngini\n4\n0.638889\n\n\n4\n2\ngini\n5\n0.638889\n\n\n...\n...\n...\n...\n...\n\n\n265\n10\nentropy\n11\n0.694444\n\n\n266\n10\nentropy\n12\n0.666667\n\n\n267\n10\nentropy\n13\n0.666667\n\n\n268\n10\nentropy\n14\n0.666667\n\n\n269\n10\nentropy\n15\n0.666667\n\n\n\n\n270 rows × 4 columns\n\n\n\n\nhparam_df.sort_values(by = \"val_score\", ascending = False).head(10)\n\n\n\n\n\n\n\n\nmax_depth\ncriterion\nmin_samples_leaf\nval_score\n\n\n\n\n136\n6\nentropy\n2\n0.777778\n\n\n256\n10\nentropy\n2\n0.722222\n\n\n255\n10\nentropy\n1\n0.722222\n\n\n115\n5\nentropy\n11\n0.694444\n\n\n130\n6\ngini\n11\n0.694444\n\n\n220\n9\ngini\n11\n0.694444\n\n\n241\n10\ngini\n2\n0.694444\n\n\n100\n5\ngini\n11\n0.694444\n\n\n250\n10\ngini\n11\n0.694444\n\n\n190\n8\ngini\n11\n0.694444\n\n\n\n\n\n\n\n\ndfTrain_Val_Test = pd.concat([dfTrain, dfVal, dfTest], axis = 0)\ny_train_test_val = np.hstack([y_train, y_val, y_test])\ndfTrain_Val_Test\n\n\n\n\n\n\n\n\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n\n\n\n\n0\n1.058182\n0.000441\n0.276308\n0.010239\n\n\n0\n1.072680\n0.000439\n0.302652\n0.011554\n\n\n0\n1.141142\n0.281282\n2.951101\n0.382585\n\n\n0\n1.193139\n0.442850\n2.853736\n0.537589\n\n\n0\n1.005901\n0.000026\n0.042222\n0.004003\n\n\n...\n...\n...\n...\n...\n\n\n0\n1.012756\n0.000036\n0.046293\n0.004625\n\n\n0\n1.029612\n0.000035\n0.055289\n0.004476\n\n\n0\n1.221348\n0.614296\n3.526818\n0.662692\n\n\n0\n1.168859\n0.455832\n3.239159\n0.501238\n\n\n0\n1.159747\n0.317845\n2.934196\n0.436656\n\n\n\n\n180 rows × 4 columns\n\n\n\n\nmodel = DecisionTreeClassifier(max_depth = 6, min_samples_leaf = 2, criterion = \"entropy\", random_state = 42)\nmodel.fit(dfTrain_Val_Test, y_train_test_val)\n\nDecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=2,\n                       random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=2,\n                       random_state=42)\n\n\n\ndef getTimeSeries(filename):\n    filePath = f\"./Time Series Data/{filename}\"\n    df = pd.read_csv(filePath)\n    return df\n\n\ndf = getTimeSeries('TS2Walking.csv')\ndf\n\n\n\n\n\n\n\n\ntime\ngFx\ngFy\ngFz\n0\n\n\n\n\n0\n0.004371\n-0.9965\n0.1796\n0.2842\n1.068\n\n\n1\n0.005229\n-1.0007\n0.1845\n0.2910\n1.075\n\n\n2\n0.005670\n-1.0034\n0.1886\n0.2964\n1.080\n\n\n3\n0.006074\n-1.0026\n0.1903\n0.2984\n1.080\n\n\n4\n0.006489\n-0.9985\n0.1920\n0.2954\n1.076\n\n\n...\n...\n...\n...\n...\n...\n\n\n19310\n38.514996\n-0.9303\n-0.0344\n0.4249\n1.032\n\n\n19311\n38.516256\n-0.9301\n-0.0349\n0.4234\n1.031\n\n\n19312\n38.518234\n-0.9315\n-0.0347\n0.4222\n1.032\n\n\n19313\n38.520242\n-0.9335\n-0.0354\n0.4229\n1.034\n\n\n19314\n38.522770\n-0.9354\n-0.0364\n0.4251\n1.037\n\n\n\n\n19315 rows × 5 columns\n\n\n\n\ndef fetchTotTS(dataFrame):\n    return pd.DataFrame(dataFrame.iloc[:, 4]**2)\n\n\ndef PlotTimeSeries(df, flag):\n    latexify()\n    if flag:\n        plt.figure(figsize = (9, 3))\n        plt.title(r\"Time Series of Acceleration $(acc_x, acc_y, acc_z)$\")\n        colors = [\"red\", \"green\", \"blue\"]\n        for k in range(1, 4):\n            plt.plot(df.iloc[:, k], color = colors[k - 1], linewidth = 0.8)\n        plt.xlabel(\"Time Samples\")\n        plt.ylabel(r\"Acceleration in $m/s^2$\")\n        plt.legend([r\"$a_x$\", r\"$a_y$\", r\"$a_z$\"])\n        plt.grid()\n        plt.show()\n    else:\n        plt.figure(figsize = (9, 3))\n        plt.title(r\"Time Series of Total Acceleration $(acc_x^2 + acc_y^2 + acc_z^2)$\")\n        plt.plot(df.iloc[:, 4]**2, color = \"deeppink\", linewidth = 0.8)\n        plt.xlabel(\"Time Samples\")\n        plt.ylabel(r\"Total Acceleration in $m/s^2$\")\n        plt.legend([r\"$(acc_x^2 + acc_y^2 + acc_z^2)$\"])\n        plt.grid()\n        plt.show()\n\n\n\n\n\n\n\ndf.shape[0] / 500.0\n\n38.63\n\n\n\ndef FeaturiserN(XTimeSeries, features):\n    model1 = tsfel.get_features_by_domain()\n    df = pd.DataFrame(XTimeSeries).T\n    dfN = tsfel.time_series_features_extractor(model1, signal_windows = list(df.iloc[0, :]), fs = 50)\n    dfNFeaturized = dfN[features]\n    return dfNFeaturized\n\n\npd.DataFrame(fetchTotTS(df.iloc[2500:7500, :])).T\n\n\n\n\n\n\n\n\n2500\n2501\n2502\n2503\n2504\n2505\n2506\n2507\n2508\n2509\n...\n7490\n7491\n7492\n7493\n7494\n7495\n7496\n7497\n7498\n7499\n\n\n\n\n0\n0.751689\n0.755161\n0.758641\n0.765625\n0.776161\n0.786769\n0.799236\n0.808201\n0.817216\n0.8281\n...\n1.646089\n1.565001\n1.517824\n1.485961\n1.452025\n1.420864\n1.3924\n1.364224\n1.331716\n1.301881\n\n\n\n\n1 rows × 5000 columns\n\n\n\n\ndfN1 = FeaturiserN(fetchTotTS(df.iloc[2500:7500, :]), features_sel)\ndfN1\n\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\n\n\n\n\n\n\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n\n\n\n\n0\n1.18036\n0.49156\n3.675776\n0.540058\n\n\n\n\n\n\n\n\nPlotTimeSeries(df.iloc[2500:7500,:], 1)\nPlotTimeSeries(df.iloc[2500:7500,:], 0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny_pred = model.predict(dfN1)\ny_pred\n\narray([2])\n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\ndf1 = getTimeSeries('TS4Walking.csv')\nDF = df1.iloc[2500:7500, :]\nPlotTimeSeries(DF, 1)\nPlotTimeSeries(DF, 0)\ndfN2 = FeaturiserN(fetchTotTS(DF), features_sel)\ny_pred = model.predict(dfN2)\nclassesN[y_pred[0]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n'WALKING'\n\n\n\ndf2 = getTimeSeries('TS10Sitting.csv')\nDF = df2.iloc[2500:7500, :]\nPlotTimeSeries(DF, 1)\nPlotTimeSeries(DF, 0)\ndfN3 = FeaturiserN(fetchTotTS(DF), features_sel)\ny_pred = model.predict(dfN3)\nclassesN[y_pred[0]]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n'SITTING'\n\n\n\n# flag = 1 -&gt; Only display the orginal untrimmed TS and trim-prediction on flag != 1\ndef PredictPlot(filename,  flag = 1, start = None, end = None):\n    df = getTimeSeries(filename)\n    if flag:\n        print(\"Original Time Series\")\n        PlotTimeSeries(df, 1)\n        PlotTimeSeries(df, 0)\n    else:\n        DF = df.iloc[start : end, :]\n        print(\"Trimmed Time Series\")\n        PlotTimeSeries(DF, 1)\n        PlotTimeSeries(DF, 0)\n        dfN = FeaturiserN(fetchTotTS(DF), features_sel)\n        y_pred = model.predict(dfN)\n        print(classesN[y_pred[0]])\n\n\nPredictPlot(\"TS5WalkingUpstairs.csv\", 0, 6000, 11000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS6WalkingDownstairs.csv\", 0, 6000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS7WalkingDownstairs.csv\", 0, 4000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS8WalkingUpstairs.csv\", 0, 4000, 11000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSTANDING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS9Sitting.csv\", 0, 4000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS11Standing.csv\", 0, 4000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS12Standing.csv\", 0, 4000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 1)\n\nOriginal Time Series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 0, 6000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 0, 17000, 22000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS13SittingStanding.csv\", 0, 1000, 7800)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS15Walking.csv\", 0, 2600, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS16Laying.csv\", 0, 5000, 17000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\n## flag = 1 for a single plot and 0 for subplots for 2 - 8 depths\ndef confMatrix(dataFrame, flag = 1, accuracies = None):\n    if flag:\n        plt.figure(figsize = (6, 6))\n        ax = sns.heatmap(dataFrame, annot = True, cmap = \"PuBu\")\n        plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 8)\n        plt.setp(ax.get_yticklabels(), fontsize = 8)\n        plt.ylabel(\"True label\", fontsize = 18)\n        plt.xlabel(\"Predicted label\", fontsize = 18)\n        plt.title(f\"Accuracy = {accuracy_score(y, y_pred)*100: .4f}%\", fontweight = \"bold\", fontsize = 13)\n        plt.show()\n    else:\n        fig, axes = plt.subplots(3, 3, figsize = (25, 25))\n        axes = axes.flatten()\n\n        for i, df in enumerate(dataFrame):\n            ax = sns.heatmap(df, annot = True, ax = axes[i], cbar = False, cmap = \"PuBu\")\n            \n            plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 6)\n            plt.setp(ax.get_yticklabels(), fontsize = 8)\n            ax.set_title(f\"Depth = {i + 2}\\nAccuracy = {accuracies[i] * 100: .4f}%\", fontsize = 10)\n            ax.set_ylabel(\"True label\", fontsize = 12)\n            ax.set_xlabel(\"Predicted label\", fontsize = 12)\n            \n        plt.delaxes(axes[7])\n        plt.delaxes(axes[8])\n        plt.tight_layout()\n        plt.subplots_adjust(wspace = 1.1, hspace = 1.1)\n        plt.show()\n\n\nPredictPlot(\"TS17Sitting.csv\", 0, 5000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS18Sitting.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS19Sitting.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS21Sitting.csv\", 0, 5000, 15000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS22Standing.csv\", 0, 5000, 11000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS23Standing.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS24Standing.csv\", 0, 5000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS25Standing.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS26Standing.csv\", 0, 5000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSITTING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS27Laying.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS28Laying.csv\", 0, 5000, 15000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS29Laying.csv\", 0, 4000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS30Laying.csv\", 0, 4000, 12000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS31Laying.csv\", 0, 5000, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nLAYING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS32Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS33Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS34Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS35Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS36Walking.csv\", 0, 2500, 12500)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS37Upstairs.csv\", 0, 2000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nSTANDING\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS38Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS39Upstairs.csv\", 0, 2000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS40Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS41Upstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS42Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS43Upstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS44Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS45Upstairs.csv\", 0, 2000, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_UPSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nPredictPlot(\"TS46Downstairs.csv\", 0, 2500, 10000)\n\nTrimmed Time Series\n*** Feature extraction started ***\n\n*** Feature extraction finished ***\nWALKING_DOWNSTAIRS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              \n                  Progress: 100% Complete\n              \n              \n                  60\n              \n\n    \n\n\n\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\ny = [1, 4, 2, 3, 3, 2, 4, 5, 5, 4, 5, 1, 6, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3]\ny_pred = [1, 4, 2, 3, 3, 5, 4, 4, 4, 4, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 1, 1, 1, 2, 1, 5, 3, 2, 2, 2, 3, 2, 3, 2, 3]\n\n42 42\n\n\n\ncm = confusion_matrix(y, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\ndf_cm\n\n\n\n\n\n\n\n\nWALKING\nWALKING_UPSTAIRS\nWALKING_DOWNSTAIRS\nSITTING\nSTANDING\nLAYING\n\n\n\n\nWALKING\n5\n1\n0\n0\n0\n1\n\n\nWALKING_UPSTAIRS\n0\n5\n0\n0\n2\n0\n\n\nWALKING_DOWNSTAIRS\n0\n1\n6\n0\n0\n0\n\n\nSITTING\n0\n0\n0\n7\n0\n0\n\n\nSTANDING\n0\n0\n0\n7\n0\n1\n\n\nLAYING\n0\n0\n0\n1\n0\n5\n\n\n\n\n\n\n\n\nprint(classification_report(y, y_pred, labels = np.unique(y_pred)))\n\n              precision    recall  f1-score   support\n\n           1       1.00      0.71      0.83         7\n           2       0.71      0.71      0.71         7\n           3       1.00      0.86      0.92         7\n           4       0.47      1.00      0.64         7\n           5       0.00      0.00      0.00         8\n           6       0.71      0.83      0.77         6\n\n    accuracy                           0.67        42\n   macro avg       0.65      0.69      0.65        42\nweighted avg       0.63      0.67      0.63        42\n\n\n\n\nconfMatrix(df_cm, 1)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html",
    "href": "posts/Feature-Extraction/FeaturesExtr.html",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "",
    "text": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom latex import latexify, format_axes\nimport numpy as np\nimport tsfel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom MakeDataset import *\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#power-bandwidth",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#power-bandwidth",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Power Bandwidth",
    "text": "Power Bandwidth\n\nFeaturePlot(dfN, idx = 325)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#signal-distance",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#signal-distance",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Signal Distance",
    "text": "Signal Distance\n\nFeaturePlot(dfN, idx = 327)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#negative-turning-point",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#negative-turning-point",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Negative Turning Point",
    "text": "Negative Turning Point\n\nFeaturePlot(dfN, idx = 321)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#maximum-frequency",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#maximum-frequency",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Maximum Frequency",
    "text": "Maximum Frequency\n\nFeaturePlot(dfN, idx = 310)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#area-under-curve",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#area-under-curve",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Area under Curve",
    "text": "Area under Curve\n\nFeaturePlot(dfN, feature = features_sel[0])"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#mean",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#mean",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Mean",
    "text": "Mean\n\nFeaturePlot(dfN, feature = features_sel[1])"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#variance",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#variance",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Variance",
    "text": "Variance\n\nFeaturePlot(dfN, feature = features_sel[2])"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#time-series-peak-to-peak-distance",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#time-series-peak-to-peak-distance",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "TIME SERIES PEAK-TO-PEAK DISTANCE",
    "text": "TIME SERIES PEAK-TO-PEAK DISTANCE\n\nFeaturePlot(dfN, feature = features_sel[3])"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#mean-absolute-deviation",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#mean-absolute-deviation",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Mean Absolute Deviation",
    "text": "Mean Absolute Deviation\n\nFeaturePlot(dfN, feature = features_sel[4])\n\n\n\n\n\n\n\n\n\nOur Selected Features\n\nMean\nArea under Curve\nPeak-to-Peak Distance\nVariance\nMean Absolute Deviation\nMaximum Frequency &gt; Newly Added\n0_Power bandwidth\n0_Spectral centroid\n0_Spectral decrease\n0_Spectral distance\n0_Spectral entropy\n0_Spectral kurtosis\n0_Spectral positive turning points\n0_Spectral roll-off\n0_Spectral roll-on\n0_Spectral skewness\n0_Spectral slope\n0_Spectral spread\n0_Spectral variation\n\n\n\nLet’s add some spectral features too to the 5 already selected -&gt; 18 Featured Data\n\nf_sel = [\"0_Area under the curve\", \"0_Mean\", \"0_Variance\", \"0_Peak to peak distance\", \"0_Mean absolute deviation\", \"0_Power bandwidth\", \"0_Spectral centroid\", \"0_Spectral decrease\", \"0_Spectral distance\", \"0_Spectral entropy\", \"0_Spectral kurtosis\", \"0_Spectral positive turning points\", \"0_Spectral roll-off\", \"0_Spectral roll-on\", \"0_Spectral skewness\", \"0_Spectral slope\", \"0_Spectral spread\", \"0_Spectral variation\", \"Labels\", \"Subject\", \"Named_Subject\"]\ndfFeat = dfN[f_sel]\ndfFeat\n\n\n\n\n\n\n\n\n0_Area under the curve\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\n0_Power bandwidth\n0_Spectral centroid\n0_Spectral decrease\n0_Spectral distance\n0_Spectral entropy\n...\n0_Spectral positive turning points\n0_Spectral roll-off\n0_Spectral roll-on\n0_Spectral skewness\n0_Spectral slope\n0_Spectral spread\n0_Spectral variation\nLabels\nSubject\nNamed_Subject\n\n\n\n\n0\n10.539676\n1.058197\n0.000441\n0.276308\n0.010246\n6.212425\n0.639077\n-46.590172\n-70826.079944\n0.727967\n...\n73.0\n4.809619\n0.0\n4.906914\n-0.000905\n2.304270\n0.903439\n5\n1\nSTANDING\n\n\n0\n10.683732\n1.072680\n0.000440\n0.302652\n0.011577\n9.018036\n0.850078\n-41.721346\n-71473.612034\n0.781145\n...\n72.0\n6.513026\n0.0\n4.151249\n-0.000889\n2.828170\n0.951706\n5\n2\nSTANDING\n\n\n0\n11.328686\n1.139029\n0.279613\n2.951101\n0.380676\n5.410822\n3.887285\n-2.433189\n-169599.425066\n0.547602\n...\n78.0\n15.531062\n0.0\n1.869257\n-0.000657\n5.062929\n0.735493\n2\n3\nWALKING_UPSTAIRS\n\n\n0\n11.865417\n1.191914\n0.442988\n2.853736\n0.537075\n6.212425\n4.521254\n-1.803868\n-204731.964203\n0.592184\n...\n80.0\n16.132265\n0.0\n1.632312\n-0.000608\n5.077597\n0.698450\n3\n4\nWALKING_DOWNSTAIRS\n\n\n0\n10.018665\n1.005892\n0.000026\n0.042222\n0.004001\n13.827655\n0.281477\n-151.116426\n-63806.084893\n0.806408\n...\n79.0\n0.000000\n0.0\n7.583021\n-0.000933\n1.753868\n0.851520\n6\n5\nLAYING\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\n13.254750\n1.331151\n1.370835\n4.655614\n0.995919\n5.010020\n4.916563\n-1.218502\n-294434.103156\n0.565975\n...\n78.0\n16.332665\n0.0\n1.444425\n-0.000578\n5.130396\n0.469507\n3\n104\nWALKING_DOWNSTAIRS\n\n\n0\n11.826549\n1.187985\n0.436927\n3.625210\n0.507468\n8.216433\n5.361472\n-2.037178\n-167545.154695\n0.492424\n...\n81.0\n16.533066\n0.0\n1.054264\n-0.000544\n5.306712\n0.819865\n1\n105\nWALKING\n\n\n0\n10.618080\n1.066065\n0.000026\n0.031092\n0.004030\n12.825651\n0.275996\n-158.072078\n-67537.987024\n0.818778\n...\n87.0\n0.000000\n0.0\n7.708759\n-0.000933\n1.746586\n0.950194\n5\n106\nSTANDING\n\n\n0\n11.132367\n1.117178\n0.246346\n2.492894\n0.382848\n3.406814\n3.119947\n-2.902698\n-160379.192325\n0.469127\n...\n81.0\n12.725451\n0.0\n1.982978\n-0.000716\n4.165671\n0.738380\n2\n107\nWALKING_UPSTAIRS\n\n\n0\n11.142791\n1.118466\n0.250806\n2.180257\n0.382529\n5.410822\n3.392079\n-2.718273\n-163036.885100\n0.501480\n...\n74.0\n12.024048\n0.0\n1.739971\n-0.000695\n4.187673\n0.750399\n2\n108\nWALKING_UPSTAIRS\n\n\n\n\n108 rows × 21 columns\n\n\n\n\n\nPCA on 18 Featured Data\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(dfFeat.iloc[:, :-3])\nmodel = PCA(n_components = 2)\nX_trainFeat_2D = model.fit_transform(X_scaled)\ndfPCAFeat = pd.DataFrame(X_trainFeat_2D)\ndfPCAFeat[\"Labels\"] = y_train\ndfPCAFeat\n\n\n\n\n\n\n\n\n0\n1\nLabels\n\n\n\n\n0\n-2.119269\n1.516116\n5\n\n\n1\n-1.979877\n1.739336\n5\n\n\n2\n2.746499\n0.798706\n2\n\n\n3\n3.579078\n-0.146838\n3\n\n\n4\n-4.607082\n-0.719782\n6\n\n\n...\n...\n...\n...\n\n\n103\n6.670144\n-2.509504\n3\n\n\n104\n3.678577\n0.154650\n1\n\n\n105\n-4.411194\n-1.856612\n5\n\n\n106\n2.263685\n0.657534\n2\n\n\n107\n2.115628\n1.393518\n2\n\n\n\n\n108 rows × 3 columns"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#featured-data-pca-to-2d",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#featured-data-pca-to-2d",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "18 Featured Data PCA to 2D",
    "text": "18 Featured Data PCA to 2D\n\nPCA_Plot(dfPCAFeat)\n\n\n\n\n\n\n\n\n\nExtracting DataFrame for our 5 featurized Data\n\n\nFeaturized DataFrame\n\ndfNewFeaturized = dfN[features_sel]\ndfNewFeaturized\n\n\n\n\n\n\n\n\n0_Area under the curve\n0_Mean\n0_Variance\n0_Peak to peak distance\n0_Mean absolute deviation\nLabels\nSubject\nNamed_Subject\n\n\n\n\n0\n10.539676\n1.058197\n0.000441\n0.276308\n0.010246\n5\n1\nSTANDING\n\n\n0\n10.683732\n1.072680\n0.000440\n0.302652\n0.011577\n5\n2\nSTANDING\n\n\n0\n11.328686\n1.139029\n0.279613\n2.951101\n0.380676\n2\n3\nWALKING_UPSTAIRS\n\n\n0\n11.865417\n1.191914\n0.442988\n2.853736\n0.537075\n3\n4\nWALKING_DOWNSTAIRS\n\n\n0\n10.018665\n1.005892\n0.000026\n0.042222\n0.004001\n6\n5\nLAYING\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n0\n13.254750\n1.331151\n1.370835\n4.655614\n0.995919\n3\n104\nWALKING_DOWNSTAIRS\n\n\n0\n11.826549\n1.187985\n0.436927\n3.625210\n0.507468\n1\n105\nWALKING\n\n\n0\n10.618080\n1.066065\n0.000026\n0.031092\n0.004030\n5\n106\nSTANDING\n\n\n0\n11.132367\n1.117178\n0.246346\n2.492894\n0.382848\n2\n107\nWALKING_UPSTAIRS\n\n\n0\n11.142791\n1.118466\n0.250806\n2.180257\n0.382529\n2\n108\nWALKING_UPSTAIRS\n\n\n\n\n108 rows × 8 columns\n\n\n\n\n\nPCA on our chosen 5 Featurized Data\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(dfNewFeaturized.iloc[:, :-3])\nmodel = PCA(n_components = 2)\nX_trainOurF_2D = model.fit_transform(X_scaled)\ndfPCAFeat = pd.DataFrame(X_trainFeat_2D)\ndfPCAFeat[\"Labels\"] = y_train\ndfPCAFeat\n\n\n\n\n\n\n\n\n0\n1\nLabels\n\n\n\n\n0\n-2.408634\n1.329825\n5\n\n\n1\n-2.039138\n1.697191\n5\n\n\n2\n2.543982\n0.746600\n2\n\n\n3\n3.471005\n-0.131893\n3\n\n\n4\n-4.349947\n-0.661725\n6\n\n\n...\n...\n...\n...\n\n\n103\n6.572669\n-2.406062\n3\n\n\n104\n3.731875\n0.248615\n1\n\n\n105\n-4.227187\n-1.883089\n5\n\n\n106\n1.883058\n0.470228\n2\n\n\n107\n1.891408\n1.328787\n2\n\n\n\n\n108 rows × 3 columns\n\n\n\n\ndfPCAOurF = pd.DataFrame(X_trainOurF_2D)\ndfPCAOurF[\"Labels\"] = y_train\ndfPCAOurF\n\n\n\n\n\n\n\n\n0\n1\nLabels\n\n\n\n\n0\n-1.591114\n-0.091353\n5\n\n\n1\n-1.427712\n-0.222992\n5\n\n\n2\n0.880008\n-0.234883\n2\n\n\n3\n1.874859\n-0.287865\n3\n\n\n4\n-2.220049\n0.391626\n6\n\n\n...\n...\n...\n...\n\n\n103\n5.704453\n0.657241\n3\n\n\n104\n1.976794\n-0.320237\n1\n\n\n105\n-1.580248\n-0.149378\n5\n\n\n106\n0.502482\n-0.098926\n2\n\n\n107\n0.439838\n-0.079656\n2\n\n\n\n\n108 rows × 3 columns"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#featurized-pca-datapoints",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#featurized-pca-datapoints",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "5 Featurized PCA datapoints",
    "text": "5 Featurized PCA datapoints\n\nPCA_Plot(dfPCAOurF)\n\n\n\n\n\n\n\n\n\nPCA on our raw timeseries data\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train_TS)\nmodel = PCA(n_components = 2)\nX_train_2D = model.fit_transform(X_scaled)\n\n\ndfPCA = pd.DataFrame(X_train_2D)\ndfPCA[\"Labels\"] = y_train\ndfPCA\n\n\n\n\n\n\n\n\n0\n1\nLabels\n\n\n\n\n0\n0.171000\n-0.058009\n5\n\n\n1\n-0.008648\n0.128133\n5\n\n\n2\n2.803362\n2.955462\n2\n\n\n3\n2.856289\n-7.198528\n3\n\n\n4\n0.192954\n-0.168915\n6\n\n\n...\n...\n...\n...\n\n\n103\n6.098028\n41.782163\n3\n\n\n104\n-4.212006\n8.896049\n1\n\n\n105\n0.105575\n0.038338\n5\n\n\n106\n2.634434\n0.671926\n2\n\n\n107\n4.157850\n2.671812\n2\n\n\n\n\n108 rows × 3 columns"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#raw-timeseries-pca-datapoints",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#raw-timeseries-pca-datapoints",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Raw Timeseries PCA datapoints",
    "text": "Raw Timeseries PCA datapoints\n\nPCA_Plot(dfPCA)\n\n\n\n\n\n\n\n\n\nPCA on entire 383 Featurized Data\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(dfN.iloc[:, :-3])\nmodel = PCA(n_components = 2)\nX_trainF_2D = model.fit_transform(X_scaled)\n\n\ndfPCAF = pd.DataFrame(X_trainF_2D)\ndfPCAF[\"Labels\"] = y_train\ndfPCAF\n\n\n\n\n\n\n\n\n0\n1\nLabels\n\n\n\n\n0\n-9.656330\n1.259515\n5\n\n\n1\n-9.507877\n1.189473\n5\n\n\n2\n0.904597\n-3.530496\n2\n\n\n3\n9.379393\n-3.324217\n3\n\n\n4\n-10.632995\n2.040261\n6\n\n\n...\n...\n...\n...\n\n\n103\n34.743740\n3.095295\n3\n\n\n104\n14.424106\n-3.185719\n1\n\n\n105\n-10.430969\n1.901364\n5\n\n\n106\n-0.354879\n-4.074157\n2\n\n\n107\n-0.018798\n-4.441626\n2\n\n\n\n\n108 rows × 3 columns"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#featurized-pca-datapoints-1",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#featurized-pca-datapoints-1",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "383 Featurized PCA Datapoints",
    "text": "383 Featurized PCA Datapoints\n\nPCA_Plot(dfPCAF)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#dfnewfeaturized-has-5-selected-features-and-dffeat-has-18-selected-features",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#dfnewfeaturized-has-5-selected-features-and-dffeat-has-18-selected-features",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "dfNewFeaturized has \\(5\\) selected features and dfFeat has \\(18\\) selected features",
    "text": "dfNewFeaturized has \\(5\\) selected features and dfFeat has \\(18\\) selected features"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#template-funtion-to-featurize-a-dataset",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#template-funtion-to-featurize-a-dataset",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Template Funtion to Featurize a Dataset",
    "text": "Template Funtion to Featurize a Dataset\n\ndef Featuriser(XTimeSeries, YTimeSeries, features):\n    cfg = tsfel.get_features_by_domain()\n    df = pd.DataFrame(XTimeSeries)\n    dataFrames = []\n    for i in df.index:\n        dataFrames.append(tsfel.time_series_features_extractor(cfg, df.iloc[i,:], fs = 50))\n    dfN = pd.concat(dataFrames, axis = 0)\n    dfN[\"Labels\"] = YTimeSeries\n    namedLabel = [classesN[i] for i in YTimeSeries]\n    dfN[\"Named_Subject\"] = namedLabel\n    dfN[\"Subject\"] = range(1, len(XTimeSeries) + 1)\n    dfNFeaturized = dfN[features]\n    return dfNFeaturized\n\n\nThe features we wish to select for our dataframe\n\n# 5 Features\nfeatures_sel = [\"0_Area under the curve\", \"0_Mean\", \"0_Variance\", \"0_Peak to peak distance\", \"0_Mean absolute deviation\", \"Labels\", \"Subject\", \"Named_Subject\"]\n\n# 18 Features\nf_sel = [\"0_Area under the curve\", \"0_Mean\", \"0_Variance\", \"0_Peak to peak distance\", \"0_Mean absolute deviation\", \"0_Power bandwidth\", \"0_Spectral centroid\", \"0_Spectral decrease\", \"0_Spectral distance\", \"0_Spectral entropy\", \"0_Spectral kurtosis\", \"0_Spectral positive turning points\", \"0_Spectral roll-off\", \"0_Spectral roll-on\", \"0_Spectral skewness\", \"0_Spectral slope\", \"0_Spectral spread\", \"0_Spectral variation\", \"Labels\", \"Subject\", \"Named_Subject\"]\n\n\n\nFeaturizing the TEST dataset for our chosen \\(5\\) features\n\ndfNF_test = Featuriser(X_test_TS, y_test, features_sel)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#decision-tree-classifier-on-our-5-featurized-data",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#decision-tree-classifier-on-our-5-featurized-data",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Decision Tree Classifier on our 5 Featurized Data",
    "text": "Decision Tree Classifier on our 5 Featurized Data"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#classifier-for-5-featured-dfnewfeaturized",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#classifier-for-5-featured-dfnewfeaturized",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Classifier for \\(5\\) Featured dfNewFeaturized",
    "text": "Classifier for \\(5\\) Featured dfNewFeaturized\n\nmodel = DecisionTreeClassifier()\nclfg = model.fit(dfNewFeaturized.iloc[:, :-3], dfNewFeaturized.iloc[:, 5])\ny_pred = clfg.predict(dfNF_test.iloc[:, :-3])\ny_pred\n\narray([3, 3, 6, 2, 6, 5, 6, 1, 2, 3, 5, 6, 2, 5, 2, 4, 5, 5, 1, 6, 5, 1,\n       2, 5, 2, 1, 2, 4, 3, 6, 4, 6, 4, 2, 3, 1])\n\n\n\ny_test\n\narray([3, 3, 6, 2, 6, 5, 6, 1, 1, 3, 5, 6, 1, 5, 3, 4, 5, 5, 1, 6, 4, 1,\n       2, 5, 2, 1, 3, 6, 3, 4, 4, 4, 4, 2, 2, 2])\n\n\n\nAccuracy Score for decision tree classifier on TEST data trained on our 5 featurized dataset\n\naccuracy_score(y_test, y_pred)\n\n0.7222222222222222\n\n\n\n\nClassification Report for decision tree classifier on TEST data trained on our 5 featurized dataset\n\nprint(classification_report(y_test, y_pred, labels = np.unique(y_pred)))\n\n              precision    recall  f1-score   support\n\n           1       0.80      0.67      0.73         6\n           2       0.50      0.67      0.57         6\n           3       0.80      0.67      0.73         6\n           4       0.75      0.50      0.60         6\n           5       0.86      1.00      0.92         6\n           6       0.71      0.83      0.77         6\n\n    accuracy                           0.72        36\n   macro avg       0.74      0.72      0.72        36\nweighted avg       0.74      0.72      0.72        36\n\n\n\n\n\nConfusion Matrix for the above prediction\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\ndf_cm\n\n\n\n\n\n\n\n\nWALKING\nWALKING_UPSTAIRS\nWALKING_DOWNSTAIRS\nSITTING\nSTANDING\nLAYING\n\n\n\n\nWALKING\n4\n2\n0\n0\n0\n0\n\n\nWALKING_UPSTAIRS\n1\n4\n1\n0\n0\n0\n\n\nWALKING_DOWNSTAIRS\n0\n2\n4\n0\n0\n0\n\n\nSITTING\n0\n0\n0\n3\n1\n2\n\n\nSTANDING\n0\n0\n0\n0\n6\n0\n\n\nLAYING\n0\n0\n0\n1\n0\n5"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#template-code-for-displaying-confusion-matrix",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#template-code-for-displaying-confusion-matrix",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Template Code for Displaying Confusion Matrix",
    "text": "Template Code for Displaying Confusion Matrix\n\n## flag = 1 for a single plot and 0 for subplots for 2 - 8 depths\ndef confMatrix(dataFrame, flag = 1, accuracies = None):\n    if flag:\n        plt.figure(figsize = (6, 6))\n        ax = sns.heatmap(dataFrame, annot = True, cmap = \"PuBu\")\n        plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 8)\n        plt.setp(ax.get_yticklabels(), fontsize = 8)\n        plt.ylabel(\"True label\", fontsize = 18)\n        plt.xlabel(\"Predicted label\", fontsize = 18)\n        plt.title(f\"Accuracy = {accuracy_score(y_test, y_pred)*100: .4f}%\", fontweight = \"bold\", fontsize = 13)\n        plt.show()\n    else:\n        fig, axes = plt.subplots(3, 3, figsize = (25, 25))\n        axes = axes.flatten()\n\n        for i, df in enumerate(dataFrame):\n            ax = sns.heatmap(df, annot = True, ax = axes[i], cbar = False, cmap = \"PuBu\")\n            \n            plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 6)\n            plt.setp(ax.get_yticklabels(), fontsize = 8)\n            ax.set_title(f\"Depth = {i + 2}\\nAccuracy = {accuracies[i] * 100: .4f}%\", fontsize = 10)\n            ax.set_ylabel(\"True label\", fontsize = 12)\n            ax.set_xlabel(\"Predicted label\", fontsize = 12)\n            \n        plt.delaxes(axes[7])\n        plt.delaxes(axes[8])\n        plt.tight_layout()\n        plt.subplots_adjust(wspace = 1.1, hspace = 1.1)\n        plt.show()\n\n\nConfusion Matrix for the model trained on our 5-featured Dataset\n\nconfMatrix(df_cm, flag = 1)\n\n\n\n\n\n\n\n\n\n\nFetching the Connfusion Matrices, Class Reports, Accuracies for Depth \\((2 - 8)\\) Tree on 5-Featurized Data\n\nconfusion_matrices, class_reports, class_reports_dict, accuracies = [], [], [], []\nfor i in range(2, 9):\n    model = DecisionTreeClassifier(max_depth = i, random_state = 42)\n    clfg = model.fit(dfNewFeaturized.iloc[:, :-3], dfNewFeaturized.iloc[:, 5])\n    y_pred = clfg.predict(dfNF_test.iloc[:, :-3])\n    \n    pred, actual = y_pred, y_test\n    \n    cm = confusion_matrix(actual, pred)\n    \n    confusion_matrices.append(pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes]))\n    class_reports.append(classification_report(actual, pred, labels = np.unique(pred)))\n    class_reports_dict.append(classification_report(actual, pred, labels = np.unique(pred), output_dict = True))\n    accuracies.append(accuracy_score(actual, pred))\n\n\n\n\\(7\\) Confusion Matrices for 5-Featurized Data\n\nconfMatrix(confusion_matrices, flag = 0, accuracies = accuracies)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#decision-tree-classifier-on-raw-timeseries-data-x_train_ts",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#decision-tree-classifier-on-raw-timeseries-data-x_train_ts",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Decision Tree Classifier on RAW TimeSeries Data X_train_TS",
    "text": "Decision Tree Classifier on RAW TimeSeries Data X_train_TS\n\nmodel = DecisionTreeClassifier()\nclfg = model.fit(X_train_TS, y_train)\ny_pred1 = clfg.predict(X_test_TS)\ncm1 = confusion_matrix(y_test, y_pred1)\ndf_cm1 = pd.DataFrame(cm1, index = [classT for classT in classes], columns = [classT for classT in classes])\ndf_cm1\n\n\n\n\n\n\n\n\nWALKING\nWALKING_UPSTAIRS\nWALKING_DOWNSTAIRS\nSITTING\nSTANDING\nLAYING\n\n\n\n\nWALKING\n3\n1\n1\n1\n0\n0\n\n\nWALKING_UPSTAIRS\n2\n1\n1\n1\n1\n0\n\n\nWALKING_DOWNSTAIRS\n0\n3\n2\n1\n0\n0\n\n\nSITTING\n0\n0\n0\n4\n0\n2\n\n\nSTANDING\n0\n0\n0\n1\n5\n0\n\n\nLAYING\n0\n0\n0\n5\n0\n1\n\n\n\n\n\n\n\n\nConfusion Matrix for the model trained on RAW TimeSeries Data\n\nconfMatrix(df_cm1, flag = 1)\n\n\n\n\n\n\n\n\n\n\nFetching the Connfusion Matrices, Class Reports, Accuracies for Depth \\((2 - 8)\\) Tree on Raw Time Series Data\n\nconfusion_matrices1, class_reports1, class_reports_dict1, accuracies1 = [], [], [], []\nfor i in range(2, 9):\n    model = DecisionTreeClassifier(max_depth = i,random_state=42)\n    clfg = model.fit(X_train_TS, y_train)\n    y_pred = clfg.predict(X_test_TS)\n    \n    pred, actual = y_pred, y_test\n    \n    cm = confusion_matrix(actual, pred)\n    \n    confusion_matrices1.append(pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes]))\n    class_reports1.append(classification_report(actual, pred, labels = np.unique(pred)))\n    class_reports_dict1.append(classification_report(actual, pred, labels = np.unique(pred), output_dict = True))\n    accuracies1.append(accuracy_score(actual, pred))\n\n\n\n\\(7\\) Confusion Matrices for Raw Time Series Data\n\nconfMatrix(confusion_matrices1, flag = 0, accuracies = accuracies1)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#accuracy-comparison-for-both-raw-timeseries-and-5-featurized-data",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#accuracy-comparison-for-both-raw-timeseries-and-5-featurized-data",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Accuracy Comparison for both RAW TimeSeries and 5-Featurized Data",
    "text": "Accuracy Comparison for both RAW TimeSeries and 5-Featurized Data\n\nprint(accuracies)\nprint(accuracies1)\n\n[0.6111111111111112, 0.6388888888888888, 0.6388888888888888, 0.7222222222222222, 0.6666666666666666, 0.6944444444444444, 0.6388888888888888]\n[0.3888888888888889, 0.4444444444444444, 0.5277777777777778, 0.5555555555555556, 0.6111111111111112, 0.5277777777777778, 0.5277777777777778]\n\n\n\nplt.plot(range(2, 9), accuracies1, color = \"r\", marker = \"o\")\nplt.plot(range(2, 9), accuracies, color = \"g\", marker = \"s\")\nplt.xlabel(\"Decision Tree Depth\")\nplt.ylabel(\"Accuracies\")\nplt.title(\"Accuracy Variation of Test-Train vs. Depth\")\nplt.legend([\"Raw Data\", \"Featurized Data\"])\nplt.grid()\n\n\n\n\n\n\n\n\n\nplt.scatter(accuracies, accuracies1, marker = \"x\", color = \"deeppink\", s = 30)\nplt.xlabel(\"Accuracies for Decision Tree on Featurized Data\")\nplt.ylabel(\"Accuracies for Decision Tree on Raw Data\")\nplt.grid()"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#now-same-for-18-featured-dffeat",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#now-same-for-18-featured-dffeat",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Now same for \\(18\\) Featured dfFeat",
    "text": "Now same for \\(18\\) Featured dfFeat\n\nFirstly Featurize the Test Dataset according to the \\(18\\) features\n\ndfNF_test = Featuriser(X_test_TS, y_test, f_sel)\n\n\nmodel = DecisionTreeClassifier()\nclfg = model.fit(dfFeat.iloc[:, :-3], dfFeat.iloc[:, 18])\ny_pred = clfg.predict(dfNF_test.iloc[:, :-3])\ny_pred\n\narray([3, 3, 6, 2, 6, 4, 6, 1, 1, 2, 4, 6, 2, 5, 2, 4, 5, 5, 2, 6, 4, 1,\n       2, 5, 3, 2, 2, 4, 3, 4, 6, 6, 4, 3, 3, 1])\n\n\n\ny_test\n\narray([3, 3, 6, 2, 6, 5, 6, 1, 1, 3, 5, 6, 1, 5, 3, 4, 5, 5, 1, 6, 4, 1,\n       2, 5, 2, 1, 3, 6, 3, 4, 4, 4, 4, 2, 2, 2])\n\n\n\n\nAccuracy Score for decision tree classifier on TEST data trained on our 18 featurized dataset\n\naccuracy_score(y_test, y_pred)\n\n0.5833333333333334\n\n\n\n\nClassification Report for decision tree classifier on TEST data trained on our 18 featurized dataset\n\nprint(classification_report(y_test, y_pred, labels = np.unique(y_pred)))\n\n              precision    recall  f1-score   support\n\n           1       0.75      0.50      0.60         6\n           2       0.25      0.33      0.29         6\n           3       0.50      0.50      0.50         6\n           4       0.57      0.67      0.62         6\n           5       1.00      0.67      0.80         6\n           6       0.71      0.83      0.77         6\n\n    accuracy                           0.58        36\n   macro avg       0.63      0.58      0.60        36\nweighted avg       0.63      0.58      0.60        36\n\n\n\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\n\n\nconfMatrix(df_cm, flag = 1)\n\n\n\n\n\n\n\n\n\n\nFetching the Connfusion Matrices, Class Reports, Accuracies for Depth \\((2 - 8)\\) Tree on 18-Featurized Data\n\nconfusion_matrices, class_reports, class_reports_dict, accuracies = [], [], [], []\nfor i in range(2, 9):\n    model = DecisionTreeClassifier(max_depth = i, random_state = 42)\n    clfg = model.fit(dfFeat.iloc[:, :-3], dfFeat.iloc[:, 18])\n    y_pred = clfg.predict(dfNF_test.iloc[:, :-3])\n    \n    pred, actual = y_pred, y_test\n    \n    cm = confusion_matrix(actual, pred)\n    \n    confusion_matrices.append(pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes]))\n    class_reports.append(classification_report(actual, pred, labels = np.unique(pred)))\n    class_reports_dict.append(classification_report(actual, pred, labels = np.unique(pred), output_dict = True))\n    accuracies.append(accuracy_score(actual, pred))\n\n\n\n\\(7\\) Confusion Matrices for 18-Featurized Data\n\nconfMatrix(confusion_matrices, flag = 0, accuracies = accuracies)"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#accuracy-comparison-for-both-raw-timeseries-and-18-featurized-data",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#accuracy-comparison-for-both-raw-timeseries-and-18-featurized-data",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "Accuracy Comparison for both RAW TimeSeries and 18-Featurized Data",
    "text": "Accuracy Comparison for both RAW TimeSeries and 18-Featurized Data\n\nplt.plot(range(2, 9), accuracies1, color = \"r\", marker = \"o\")\nplt.plot(range(2, 9), accuracies, color = \"g\", marker = \"s\")\nplt.xlabel(\"Decision Tree Depth\")\nplt.ylabel(\"Accuracies\")\nplt.title(\"Accuracy Variation of Test-Train vs. Depth\")\nplt.legend([\"Raw Data\", \"Featurized Data\"])\nplt.grid()"
  },
  {
    "objectID": "posts/Feature-Extraction/FeaturesExtr.html#the-5---featured-dfnewfeaturized-is-better-than-the-18---featured-dffeat-that-had-spectral-features-included-too",
    "href": "posts/Feature-Extraction/FeaturesExtr.html#the-5---featured-dfnewfeaturized-is-better-than-the-18---featured-dffeat-that-had-spectral-features-included-too",
    "title": "Feature Extraction using TSFEL on UCI-HAR Dataset",
    "section": "The 5 - Featured dfNewFeaturized is better than the 18 - Featured dfFeat that had spectral features included too",
    "text": "The 5 - Featured dfNewFeaturized is better than the 18 - Featured dfFeat that had spectral features included too"
  },
  {
    "objectID": "posts/Time-Series/DataAnalysis_HrridayWork.html",
    "href": "posts/Time-Series/DataAnalysis_HrridayWork.html",
    "title": "Time Series data of UCI-HAR Dataset",
    "section": "",
    "text": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom latex import latexify, format_axes\nimport numpy as np\nimport tsfel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom MakeDataset import *\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\nDataset Generated from MakeDataset.py\n\nX_train,X_test,y_train,y_test\nX_test,X_val,y_test,y_val\n\n\nX_train\n\narray([[[ 0.9736077 , -0.1844755 , -0.2821974 ],\n        [ 0.9760866 , -0.1867793 , -0.2848794 ],\n        [ 0.977865  , -0.191836  , -0.2891687 ],\n        ...,\n        [ 0.9779202 , -0.1834941 , -0.2829651 ],\n        [ 0.9796224 , -0.1832831 , -0.279844  ],\n        [ 0.9775468 , -0.1833646 , -0.2764387 ]],\n\n       [[ 1.00564   , -0.1732591 , -0.2299191 ],\n        [ 1.006267  , -0.1727248 , -0.2516695 ],\n        [ 1.004331  , -0.1783138 , -0.2447012 ],\n        ...,\n        [ 0.9963187 , -0.165975  , -0.2166365 ],\n        [ 0.998345  , -0.1662256 , -0.2176124 ],\n        [ 1.00105   , -0.1642913 , -0.2210956 ]],\n\n       [[ 0.784794  , -0.2597323 , -0.2317497 ],\n        [ 0.8028195 , -0.2151319 , -0.2276441 ],\n        [ 0.7250539 , -0.2064177 , -0.2095281 ],\n        ...,\n        [ 0.6540971 , -0.140727  , -0.2860766 ],\n        [ 0.6268603 , -0.2748843 , -0.2455943 ],\n        [ 0.6052588 , -0.3292142 , -0.1952567 ]],\n\n       ...,\n\n       [[ 1.013856  , -0.08463204, -0.1833906 ],\n        [ 1.018295  , -0.08470217, -0.1755404 ],\n        [ 1.017008  , -0.08552211, -0.1765002 ],\n        ...,\n        [ 1.009988  , -0.09727326, -0.1556776 ],\n        [ 1.009527  , -0.1015205 , -0.1545634 ],\n        [ 1.012576  , -0.1036466 , -0.1521443 ]],\n\n       [[ 1.479893  , -0.4783501 , -0.1348317 ],\n        [ 1.417838  , -0.5264301 , -0.02401967],\n        [ 1.12661   , -0.4468493 , -0.05762023],\n        ...,\n        [ 0.8351439 , -0.1607123 , -0.1592067 ],\n        [ 0.7713394 , -0.1248221 , -0.1388356 ],\n        [ 0.7175624 , -0.1262066 , -0.1470366 ]],\n\n       [[ 0.7170605 , -0.02063687, -0.1085871 ],\n        [ 0.7705297 , -0.0618509 , -0.1241441 ],\n        [ 0.7802221 , -0.0469533 , -0.1191337 ],\n        ...,\n        [ 0.7315363 , -0.1621981 , -0.04988996],\n        [ 0.7622148 , -0.1765388 , -0.03800902],\n        [ 0.7644377 , -0.2050919 , -0.02824738]]])\n\n\nExtract \\(a_x, a_y, a_z\\) from X_train\n\naXYZ_Xtrain = X_train[:, :, 0], X_train[:, :, 1], X_train[:, :, 2]\n\nGet Total Acceleration \\((a_x^2 + a_y^2 + a_z^2)\\) Time Series from X_train, X_test, X_val\n\nX_train_TS = np.sum(np.square(X_train), axis = -1)\nX_test_TS = np.sum(np.square(X_test), axis = -1)\nX_val_TS = np.sum(np.square(X_val), axis = -1)\n\n\nprint(X_train_TS.shape, X_test_TS.shape, X_val_TS.shape)\n\n(108, 500) (36, 500) (36, 500)\n\n\n\ny_train\n\narray([5, 5, 2, 3, 6, 4, 1, 5, 5, 4, 3, 1, 6, 5, 4, 1, 5, 1, 4, 6, 2, 4,\n       6, 3, 6, 1, 1, 6, 6, 2, 5, 1, 4, 2, 6, 4, 3, 3, 6, 4, 2, 3, 6, 3,\n       5, 5, 3, 1, 3, 1, 4, 6, 4, 5, 4, 4, 3, 4, 2, 4, 2, 1, 2, 5, 4, 2,\n       5, 2, 3, 6, 1, 3, 2, 2, 1, 3, 6, 2, 5, 1, 3, 2, 4, 5, 4, 2, 1, 1,\n       1, 3, 5, 5, 6, 1, 4, 6, 6, 5, 3, 3, 2, 6, 6, 3, 1, 5, 2, 2])\n\n\nThe Sort Order\n\ndf = pd.DataFrame(X_train_TS)\ndf[\"Label\"] = y_train\ndf.sort_values(by = \"Label\", inplace = True)\nS = np.array(df.index)\nS\n\narray([ 86,  26,  25,  87,  93,  47,  49,  17,  15,  31,  61,  88,  70,\n        74,  79,   6,  11, 104,  33,  73,  72,  40,  62, 106,  58,  67,\n        65,  81,  85,  60, 107,  77,   2, 100,  29,  20,  75,   3,  71,\n       103,  68,  80,  10,  99,  98,  56,  48,  89,  41,  36,  37,  43,\n        23,  46,  32,  35,   5,  39,   9,  64,  21,  54,  59,  57,  55,\n        84,  18,  52,  82,  50,  94,  14,  97,  78,  91,  90, 105,  83,\n         0,  53,  66,  63,   1,  45,  44,   7,  30,   8,  13,  16,   4,\n       102, 101,  12,  96,  69,  19,  76,  92,  24,  27,  28,  34,  38,\n        42,  51,  95,  22], dtype=int64)\n\n\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLabel\n\n\n\n\n86\n0.714053\n0.732246\n0.768960\n0.853281\n0.950569\n1.001927\n1.025067\n1.039051\n1.033828\n1.021974\n...\n1.477209\n1.245564\n0.884933\n0.751383\n0.895341\n1.111554\n1.159284\n1.061964\n0.962311\n1\n\n\n26\n0.751970\n0.651734\n0.579656\n0.587043\n0.606218\n0.727218\n0.797291\n0.794672\n0.834353\n0.888909\n...\n0.396283\n0.573048\n0.702095\n0.827120\n0.867516\n0.805562\n0.813949\n0.771488\n0.791860\n1\n\n\n25\n0.596975\n0.577279\n0.564058\n0.773096\n0.871679\n1.057734\n1.427447\n1.728494\n1.920396\n1.673412\n...\n0.961942\n1.053653\n1.100745\n1.275938\n1.522039\n1.531372\n1.548665\n1.541446\n1.412323\n1\n\n\n87\n1.447083\n1.243813\n0.999385\n0.811583\n0.683452\n0.727751\n0.759291\n0.818850\n0.947241\n0.960450\n...\n1.395899\n1.404201\n1.532946\n2.060462\n2.916117\n3.367976\n2.650901\n1.300202\n0.516547\n1\n\n\n93\n0.678714\n0.700478\n1.148305\n2.166170\n2.921367\n2.662965\n1.954290\n1.440979\n1.186594\n1.258637\n...\n1.112212\n1.326475\n1.516445\n1.761912\n2.096122\n1.985628\n1.334452\n0.939196\n0.868019\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n38\n0.996194\n0.991963\n0.984747\n0.987995\n0.992127\n0.994089\n0.995440\n0.998901\n1.003637\n1.002344\n...\n1.010409\n1.008286\n1.004564\n1.001250\n0.997999\n0.996779\n0.991216\n0.983254\n0.985567\n6\n\n\n42\n1.030783\n1.005243\n0.994559\n0.997765\n1.005931\n1.004629\n0.970352\n0.929467\n0.959656\n1.012440\n...\n0.978252\n1.000408\n1.027792\n1.028454\n1.006944\n0.987428\n0.983824\n0.990666\n0.990392\n6\n\n\n51\n1.017841\n1.016995\n1.024412\n1.029757\n1.026955\n1.028478\n1.026419\n1.026562\n1.034466\n1.037380\n...\n1.035634\n1.040731\n1.038952\n1.027263\n1.011913\n1.013323\n1.020733\n1.019209\n1.022797\n6\n\n\n95\n1.003037\n1.000143\n1.002048\n0.993890\n0.997013\n1.012518\n1.025586\n1.026735\n1.021032\n1.016388\n...\n1.010742\n1.016365\n1.022836\n1.018828\n1.014693\n1.025160\n1.033535\n1.029625\n1.023284\n6\n\n\n22\n1.027639\n1.033972\n1.033586\n1.024781\n1.022987\n1.022073\n1.025497\n1.025356\n1.019591\n1.021240\n...\n1.031073\n1.032912\n1.032843\n1.028517\n1.027242\n1.030718\n1.032342\n1.033870\n1.031274\n6\n\n\n\n\n108 rows × 501 columns\n\n\n\nSorting the \\(a_x, a_y, a_z\\) also as according to the Label sort index\n\naXYZ_Xtrain = aXYZ_Xtrain[0][S], aXYZ_Xtrain[1][S], aXYZ_Xtrain[2][S]\n\nSort the total acceleration \\((a_x^2 + a_y^2 + a_z^2)\\) according to label sort index and get the FINAL TIME SERIES for all test data subjects \\(108\\)\n\ndf_Xtrain = pd.DataFrame(X_train_TS)\ndf_Xtrain[\"Label\"] = y_train\ndf_Xtrain.sort_values(by = \"Label\", inplace = True)\ndf_Xtrain.set_index(pd.Series(range(108)), inplace = True)\ndf_Xtrain\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLabel\n\n\n\n\n0\n0.714053\n0.732246\n0.768960\n0.853281\n0.950569\n1.001927\n1.025067\n1.039051\n1.033828\n1.021974\n...\n1.477209\n1.245564\n0.884933\n0.751383\n0.895341\n1.111554\n1.159284\n1.061964\n0.962311\n1\n\n\n1\n0.751970\n0.651734\n0.579656\n0.587043\n0.606218\n0.727218\n0.797291\n0.794672\n0.834353\n0.888909\n...\n0.396283\n0.573048\n0.702095\n0.827120\n0.867516\n0.805562\n0.813949\n0.771488\n0.791860\n1\n\n\n2\n0.596975\n0.577279\n0.564058\n0.773096\n0.871679\n1.057734\n1.427447\n1.728494\n1.920396\n1.673412\n...\n0.961942\n1.053653\n1.100745\n1.275938\n1.522039\n1.531372\n1.548665\n1.541446\n1.412323\n1\n\n\n3\n1.447083\n1.243813\n0.999385\n0.811583\n0.683452\n0.727751\n0.759291\n0.818850\n0.947241\n0.960450\n...\n1.395899\n1.404201\n1.532946\n2.060462\n2.916117\n3.367976\n2.650901\n1.300202\n0.516547\n1\n\n\n4\n0.678714\n0.700478\n1.148305\n2.166170\n2.921367\n2.662965\n1.954290\n1.440979\n1.186594\n1.258637\n...\n1.112212\n1.326475\n1.516445\n1.761912\n2.096122\n1.985628\n1.334452\n0.939196\n0.868019\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n103\n0.996194\n0.991963\n0.984747\n0.987995\n0.992127\n0.994089\n0.995440\n0.998901\n1.003637\n1.002344\n...\n1.010409\n1.008286\n1.004564\n1.001250\n0.997999\n0.996779\n0.991216\n0.983254\n0.985567\n6\n\n\n104\n1.030783\n1.005243\n0.994559\n0.997765\n1.005931\n1.004629\n0.970352\n0.929467\n0.959656\n1.012440\n...\n0.978252\n1.000408\n1.027792\n1.028454\n1.006944\n0.987428\n0.983824\n0.990666\n0.990392\n6\n\n\n105\n1.017841\n1.016995\n1.024412\n1.029757\n1.026955\n1.028478\n1.026419\n1.026562\n1.034466\n1.037380\n...\n1.035634\n1.040731\n1.038952\n1.027263\n1.011913\n1.013323\n1.020733\n1.019209\n1.022797\n6\n\n\n106\n1.003037\n1.000143\n1.002048\n0.993890\n0.997013\n1.012518\n1.025586\n1.026735\n1.021032\n1.016388\n...\n1.010742\n1.016365\n1.022836\n1.018828\n1.014693\n1.025160\n1.033535\n1.029625\n1.023284\n6\n\n\n107\n1.027639\n1.033972\n1.033586\n1.024781\n1.022987\n1.022073\n1.025497\n1.025356\n1.019591\n1.021240\n...\n1.031073\n1.032912\n1.032843\n1.028517\n1.027242\n1.030718\n1.032342\n1.033870\n1.031274\n6\n\n\n\n\n108 rows × 501 columns\n\n\n\n\ndf_Xtrain[\"Label\"].value_counts()\n\nLabel\n1    18\n2    18\n3    18\n4    18\n5    18\n6    18\nName: count, dtype: int64\n\n\n\nclasses\n\n{'WALKING': 1,\n 'WALKING_UPSTAIRS': 2,\n 'WALKING_DOWNSTAIRS': 3,\n 'SITTING': 4,\n 'STANDING': 5,\n 'LAYING': 6}\n\n\n\nclassesN = {1 : 'WALKING', 2 : 'WALKING_UPSTAIRS', 3 : 'WALKING_DOWNSTAIRS', 4 : 'SITTING', 5 : 'STANDING', 6 : 'LAYING'}\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}\n\n\n\n\n\n\n\nPlot of each \\((a_x, a_y, a_z)\\) for all subjects $ (a_x, a_y, a_z) = [, , ] $\n\nfig, axes = plt.subplots(18, 6, figsize = (25, 27.5))\nfig.suptitle(r\"Time Series of Acceleration $(acc_x, acc_y, acc_z)$\", size = 24)\nsortedY_train = np.array(df_Xtrain[\"Label\"])\ncolors = [\"red\", \"green\", \"blue\"]\nc = 0\nfor i in range(6):\n    for j in range(18):\n        for k in range(3):\n            time_series = aXYZ_Xtrain[k][c]\n            axes[j, i].plot(time_series, color = colors[k], linewidth = 3)\n            axes[j, i].set_title(f\"Subject {c + 1}, Class = {classesN[sortedY_train[c]]}\", fontsize = 9)\n            axes[j, i].set_ylabel(r\"Acceleration in $ms^{-2}/g$\", fontsize = 6)\n            axes[j, i].set_xlabel(\"Time Samples\", fontsize = 6)\n        c += 1\n\n\n# plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n# plt.savefig(\"Activity_Individual.png\")\n# plt.close()\n\nAnswer: #### 1. From the above plots, one can easily differentiate between Static Activities (like Laying, Sitting, Standing) and Dynamic Activities (Walking, Walking Downstairs, Walking Upstairs) as the Dynamic Activities are more volatile and seem to have greater variance as compared to the Static Activities.\n\n\n\n\n\n\n\n\n\n\nlatexify()\nfig, axes = plt.subplots(18, 6, figsize = (25, 27.5))\nfig.suptitle(r\"Time Series of Total Acceleration $(acc_x^2 + acc_y^2 + acc_z^2)$\", size = 24)\nsortedY_train = np.array(df_Xtrain[\"Label\"])\ncolors = [\"red\", \"deeppink\", \"purple\", \"teal\", \"green\", \"blue\"]\nc = 0\nfor i in range(6):\n    for j in range(18):\n        time_series = df_Xtrain.iloc[c, : -1]\n        axes[j, i].plot(time_series, color = colors[i], linewidth = 4)\n        axes[j, i].set_title(f\"Subject {c + 1}, Class = {classesN[sortedY_train[c]]}\", fontsize = 9)\n        axes[j, i].set_ylabel(r\"Total Acceleration in  $m^{2}s^{-4}/g^2$\", fontsize = 6)\n        axes[j, i].set_xlabel(\"Time Samples\", fontsize = 6)\n        c += 1\n\n# plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n# plt.savefig(\"Activity_Tot.png\")\n# plt.close()\n\nAnswer: #### 1. For differentiating between Static and Dynamic Activities, implementation of a machine learning model may not be necessary but it may be a sufficient condition.\n\n\n\n\n\n\n\n\n\n\n\n\nRaw Time Series for \\(108\\) Training Subjects and \\(500\\) features/samples\n\nmodel = DecisionTreeClassifier(random_state=42)\nclfg = model.fit(X_train_TS, y_train)\ny_pred = clfg.predict(X_test_TS)\ny_pred\n\narray([6, 1, 6, 1, 6, 5, 6, 1, 1, 1, 4, 6, 6, 5, 2, 6, 5, 5, 3, 6, 4, 1,\n       4, 5, 2, 1, 3, 6, 1, 4, 6, 6, 6, 1, 1, 3])\n\n\nClassification Report\n\\(\\text{Accuracy} = \\frac{||y = \\hat{y}||}{||y||}\\)\n\\(\\text{Precision} = \\frac{||y = \\hat{y} = \\text{Class}||}{||\\hat{y} = \\text{Class}||} = \\frac{\\text{T.P.}}{\\text{T.P.} + \\text{F.P.}}\\)\n\\(\\text{Recall} = \\frac{||y = \\hat{y} = \\text{Class}||}{||y = \\text{Class}||} = \\frac{\\text{T.P.}}{\\text{T.P.} + \\text{F.N.}}\\)\n\\(\\text{F-Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.40      0.67      0.50         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.50      0.33      0.40         6\n           5       1.00      0.83      0.91         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.53        36\n   macro avg       0.54      0.53      0.49        36\nweighted avg       0.54      0.53      0.49        36\n\n\n\nConfusion Matrix\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\n\nConfusion Matrix Plot Handler\n\n## flag = 1 for a single plot and 0 for subplots for 2 - 8 depths\ndef confMatrix(dataFrame, flag = 1, accuracies = None):\n    if flag:\n        plt.figure(figsize = (15, 15))\n        ax = sns.heatmap(dataFrame, annot = True, cmap = \"PuBu\")\n        plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 8)\n        plt.setp(ax.get_yticklabels(), fontsize = 8)\n        plt.ylabel(\"True label\", fontsize = 18)\n        plt.xlabel(\"Predicted label\", fontsize = 18)\n        plt.title(f\"Accuracy = {accuracy_score(y_test, y_pred)*100: .4f}%\", fontweight = \"bold\", fontsize = 13)\n        plt.savefig(\"Single_ConfusionM.png\")\n        plt.show()\n    else:\n        fig, axes = plt.subplots(3, 3, figsize = (25, 25))\n        axes = axes.flatten()\n\n        for i, df in enumerate(dataFrame):\n            ax = sns.heatmap(df, annot = True, ax = axes[i], cbar = False, cmap = \"PuBu\")\n            \n            plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 6)\n            plt.setp(ax.get_yticklabels(), fontsize = 8)\n            ax.set_title(f\"Depth = {i + 2}\\nAccuracy = {accuracies[i] * 100: .4f}%\", fontsize = 10)\n            ax.set_ylabel(\"True label\", fontsize = 12)\n            ax.set_xlabel(\"Predicted label\", fontsize = 12)\n            \n        plt.delaxes(axes[7])\n        plt.delaxes(axes[8])\n        plt.tight_layout()\n        plt.subplots_adjust(wspace = 1.1, hspace = 1.1)\n        plt.savefig(\"DepthConfusionM.png\")\n        plt.show()\n\nConfusion Matrix Image\n\nconfMatrix(df_cm, 1)\n\nAnswer: #### 1. The overall performance of the Decision Tree Classifier model trained on the raw total acceleration \\((acc_x^2 + acc_y^2 + acc_z^2)\\) dataset was on average barely better than a fair coin toss.\n\n\n\n\n\n\nThe model performed the best in classifying the activity STANDING, while it mispredicted LAYING as SITTING.\nOn the contrary, classifications among the Dynamic Activities showed confusion.\n\n\n\n\n\n\n\n\nVarying Tree Depths \\((2 - 8)\\) and looking at the results for each depth\n\nconfusion_matrices, class_reports, class_reports_dict, accuracies = [], [], [], []\nfor i in range(2, 9):\n    model = DecisionTreeClassifier(max_depth = i,random_state=42)\n    clfg = model.fit(X_train_TS, y_train)\n    y_pred = clfg.predict(X_test_TS)\n    \n    pred, actual = y_pred, y_test\n    \n    cm = confusion_matrix(actual, pred)\n    \n    confusion_matrices.append(pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes]))\n    class_reports.append(classification_report(actual, pred, labels = np.unique(pred)))\n    class_reports_dict.append(classification_report(actual, pred, labels = np.unique(pred), output_dict = True))\n    accuracies.append(accuracy_score(actual, pred))\n\nConfusion Matrix Image (for varying tree depths)\n\nconfMatrix(confusion_matrices, 0, accuracies = accuracies)\n\nClassification reports\n\nfor i in range(2,9):\n    print(f'Depth = {i}\\n{class_reports[i-2]}\\n\\n')\n\nDepth = 2\n              precision    recall  f1-score   support\n\n           1       0.38      0.50      0.43         6\n           3       0.50      0.17      0.25         6\n           5       0.75      1.00      0.86         6\n           6       0.33      1.00      0.50         6\n\n   micro avg       0.44      0.67      0.53        24\n   macro avg       0.49      0.67      0.51        24\nweighted avg       0.49      0.67      0.51        24\n\n\n\nDepth = 3\n              precision    recall  f1-score   support\n\n           1       0.50      0.50      0.50         6\n           2       1.00      0.17      0.29         6\n           3       0.50      0.33      0.40         6\n           4       0.29      0.33      0.31         6\n           5       0.86      1.00      0.92         6\n           6       0.55      1.00      0.71         6\n\n    accuracy                           0.56        36\n   macro avg       0.61      0.56      0.52        36\nweighted avg       0.61      0.56      0.52        36\n\n\n\nDepth = 4\n              precision    recall  f1-score   support\n\n           1       0.50      0.67      0.57         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.29      0.33      0.31         6\n           5       1.00      0.83      0.91         6\n           6       0.55      1.00      0.71         6\n\n    accuracy                           0.53        36\n   macro avg       0.53      0.53      0.49        36\nweighted avg       0.53      0.53      0.49        36\n\n\n\nDepth = 5\n              precision    recall  f1-score   support\n\n           1       0.45      0.83      0.59         6\n           2       0.50      0.17      0.25         6\n           3       0.50      0.17      0.25         6\n           4       0.40      0.33      0.36         6\n           5       1.00      0.83      0.91         6\n           6       0.55      1.00      0.71         6\n\n    accuracy                           0.56        36\n   macro avg       0.57      0.56      0.51        36\nweighted avg       0.57      0.56      0.51        36\n\n\n\nDepth = 6\n              precision    recall  f1-score   support\n\n           1       0.56      0.83      0.67         6\n           2       0.33      0.17      0.22         6\n           3       0.50      0.17      0.25         6\n           4       0.50      0.33      0.40         6\n           5       0.83      0.83      0.83         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.56        36\n   macro avg       0.54      0.56      0.51        36\nweighted avg       0.54      0.56      0.51        36\n\n\n\nDepth = 7\n              precision    recall  f1-score   support\n\n           1       0.40      0.67      0.50         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.50      0.33      0.40         6\n           5       1.00      0.83      0.91         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.53        36\n   macro avg       0.54      0.53      0.49        36\nweighted avg       0.54      0.53      0.49        36\n\n\n\nDepth = 8\n              precision    recall  f1-score   support\n\n           1       0.40      0.67      0.50         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.50      0.33      0.40         6\n           5       1.00      0.83      0.91         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.53        36\n   macro avg       0.54      0.53      0.49        36\nweighted avg       0.54      0.53      0.49        36\n\n\n\n\n\nAnswer: #### 1. From the classification reports, one can infer that the accuracy of classifying activities is in general increasing."
  },
  {
    "objectID": "posts/Time-Series/DataAnalysis_HrridayWork.html#importing-libraries-and-preprocessing",
    "href": "posts/Time-Series/DataAnalysis_HrridayWork.html#importing-libraries-and-preprocessing",
    "title": "Time Series data of UCI-HAR Dataset",
    "section": "",
    "text": "from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom latex import latexify, format_axes\nimport numpy as np\nimport tsfel\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nimport graphviz\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns\nfrom MakeDataset import *\n%matplotlib inline\n# Retina\n%config InlineBackend.figure_format = 'retina'\n\nDataset Generated from MakeDataset.py\n\nX_train,X_test,y_train,y_test\nX_test,X_val,y_test,y_val\n\n\nX_train\n\narray([[[ 0.9736077 , -0.1844755 , -0.2821974 ],\n        [ 0.9760866 , -0.1867793 , -0.2848794 ],\n        [ 0.977865  , -0.191836  , -0.2891687 ],\n        ...,\n        [ 0.9779202 , -0.1834941 , -0.2829651 ],\n        [ 0.9796224 , -0.1832831 , -0.279844  ],\n        [ 0.9775468 , -0.1833646 , -0.2764387 ]],\n\n       [[ 1.00564   , -0.1732591 , -0.2299191 ],\n        [ 1.006267  , -0.1727248 , -0.2516695 ],\n        [ 1.004331  , -0.1783138 , -0.2447012 ],\n        ...,\n        [ 0.9963187 , -0.165975  , -0.2166365 ],\n        [ 0.998345  , -0.1662256 , -0.2176124 ],\n        [ 1.00105   , -0.1642913 , -0.2210956 ]],\n\n       [[ 0.784794  , -0.2597323 , -0.2317497 ],\n        [ 0.8028195 , -0.2151319 , -0.2276441 ],\n        [ 0.7250539 , -0.2064177 , -0.2095281 ],\n        ...,\n        [ 0.6540971 , -0.140727  , -0.2860766 ],\n        [ 0.6268603 , -0.2748843 , -0.2455943 ],\n        [ 0.6052588 , -0.3292142 , -0.1952567 ]],\n\n       ...,\n\n       [[ 1.013856  , -0.08463204, -0.1833906 ],\n        [ 1.018295  , -0.08470217, -0.1755404 ],\n        [ 1.017008  , -0.08552211, -0.1765002 ],\n        ...,\n        [ 1.009988  , -0.09727326, -0.1556776 ],\n        [ 1.009527  , -0.1015205 , -0.1545634 ],\n        [ 1.012576  , -0.1036466 , -0.1521443 ]],\n\n       [[ 1.479893  , -0.4783501 , -0.1348317 ],\n        [ 1.417838  , -0.5264301 , -0.02401967],\n        [ 1.12661   , -0.4468493 , -0.05762023],\n        ...,\n        [ 0.8351439 , -0.1607123 , -0.1592067 ],\n        [ 0.7713394 , -0.1248221 , -0.1388356 ],\n        [ 0.7175624 , -0.1262066 , -0.1470366 ]],\n\n       [[ 0.7170605 , -0.02063687, -0.1085871 ],\n        [ 0.7705297 , -0.0618509 , -0.1241441 ],\n        [ 0.7802221 , -0.0469533 , -0.1191337 ],\n        ...,\n        [ 0.7315363 , -0.1621981 , -0.04988996],\n        [ 0.7622148 , -0.1765388 , -0.03800902],\n        [ 0.7644377 , -0.2050919 , -0.02824738]]])\n\n\nExtract \\(a_x, a_y, a_z\\) from X_train\n\naXYZ_Xtrain = X_train[:, :, 0], X_train[:, :, 1], X_train[:, :, 2]\n\nGet Total Acceleration \\((a_x^2 + a_y^2 + a_z^2)\\) Time Series from X_train, X_test, X_val\n\nX_train_TS = np.sum(np.square(X_train), axis = -1)\nX_test_TS = np.sum(np.square(X_test), axis = -1)\nX_val_TS = np.sum(np.square(X_val), axis = -1)\n\n\nprint(X_train_TS.shape, X_test_TS.shape, X_val_TS.shape)\n\n(108, 500) (36, 500) (36, 500)\n\n\n\ny_train\n\narray([5, 5, 2, 3, 6, 4, 1, 5, 5, 4, 3, 1, 6, 5, 4, 1, 5, 1, 4, 6, 2, 4,\n       6, 3, 6, 1, 1, 6, 6, 2, 5, 1, 4, 2, 6, 4, 3, 3, 6, 4, 2, 3, 6, 3,\n       5, 5, 3, 1, 3, 1, 4, 6, 4, 5, 4, 4, 3, 4, 2, 4, 2, 1, 2, 5, 4, 2,\n       5, 2, 3, 6, 1, 3, 2, 2, 1, 3, 6, 2, 5, 1, 3, 2, 4, 5, 4, 2, 1, 1,\n       1, 3, 5, 5, 6, 1, 4, 6, 6, 5, 3, 3, 2, 6, 6, 3, 1, 5, 2, 2])\n\n\nThe Sort Order\n\ndf = pd.DataFrame(X_train_TS)\ndf[\"Label\"] = y_train\ndf.sort_values(by = \"Label\", inplace = True)\nS = np.array(df.index)\nS\n\narray([ 86,  26,  25,  87,  93,  47,  49,  17,  15,  31,  61,  88,  70,\n        74,  79,   6,  11, 104,  33,  73,  72,  40,  62, 106,  58,  67,\n        65,  81,  85,  60, 107,  77,   2, 100,  29,  20,  75,   3,  71,\n       103,  68,  80,  10,  99,  98,  56,  48,  89,  41,  36,  37,  43,\n        23,  46,  32,  35,   5,  39,   9,  64,  21,  54,  59,  57,  55,\n        84,  18,  52,  82,  50,  94,  14,  97,  78,  91,  90, 105,  83,\n         0,  53,  66,  63,   1,  45,  44,   7,  30,   8,  13,  16,   4,\n       102, 101,  12,  96,  69,  19,  76,  92,  24,  27,  28,  34,  38,\n        42,  51,  95,  22], dtype=int64)\n\n\n\ndf\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLabel\n\n\n\n\n86\n0.714053\n0.732246\n0.768960\n0.853281\n0.950569\n1.001927\n1.025067\n1.039051\n1.033828\n1.021974\n...\n1.477209\n1.245564\n0.884933\n0.751383\n0.895341\n1.111554\n1.159284\n1.061964\n0.962311\n1\n\n\n26\n0.751970\n0.651734\n0.579656\n0.587043\n0.606218\n0.727218\n0.797291\n0.794672\n0.834353\n0.888909\n...\n0.396283\n0.573048\n0.702095\n0.827120\n0.867516\n0.805562\n0.813949\n0.771488\n0.791860\n1\n\n\n25\n0.596975\n0.577279\n0.564058\n0.773096\n0.871679\n1.057734\n1.427447\n1.728494\n1.920396\n1.673412\n...\n0.961942\n1.053653\n1.100745\n1.275938\n1.522039\n1.531372\n1.548665\n1.541446\n1.412323\n1\n\n\n87\n1.447083\n1.243813\n0.999385\n0.811583\n0.683452\n0.727751\n0.759291\n0.818850\n0.947241\n0.960450\n...\n1.395899\n1.404201\n1.532946\n2.060462\n2.916117\n3.367976\n2.650901\n1.300202\n0.516547\n1\n\n\n93\n0.678714\n0.700478\n1.148305\n2.166170\n2.921367\n2.662965\n1.954290\n1.440979\n1.186594\n1.258637\n...\n1.112212\n1.326475\n1.516445\n1.761912\n2.096122\n1.985628\n1.334452\n0.939196\n0.868019\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n38\n0.996194\n0.991963\n0.984747\n0.987995\n0.992127\n0.994089\n0.995440\n0.998901\n1.003637\n1.002344\n...\n1.010409\n1.008286\n1.004564\n1.001250\n0.997999\n0.996779\n0.991216\n0.983254\n0.985567\n6\n\n\n42\n1.030783\n1.005243\n0.994559\n0.997765\n1.005931\n1.004629\n0.970352\n0.929467\n0.959656\n1.012440\n...\n0.978252\n1.000408\n1.027792\n1.028454\n1.006944\n0.987428\n0.983824\n0.990666\n0.990392\n6\n\n\n51\n1.017841\n1.016995\n1.024412\n1.029757\n1.026955\n1.028478\n1.026419\n1.026562\n1.034466\n1.037380\n...\n1.035634\n1.040731\n1.038952\n1.027263\n1.011913\n1.013323\n1.020733\n1.019209\n1.022797\n6\n\n\n95\n1.003037\n1.000143\n1.002048\n0.993890\n0.997013\n1.012518\n1.025586\n1.026735\n1.021032\n1.016388\n...\n1.010742\n1.016365\n1.022836\n1.018828\n1.014693\n1.025160\n1.033535\n1.029625\n1.023284\n6\n\n\n22\n1.027639\n1.033972\n1.033586\n1.024781\n1.022987\n1.022073\n1.025497\n1.025356\n1.019591\n1.021240\n...\n1.031073\n1.032912\n1.032843\n1.028517\n1.027242\n1.030718\n1.032342\n1.033870\n1.031274\n6\n\n\n\n\n108 rows × 501 columns\n\n\n\nSorting the \\(a_x, a_y, a_z\\) also as according to the Label sort index\n\naXYZ_Xtrain = aXYZ_Xtrain[0][S], aXYZ_Xtrain[1][S], aXYZ_Xtrain[2][S]\n\nSort the total acceleration \\((a_x^2 + a_y^2 + a_z^2)\\) according to label sort index and get the FINAL TIME SERIES for all test data subjects \\(108\\)\n\ndf_Xtrain = pd.DataFrame(X_train_TS)\ndf_Xtrain[\"Label\"] = y_train\ndf_Xtrain.sort_values(by = \"Label\", inplace = True)\ndf_Xtrain.set_index(pd.Series(range(108)), inplace = True)\ndf_Xtrain\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n491\n492\n493\n494\n495\n496\n497\n498\n499\nLabel\n\n\n\n\n0\n0.714053\n0.732246\n0.768960\n0.853281\n0.950569\n1.001927\n1.025067\n1.039051\n1.033828\n1.021974\n...\n1.477209\n1.245564\n0.884933\n0.751383\n0.895341\n1.111554\n1.159284\n1.061964\n0.962311\n1\n\n\n1\n0.751970\n0.651734\n0.579656\n0.587043\n0.606218\n0.727218\n0.797291\n0.794672\n0.834353\n0.888909\n...\n0.396283\n0.573048\n0.702095\n0.827120\n0.867516\n0.805562\n0.813949\n0.771488\n0.791860\n1\n\n\n2\n0.596975\n0.577279\n0.564058\n0.773096\n0.871679\n1.057734\n1.427447\n1.728494\n1.920396\n1.673412\n...\n0.961942\n1.053653\n1.100745\n1.275938\n1.522039\n1.531372\n1.548665\n1.541446\n1.412323\n1\n\n\n3\n1.447083\n1.243813\n0.999385\n0.811583\n0.683452\n0.727751\n0.759291\n0.818850\n0.947241\n0.960450\n...\n1.395899\n1.404201\n1.532946\n2.060462\n2.916117\n3.367976\n2.650901\n1.300202\n0.516547\n1\n\n\n4\n0.678714\n0.700478\n1.148305\n2.166170\n2.921367\n2.662965\n1.954290\n1.440979\n1.186594\n1.258637\n...\n1.112212\n1.326475\n1.516445\n1.761912\n2.096122\n1.985628\n1.334452\n0.939196\n0.868019\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n103\n0.996194\n0.991963\n0.984747\n0.987995\n0.992127\n0.994089\n0.995440\n0.998901\n1.003637\n1.002344\n...\n1.010409\n1.008286\n1.004564\n1.001250\n0.997999\n0.996779\n0.991216\n0.983254\n0.985567\n6\n\n\n104\n1.030783\n1.005243\n0.994559\n0.997765\n1.005931\n1.004629\n0.970352\n0.929467\n0.959656\n1.012440\n...\n0.978252\n1.000408\n1.027792\n1.028454\n1.006944\n0.987428\n0.983824\n0.990666\n0.990392\n6\n\n\n105\n1.017841\n1.016995\n1.024412\n1.029757\n1.026955\n1.028478\n1.026419\n1.026562\n1.034466\n1.037380\n...\n1.035634\n1.040731\n1.038952\n1.027263\n1.011913\n1.013323\n1.020733\n1.019209\n1.022797\n6\n\n\n106\n1.003037\n1.000143\n1.002048\n0.993890\n0.997013\n1.012518\n1.025586\n1.026735\n1.021032\n1.016388\n...\n1.010742\n1.016365\n1.022836\n1.018828\n1.014693\n1.025160\n1.033535\n1.029625\n1.023284\n6\n\n\n107\n1.027639\n1.033972\n1.033586\n1.024781\n1.022987\n1.022073\n1.025497\n1.025356\n1.019591\n1.021240\n...\n1.031073\n1.032912\n1.032843\n1.028517\n1.027242\n1.030718\n1.032342\n1.033870\n1.031274\n6\n\n\n\n\n108 rows × 501 columns\n\n\n\n\ndf_Xtrain[\"Label\"].value_counts()\n\nLabel\n1    18\n2    18\n3    18\n4    18\n5    18\n6    18\nName: count, dtype: int64\n\n\n\nclasses\n\n{'WALKING': 1,\n 'WALKING_UPSTAIRS': 2,\n 'WALKING_DOWNSTAIRS': 3,\n 'SITTING': 4,\n 'STANDING': 5,\n 'LAYING': 6}\n\n\n\nclassesN = {1 : 'WALKING', 2 : 'WALKING_UPSTAIRS', 3 : 'WALKING_DOWNSTAIRS', 4 : 'SITTING', 5 : 'STANDING', 6 : 'LAYING'}\nclassesN\n\n{1: 'WALKING',\n 2: 'WALKING_UPSTAIRS',\n 3: 'WALKING_DOWNSTAIRS',\n 4: 'SITTING',\n 5: 'STANDING',\n 6: 'LAYING'}"
  },
  {
    "objectID": "posts/Time-Series/DataAnalysis_HrridayWork.html#questionstasks",
    "href": "posts/Time-Series/DataAnalysis_HrridayWork.html#questionstasks",
    "title": "Time Series data of UCI-HAR Dataset",
    "section": "",
    "text": "Plot of each \\((a_x, a_y, a_z)\\) for all subjects $ (a_x, a_y, a_z) = [, , ] $\n\nfig, axes = plt.subplots(18, 6, figsize = (25, 27.5))\nfig.suptitle(r\"Time Series of Acceleration $(acc_x, acc_y, acc_z)$\", size = 24)\nsortedY_train = np.array(df_Xtrain[\"Label\"])\ncolors = [\"red\", \"green\", \"blue\"]\nc = 0\nfor i in range(6):\n    for j in range(18):\n        for k in range(3):\n            time_series = aXYZ_Xtrain[k][c]\n            axes[j, i].plot(time_series, color = colors[k], linewidth = 3)\n            axes[j, i].set_title(f\"Subject {c + 1}, Class = {classesN[sortedY_train[c]]}\", fontsize = 9)\n            axes[j, i].set_ylabel(r\"Acceleration in $ms^{-2}/g$\", fontsize = 6)\n            axes[j, i].set_xlabel(\"Time Samples\", fontsize = 6)\n        c += 1\n\n\n# plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n# plt.savefig(\"Activity_Individual.png\")\n# plt.close()\n\nAnswer: #### 1. From the above plots, one can easily differentiate between Static Activities (like Laying, Sitting, Standing) and Dynamic Activities (Walking, Walking Downstairs, Walking Upstairs) as the Dynamic Activities are more volatile and seem to have greater variance as compared to the Static Activities.\n\n\n\n\n\n\n\n\n\n\nlatexify()\nfig, axes = plt.subplots(18, 6, figsize = (25, 27.5))\nfig.suptitle(r\"Time Series of Total Acceleration $(acc_x^2 + acc_y^2 + acc_z^2)$\", size = 24)\nsortedY_train = np.array(df_Xtrain[\"Label\"])\ncolors = [\"red\", \"deeppink\", \"purple\", \"teal\", \"green\", \"blue\"]\nc = 0\nfor i in range(6):\n    for j in range(18):\n        time_series = df_Xtrain.iloc[c, : -1]\n        axes[j, i].plot(time_series, color = colors[i], linewidth = 4)\n        axes[j, i].set_title(f\"Subject {c + 1}, Class = {classesN[sortedY_train[c]]}\", fontsize = 9)\n        axes[j, i].set_ylabel(r\"Total Acceleration in  $m^{2}s^{-4}/g^2$\", fontsize = 6)\n        axes[j, i].set_xlabel(\"Time Samples\", fontsize = 6)\n        c += 1\n\n# plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n# plt.savefig(\"Activity_Tot.png\")\n# plt.close()\n\nAnswer: #### 1. For differentiating between Static and Dynamic Activities, implementation of a machine learning model may not be necessary but it may be a sufficient condition.\n\n\n\n\n\n\n\n\n\n\n\n\nRaw Time Series for \\(108\\) Training Subjects and \\(500\\) features/samples\n\nmodel = DecisionTreeClassifier(random_state=42)\nclfg = model.fit(X_train_TS, y_train)\ny_pred = clfg.predict(X_test_TS)\ny_pred\n\narray([6, 1, 6, 1, 6, 5, 6, 1, 1, 1, 4, 6, 6, 5, 2, 6, 5, 5, 3, 6, 4, 1,\n       4, 5, 2, 1, 3, 6, 1, 4, 6, 6, 6, 1, 1, 3])\n\n\nClassification Report\n\\(\\text{Accuracy} = \\frac{||y = \\hat{y}||}{||y||}\\)\n\\(\\text{Precision} = \\frac{||y = \\hat{y} = \\text{Class}||}{||\\hat{y} = \\text{Class}||} = \\frac{\\text{T.P.}}{\\text{T.P.} + \\text{F.P.}}\\)\n\\(\\text{Recall} = \\frac{||y = \\hat{y} = \\text{Class}||}{||y = \\text{Class}||} = \\frac{\\text{T.P.}}{\\text{T.P.} + \\text{F.N.}}\\)\n\\(\\text{F-Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           1       0.40      0.67      0.50         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.50      0.33      0.40         6\n           5       1.00      0.83      0.91         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.53        36\n   macro avg       0.54      0.53      0.49        36\nweighted avg       0.54      0.53      0.49        36\n\n\n\nConfusion Matrix\n\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes])\n\nConfusion Matrix Plot Handler\n\n## flag = 1 for a single plot and 0 for subplots for 2 - 8 depths\ndef confMatrix(dataFrame, flag = 1, accuracies = None):\n    if flag:\n        plt.figure(figsize = (15, 15))\n        ax = sns.heatmap(dataFrame, annot = True, cmap = \"PuBu\")\n        plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 8)\n        plt.setp(ax.get_yticklabels(), fontsize = 8)\n        plt.ylabel(\"True label\", fontsize = 18)\n        plt.xlabel(\"Predicted label\", fontsize = 18)\n        plt.title(f\"Accuracy = {accuracy_score(y_test, y_pred)*100: .4f}%\", fontweight = \"bold\", fontsize = 13)\n        plt.savefig(\"Single_ConfusionM.png\")\n        plt.show()\n    else:\n        fig, axes = plt.subplots(3, 3, figsize = (25, 25))\n        axes = axes.flatten()\n\n        for i, df in enumerate(dataFrame):\n            ax = sns.heatmap(df, annot = True, ax = axes[i], cbar = False, cmap = \"PuBu\")\n            \n            plt.setp(ax.get_xticklabels(), rotation = 45, fontsize = 6)\n            plt.setp(ax.get_yticklabels(), fontsize = 8)\n            ax.set_title(f\"Depth = {i + 2}\\nAccuracy = {accuracies[i] * 100: .4f}%\", fontsize = 10)\n            ax.set_ylabel(\"True label\", fontsize = 12)\n            ax.set_xlabel(\"Predicted label\", fontsize = 12)\n            \n        plt.delaxes(axes[7])\n        plt.delaxes(axes[8])\n        plt.tight_layout()\n        plt.subplots_adjust(wspace = 1.1, hspace = 1.1)\n        plt.savefig(\"DepthConfusionM.png\")\n        plt.show()\n\nConfusion Matrix Image\n\nconfMatrix(df_cm, 1)\n\nAnswer: #### 1. The overall performance of the Decision Tree Classifier model trained on the raw total acceleration \\((acc_x^2 + acc_y^2 + acc_z^2)\\) dataset was on average barely better than a fair coin toss.\n\n\n\n\n\n\nThe model performed the best in classifying the activity STANDING, while it mispredicted LAYING as SITTING.\nOn the contrary, classifications among the Dynamic Activities showed confusion.\n\n\n\n\n\n\n\n\nVarying Tree Depths \\((2 - 8)\\) and looking at the results for each depth\n\nconfusion_matrices, class_reports, class_reports_dict, accuracies = [], [], [], []\nfor i in range(2, 9):\n    model = DecisionTreeClassifier(max_depth = i,random_state=42)\n    clfg = model.fit(X_train_TS, y_train)\n    y_pred = clfg.predict(X_test_TS)\n    \n    pred, actual = y_pred, y_test\n    \n    cm = confusion_matrix(actual, pred)\n    \n    confusion_matrices.append(pd.DataFrame(cm, index = [classT for classT in classes], columns = [classT for classT in classes]))\n    class_reports.append(classification_report(actual, pred, labels = np.unique(pred)))\n    class_reports_dict.append(classification_report(actual, pred, labels = np.unique(pred), output_dict = True))\n    accuracies.append(accuracy_score(actual, pred))\n\nConfusion Matrix Image (for varying tree depths)\n\nconfMatrix(confusion_matrices, 0, accuracies = accuracies)\n\nClassification reports\n\nfor i in range(2,9):\n    print(f'Depth = {i}\\n{class_reports[i-2]}\\n\\n')\n\nDepth = 2\n              precision    recall  f1-score   support\n\n           1       0.38      0.50      0.43         6\n           3       0.50      0.17      0.25         6\n           5       0.75      1.00      0.86         6\n           6       0.33      1.00      0.50         6\n\n   micro avg       0.44      0.67      0.53        24\n   macro avg       0.49      0.67      0.51        24\nweighted avg       0.49      0.67      0.51        24\n\n\n\nDepth = 3\n              precision    recall  f1-score   support\n\n           1       0.50      0.50      0.50         6\n           2       1.00      0.17      0.29         6\n           3       0.50      0.33      0.40         6\n           4       0.29      0.33      0.31         6\n           5       0.86      1.00      0.92         6\n           6       0.55      1.00      0.71         6\n\n    accuracy                           0.56        36\n   macro avg       0.61      0.56      0.52        36\nweighted avg       0.61      0.56      0.52        36\n\n\n\nDepth = 4\n              precision    recall  f1-score   support\n\n           1       0.50      0.67      0.57         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.29      0.33      0.31         6\n           5       1.00      0.83      0.91         6\n           6       0.55      1.00      0.71         6\n\n    accuracy                           0.53        36\n   macro avg       0.53      0.53      0.49        36\nweighted avg       0.53      0.53      0.49        36\n\n\n\nDepth = 5\n              precision    recall  f1-score   support\n\n           1       0.45      0.83      0.59         6\n           2       0.50      0.17      0.25         6\n           3       0.50      0.17      0.25         6\n           4       0.40      0.33      0.36         6\n           5       1.00      0.83      0.91         6\n           6       0.55      1.00      0.71         6\n\n    accuracy                           0.56        36\n   macro avg       0.57      0.56      0.51        36\nweighted avg       0.57      0.56      0.51        36\n\n\n\nDepth = 6\n              precision    recall  f1-score   support\n\n           1       0.56      0.83      0.67         6\n           2       0.33      0.17      0.22         6\n           3       0.50      0.17      0.25         6\n           4       0.50      0.33      0.40         6\n           5       0.83      0.83      0.83         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.56        36\n   macro avg       0.54      0.56      0.51        36\nweighted avg       0.54      0.56      0.51        36\n\n\n\nDepth = 7\n              precision    recall  f1-score   support\n\n           1       0.40      0.67      0.50         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.50      0.33      0.40         6\n           5       1.00      0.83      0.91         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.53        36\n   macro avg       0.54      0.53      0.49        36\nweighted avg       0.54      0.53      0.49        36\n\n\n\nDepth = 8\n              precision    recall  f1-score   support\n\n           1       0.40      0.67      0.50         6\n           2       0.50      0.17      0.25         6\n           3       0.33      0.17      0.22         6\n           4       0.50      0.33      0.40         6\n           5       1.00      0.83      0.91         6\n           6       0.50      1.00      0.67         6\n\n    accuracy                           0.53        36\n   macro avg       0.54      0.53      0.49        36\nweighted avg       0.54      0.53      0.49        36\n\n\n\n\n\nAnswer: #### 1. From the classification reports, one can infer that the accuracy of classifying activities is in general increasing."
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n# !pip3 install ipympl\n%matplotlib widget\nfrom google.colab import output\noutput.enable_custom_widget_manager()"
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#particle-as-an-object",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#particle-as-an-object",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "Particle as an Object",
    "text": "Particle as an Object\nIn order to create an object, we create a class - which acts as the blueprint for creating several new objects. The class defines the attributes and the behaviour of the methods that our instances of the class (our objects) will inherit.\nNew categories of objects may be created by extending the attributes and methods of some standard class. This allows us to add on deeper functionality while still preserving the fundamental structure of our objects.\nFor Example: The Particle class may be extended by Electron class."
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#attributes-and-methods-of-the-particle-class",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#attributes-and-methods-of-the-particle-class",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "Attributes and Methods of the Particle class",
    "text": "Attributes and Methods of the Particle class\n\nAttributes\n\n\n\nMass\n\n\nPosition in 3D space \\((x, y, z)\\)\n\n\nVelocity in 3D space \\((v_x, v_y, v_z)\\)\n\n\n\nMethods\n\n\n\nmove(\\(t\\))\n\n\ndistance_from_orgin(\\(x, y, z\\))\n\n\n\nclass Particle:\n  def __init__(self, m, x, y, z, vx, vy, vz):\n    self.m = m\n    self.x = x\n    self.y = y\n    self.z = z\n    self.vx = vx\n    self.vy = vy\n    self.vz = vz\n\n  def move(self, t):\n    self.x += self.vx * t\n    self.y += self.vy * t\n    self.z += self.vz * t\n\n  def origin(self):\n    return np.sqrt(self.x**2 + self.y**2 + self.z**2)\n\n  def plot(self):\n    fig = plt.figure(figsize = (5, 3))\n    ax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\n    ax.scatter(self.x, self.y, self.z, c = \"red\", s = 50, zorder = 1)\n    plt.show()\n\n  def compare_plot(self, point):\n    fig = plt.figure(figsize = (5, 3))\n    ax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\n    ax.scatter(self.x, self.y, self.z, c = \"red\", s = 50, zorder = 1)\n    ax.scatter(point[0], point[1], point[2], c = \"blue\", s = 50, zorder = 1)\n    plt.show()\n\n\np1 = Particle(5, 1, 1, 0, -1, 0, 1)\n\n\np1.origin()\n\n1.4142135623730951\n\n\n\np1.plot()\n\n\n\n\n\np1.move(10)\n\n\nprint(p1.x, p1.y, p1.z)\n\n-9 1 10\n\n\n\np1.origin()\n\n13.490737563232042\n\n\n\np1.plot()"
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#inheritence-of-class",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#inheritence-of-class",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "Inheritence of Class",
    "text": "Inheritence of Class\nElectron inherits all the characteristics of the Particle class and in addition, adds on more functionality to it. 1. Additional Attributes - * mass \\(m = 9.11 \\times 10^{-31} kg\\) - * charge \\(q = -1.602 \\times 10^{-27} C\\)\n\nChanges to the move() method\n\n\n\nMay move in an Electric field \\(\\mathbf{E} = (E_x, E_y, E_z)\\) with an acceleration \\(\\mathbf{a} = \\left(\\frac{q\\mathbf{E}}{m}\\right)\\)\n\n\nMotion now described as:\n\n\n\\(\\bar{x} = x + v_xt + \\frac{1}{2}a_xt^2\\)\n\n\n\\(\\bar{y} = y + v_yt + \\frac{1}{2}a_yt^2\\)\n\n\n\\(\\bar{z} = z + v_zt + \\frac{1}{2}a_zt^2\\)\n\n\n\\(\\bar{v_x} = v_x + a_xt\\)\n\n\n\\(\\bar{v_y} = v_y + a_yt\\)\n\n\n\\(\\bar{v_z} = v_z + a_zt\\)\n\n\n\nclass Electron(Particle):\n  def __init__(self, x, y, z, vx, vy, vz):\n    self.m = 9.11e-31\n    self.q = -1.602e-19\n    super(Electron, self).__init__(self.m, x, y, z, vx, vy, vz)\n\n  def move(self, t, Ex, Ey, Ez):\n    self.x += self.vx * t + (1/2) * ((self.q * Ex)/self.m) * t**2\n    self.y += self.vy * t + (1/2) * ((self.q * Ey)/self.m) * t**2\n    self.z += self.vz * t + (1/2) * ((self.q * Ez)/self.m) * t**2\n    self.vx += ((self.q * Ex)/self.m) * t\n    self.vy += ((self.q * Ey)/self.m) * t\n    self.vz += ((self.q * Ez)/self.m) * t\n\n  def position(self):\n    return np.array([self.x, self.y, self.z])\n\n  def velocity(self):\n    return np.array([self.vx, self.vy, self.vz])\n\n\ne1 = Electron(0, 1, 2, 1, 0, 0)\n\n\ne1.origin()\n\n2.23606797749979\n\n\n\ne1.plot()\n\n\n\n\n\ne1.velocity()\n\narray([1, 0, 0])\n\n\n\ne1.position()\n\narray([0, 1, 2])\n\n\n\ne1.move(2, 10e-10, 0, 1.2e-6)\n\n\ne1.position()\n\narray([-3.49701427e+02,  1.00000000e+00, -4.22039712e+05])\n\n\n\ne1.velocity()\n\narray([-3.50701427e+02,  0.00000000e+00, -4.22041712e+05])\n\n\n\ne1.compare_plot([0, 1, 2])"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = plt.imread(\"Image.jpeg\")\nNx, Ny, Nz = np.shape(img)\nprint(f\"Height: {Nx}, Width: {Ny}, RGB: {Nz}\")\nplt.imshow(img)\nplt.show()\n\nHeight: 3264, Width: 4928, RGB: 3\n\n\n\n\n\n\n\n\n\n\nprint(img)\n\n[[[182 202 227]\n  [172 192 217]\n  [174 194 219]\n  ...\n  [216 229 246]\n  [206 219 236]\n  [196 209 226]]\n\n [[189 209 234]\n  [179 199 224]\n  [179 199 224]\n  ...\n  [213 226 243]\n  [211 224 241]\n  [210 223 240]]\n\n [[191 211 236]\n  [182 202 227]\n  [180 200 225]\n  ...\n  [208 221 238]\n  [214 227 244]\n  [219 232 249]]\n\n ...\n\n [[ 60 107 137]\n  [ 59 106 136]\n  [ 63 110 140]\n  ...\n  [ 57  89 110]\n  [ 59  91 116]\n  [ 57  89 114]]\n\n [[ 60 107 137]\n  [ 58 105 135]\n  [ 60 105 136]\n  ...\n  [ 59  91 112]\n  [ 59  89 115]\n  [ 55  85 111]]\n\n [[ 61 108 138]\n  [ 57 104 134]\n  [ 57 102 133]\n  ...\n  [ 45  77  98]\n  [ 54  81 108]\n  [ 57  84 111]]]\n\n\n\n\n\nimgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nrgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(3264, 4928)\n[[198.8518 188.8528 190.8526 ... 227.0294 217.0304 207.0314]\n [205.8511 195.8521 195.8521 ... 224.0297 222.0299 221.03  ]\n [207.8509 198.8518 196.852  ... 219.0302 225.0296 230.0291]\n ...\n [ 96.361   95.3611  99.3607 ...  81.8203  84.2761  82.2763]\n [ 96.361   94.3612  95.073  ...  83.8201  82.9881  78.9885]\n [ 97.3609  93.3613  92.0733 ...  69.8215  75.9996  78.9993]]\n\n\n\n\n\n\\(G_x = \\begin{bmatrix}1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix}\\) and \\(G_y = \\begin{bmatrix}1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}\\)\n\nGx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\nGy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])\n\n\n\n\n\\(A = \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\) when zero padded by 1 pixel gives: \\(A' = \\begin{bmatrix}0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 2 & 3 & 0 \\\\ 0 & 4 & 5 & 6 & 0 \\\\ 0 & 7 & 8 & 9 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\end{bmatrix}\\)  This is achieved using the NumPy’s .pad() function.\nA_padded = np.pad(A, padding = 1, mode = \"constant\")\nAlso before proceeding with the convolution, the kernel must be flipped Left-Right and then Upside-Down  \\(ker = \\begin{bmatrix}a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} ⟶ \\begin{bmatrix}c & b & a \\\\ f & e & d \\\\ i & h & g \\end{bmatrix} ⟶ \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = ker'\\) \nThis is achieved as:\nker_flipped = np.flipud(np.fliplr(ker))\nfliplr denoting a left-right flip and flipud denoting a up-down flip. Choose a stride of length 1 and perform the convolution as the dot product of kernel sized chunks of \\(A\\) with the \\(ker\\):\n\\(\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 4 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_1\\)  \\(\\begin{bmatrix}0 & 0 & 0 \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_2\\)  \\(\\vdots\\) \n\\(\\begin{bmatrix}5 & 6 & 0 \\\\ 8 & 9 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_N\\)  Notice the dimensions of the final output matrix:\n\\[\\begin{equation}\nR_{\\text{height}} = \\frac{A_{\\text{height}} + 2\\cdot\\text{padding} - ker_{\\text{height}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\\[\\begin{equation}\nR_{\\text{width}} = \\frac{A_{\\text{width}} + 2\\cdot\\text{padding} - ker_{\\text{width}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\nprint(Gx); print()\nprint(np.fliplr(Gx)); print()\nprint(np.flipud(Gx))\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n[[-1.  0.  1.]\n [-2.  0.  2.]\n [-1.  0.  1.]]\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n\n\ndef convolve2d(image, kernel, padding, stride):\n    image_height, image_width = image.shape\n    kernel_height, kernel_width = kernel.shape\n\n    output_height = (image_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (image_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    kernel = np.flipud(np.fliplr(kernel))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            output[i, j] = np.sum(padded_image[i : i + kernel_height, j : j+kernel_width] * kernel)\n\n    return output\n\n\nimgX = convolve2d(grayscale_image, Gx, 1, 1)\nimgY = convolve2d(grayscale_image, Gy, 1, 1)\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\nax[0].imshow(imgX, cmap = \"gray\")\nax[0].set_title(\"X-Gradient\")\nax[1].imshow(imgY, cmap = \"gray\")\nax[1].set_title(\"Y-Gradient\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsobel_final = np.sqrt(imgX**2 + imgY**2)\nplt.imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(img)\nax[1].imshow(grayscale_image, cmap = \"gray\")\nax[2].imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.imsave(\"Sobel.jpeg\", sobel_final, cmap = \"gray\")\n\n\n\n\n\ndef edge_detect(image_org):\n    padding, stride = 1, 1\n\n    rgb_weights = [0.2989, 0.5870, 0.1140]\n    image = np.dot(image_org, rgb_weights)\n\n    Gx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\n    Gy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])\n\n    image_height, image_width = image.shape\n\n    output_height = (image_height + 2 * padding - 3) // stride + 1\n    output_width = (image_width + 2 * padding - 3) // stride + 1\n    A_sobel = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    Gx = np.flipud(np.fliplr(Gx))\n    Gy = np.flipud(np.fliplr(Gy))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            A_sobel[i, j] = (np.sum(padded_image[i : i + 3, j : j + 3] * Gx)**2 + np.sum(padded_image[i : i + 3, j : j + 3] * Gy)**2)**0.5\n\n    plt.imsave(\"Edge.jpeg\", A_sobel, cmap = \"gray\")\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\n    ax[0].imshow(image_org)\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(A_sobel, cmap = \"gray\")\n    ax[1].set_title(\"Edge-Detected\")\n    plt.show()\n\n\nedge_detect(img)"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-rgb-channels",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-rgb-channels",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "imgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-grayscale-image",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-grayscale-image",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "rgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(3264, 4928)\n[[198.8518 188.8528 190.8526 ... 227.0294 217.0304 207.0314]\n [205.8511 195.8521 195.8521 ... 224.0297 222.0299 221.03  ]\n [207.8509 198.8518 196.852  ... 219.0302 225.0296 230.0291]\n ...\n [ 96.361   95.3611  99.3607 ...  81.8203  84.2761  82.2763]\n [ 96.361   94.3612  95.073  ...  83.8201  82.9881  78.9885]\n [ 97.3609  93.3613  92.0733 ...  69.8215  75.9996  78.9993]]"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#sobel-operators---the-edge-detecting-kernels",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#sobel-operators---the-edge-detecting-kernels",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "\\(G_x = \\begin{bmatrix}1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix}\\) and \\(G_y = \\begin{bmatrix}1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}\\)\n\nGx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\nGy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#implementing-the-2d-convolution",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#implementing-the-2d-convolution",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "\\(A = \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\) when zero padded by 1 pixel gives: \\(A' = \\begin{bmatrix}0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 2 & 3 & 0 \\\\ 0 & 4 & 5 & 6 & 0 \\\\ 0 & 7 & 8 & 9 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\end{bmatrix}\\)  This is achieved using the NumPy’s .pad() function.\nA_padded = np.pad(A, padding = 1, mode = \"constant\")\nAlso before proceeding with the convolution, the kernel must be flipped Left-Right and then Upside-Down  \\(ker = \\begin{bmatrix}a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} ⟶ \\begin{bmatrix}c & b & a \\\\ f & e & d \\\\ i & h & g \\end{bmatrix} ⟶ \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = ker'\\) \nThis is achieved as:\nker_flipped = np.flipud(np.fliplr(ker))\nfliplr denoting a left-right flip and flipud denoting a up-down flip. Choose a stride of length 1 and perform the convolution as the dot product of kernel sized chunks of \\(A\\) with the \\(ker\\):\n\\(\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 4 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_1\\)  \\(\\begin{bmatrix}0 & 0 & 0 \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_2\\)  \\(\\vdots\\) \n\\(\\begin{bmatrix}5 & 6 & 0 \\\\ 8 & 9 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_N\\)  Notice the dimensions of the final output matrix:\n\\[\\begin{equation}\nR_{\\text{height}} = \\frac{A_{\\text{height}} + 2\\cdot\\text{padding} - ker_{\\text{height}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\\[\\begin{equation}\nR_{\\text{width}} = \\frac{A_{\\text{width}} + 2\\cdot\\text{padding} - ker_{\\text{width}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\nprint(Gx); print()\nprint(np.fliplr(Gx)); print()\nprint(np.flipud(Gx))\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n[[-1.  0.  1.]\n [-2.  0.  2.]\n [-1.  0.  1.]]\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n\n\ndef convolve2d(image, kernel, padding, stride):\n    image_height, image_width = image.shape\n    kernel_height, kernel_width = kernel.shape\n\n    output_height = (image_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (image_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    kernel = np.flipud(np.fliplr(kernel))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            output[i, j] = np.sum(padded_image[i : i + kernel_height, j : j+kernel_width] * kernel)\n\n    return output\n\n\nimgX = convolve2d(grayscale_image, Gx, 1, 1)\nimgY = convolve2d(grayscale_image, Gy, 1, 1)\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\nax[0].imshow(imgX, cmap = \"gray\")\nax[0].set_title(\"X-Gradient\")\nax[1].imshow(imgY, cmap = \"gray\")\nax[1].set_title(\"Y-Gradient\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsobel_final = np.sqrt(imgX**2 + imgY**2)\nplt.imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(img)\nax[1].imshow(grayscale_image, cmap = \"gray\")\nax[2].imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.imsave(\"Sobel.jpeg\", sobel_final, cmap = \"gray\")"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#wrapped-up-function",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#wrapped-up-function",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "def edge_detect(image_org):\n    padding, stride = 1, 1\n\n    rgb_weights = [0.2989, 0.5870, 0.1140]\n    image = np.dot(image_org, rgb_weights)\n\n    Gx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\n    Gy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])\n\n    image_height, image_width = image.shape\n\n    output_height = (image_height + 2 * padding - 3) // stride + 1\n    output_width = (image_width + 2 * padding - 3) // stride + 1\n    A_sobel = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    Gx = np.flipud(np.fliplr(Gx))\n    Gy = np.flipud(np.fliplr(Gy))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            A_sobel[i, j] = (np.sum(padded_image[i : i + 3, j : j + 3] * Gx)**2 + np.sum(padded_image[i : i + 3, j : j + 3] * Gy)**2)**0.5\n\n    plt.imsave(\"Edge.jpeg\", A_sobel, cmap = \"gray\")\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\n    ax[0].imshow(image_org)\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(A_sobel, cmap = \"gray\")\n    ax[1].set_title(\"Edge-Detected\")\n    plt.show()\n\n\nedge_detect(img)"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html",
    "href": "posts/Pooling/ML_Scratch_Pooling.html",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\n\nimg = plt.imread(\"ImageSq.jpeg\")\nNx, Ny, Nz = np.shape(img)\nprint(f\"Height: {Nx}, Width: {Ny}, RGB: {Nz}\")\nplt.imshow(img)\nplt.show()\n\nHeight: 6069, Width: 4855, RGB: 3\n\n\n\n\n\n\n\n\n\n\nprint(img)\n\n[[[ 86  62  38]\n  [ 86  62  38]\n  [ 86  62  38]\n  ...\n  [164 132 109]\n  [164 132 109]\n  [164 132 109]]\n\n [[ 86  62  38]\n  [ 86  62  38]\n  [ 86  62  38]\n  ...\n  [164 132 109]\n  [164 132 109]\n  [164 132 109]]\n\n [[ 86  62  38]\n  [ 86  62  38]\n  [ 86  62  38]\n  ...\n  [164 132 109]\n  [164 132 109]\n  [164 132 109]]\n\n ...\n\n [[156 129 100]\n  [156 129 100]\n  [156 129 100]\n  ...\n  [146 118  94]\n  [146 118  94]\n  [146 118  94]]\n\n [[156 129 100]\n  [156 129 100]\n  [156 129 100]\n  ...\n  [146 118  94]\n  [146 118  94]\n  [146 118  94]]\n\n [[156 129 100]\n  [156 129 100]\n  [156 129 100]\n  ...\n  [146 118  94]\n  [146 118  94]\n  [146 118  94]]]\n\n\n\n\n\nimgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nrgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(6069, 4855)\n[[ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n ...\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]]\n\n\n\n\n\n\ndef max_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.max(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgnew = max_pool(img, (3, 3), 3)\n\n\nimgnew\n\narray([[[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       ...,\n\n       [[160, 130, 102],\n        [160, 130, 102],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[159, 129, 101],\n        [159, 129, 101],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[156, 129, 100],\n        [156, 129, 100],\n        [156, 129, 100],\n        ...,\n        [148, 120,  96],\n        [146, 118,  94],\n        [146, 118,  94]]], dtype=uint8)\n\n\n\nimgnew.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgnew)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"MaxPooled.jpeg\", imgnew)\n\n\nimgnew1 = max_pool(imgnew, (3, 3), 3)\n\n\nimgnew1.shape\n\n(674, 539, 3)\n\n\n\n\n\n\nplt.imshow(imgnew1)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"FurtherMaxPooled.jpeg\", imgnew1)\n\n\n\n\n\n\ndef avg_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.mean(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgavg = avg_pool(img, (3, 3), 3)\nimgavg.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgavg)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"AvgPooled.jpeg\", imgavg)"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#the-rgb-channels",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#the-rgb-channels",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "imgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#the-grayscale-image",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#the-grayscale-image",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "rgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(6069, 4855)\n[[ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n ...\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]]"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#max-pooling",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#max-pooling",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "def max_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.max(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgnew = max_pool(img, (3, 3), 3)\n\n\nimgnew\n\narray([[[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       ...,\n\n       [[160, 130, 102],\n        [160, 130, 102],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[159, 129, 101],\n        [159, 129, 101],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[156, 129, 100],\n        [156, 129, 100],\n        [156, 129, 100],\n        ...,\n        [148, 120,  96],\n        [146, 118,  94],\n        [146, 118,  94]]], dtype=uint8)\n\n\n\nimgnew.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgnew)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"MaxPooled.jpeg\", imgnew)\n\n\nimgnew1 = max_pool(imgnew, (3, 3), 3)\n\n\nimgnew1.shape\n\n(674, 539, 3)\n\n\n\n\n\n\nplt.imshow(imgnew1)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"FurtherMaxPooled.jpeg\", imgnew1)"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#average-pooling",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#average-pooling",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "def avg_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.mean(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgavg = avg_pool(img, (3, 3), 3)\nimgavg.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgavg)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"AvgPooled.jpeg\", imgavg)"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html",
    "href": "posts/Linear-Regression/Linear_Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "A set of \\(N\\) data points \\((x_i, y_i)\\), the goal is to find the find the best linear map \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) such that \\(f(x) = mx + b\\) fits the data points. In simpler terms, we assume the relation between the dependenet variable \\(y\\) and independent variable \\(x\\) is linear and try finding the optimal \\(m\\) and \\(b\\) such that some error function is minimised.\n\n\n\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - \\widehat{y})^2\\)\nwhere,\n\\(\\widehat{y} = mx_i + b\\), hence\n\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))^2\\)\n\n\n\n\\(\\frac{∂E}{∂m} = -\\frac{2}{N}\\sum_{i = 1}^{N}(x_i \\times (y_i - (mx_i + b)) )\\)\n\\(\\frac{∂E}{∂b} = -\\frac{2}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))\\)\n\n\n\nArrive at the desired \\(m\\) and \\(b\\) by updating these values following the direction of greatest descent of this function. The learning rate \\(L\\) has to be specified.\n\\(\\bar{m} = m - L\\frac{\\partial E}{\\partial m}\\)\n\\(\\bar{b} = b - L\\frac{\\partial E}{\\partial b}\\)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib widget\n\n\ndef loss_func(m, b, data):\n  N = len(data)\n  E = 0\n  for i in range(N):\n    E += (data[i][1] - (m * data[i][0] + b))**2\n  return E/N\n\n\ndef gradient_descent(data, m_now, b_now, L):\n  N = len(data)\n  E_m, E_b = 0, 0\n  for i in range(N):\n    E_m += -2/N * (data[i][0] * (data[i][1] - m_now * data[i][0] - b_now))\n    E_b += -2/N * (data[i][1] - m_now * data[i][0] - b_now)\n\n  m_cur = m_now - L * E_m\n  b_cur = b_now - L * E_b\n\n  return m_cur, b_cur\n\n\ndf = pd.read_csv(\"Data.csv\")\ndata = df.to_numpy()\nm, b, L, epochs = 0, 0, 0.00001, 100\nfor i in range(epochs):\n  m, b = gradient_descent(data, m, b, L)\n\nprint(f\"m = {m}, b = {b}\")\nplt.scatter(data[:, 0], data[:, 1], color = \"black\")\nX = range(100)\nplt.plot(X, m*X + b, color = \"red\")\nplt.show()\n\nm = 2.4306905668231544, b = 0.045763347379328585\n\n\n\n\n\n\nm_x = np.linspace(-1, 8, 100)\nb_y = np.linspace(-20, 20, 100)\nm_mesh, b_mesh = np.meshgrid(m_x, b_y)\n\nE = loss_func(m_mesh, b_mesh, data)\n\n# The Loss Function\nfig = go.Figure(data = [go.Surface(x = m_mesh, y = b_mesh, z = E)])\n\n# The Minima point\nfig.add_trace(go.Scatter3d(x = [m], y = [b], z = [loss_func(m, b, data)], mode = \"markers\", marker = dict(size = 10, color = \"red\"), name = \"Minima\"))\n\nfig.update_layout(scene = dict(xaxis_title = \"Slope (m)\", yaxis_title = \"Intercept (b)\", zaxis_title = \"Loss Function E(m ,b)\",))\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(111, projection = \"3d\")\nax.plot_surface(m_mesh, b_mesh, E, cmap = \"viridis\")\nax.scatter(m, b, loss_func(m, b, data), c = \"red\", s = 100, label = \"Minima\")\n\nax.set_xlabel(\"Slope (m)\")\nax.set_ylabel(\"Intercept (b)\")\nax.set_zlabel(\"Loss Function E(m ,b)\")\nplt.show()"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html#losserror-function",
    "href": "posts/Linear-Regression/Linear_Regression.html#losserror-function",
    "title": "Linear Regression",
    "section": "",
    "text": "\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - \\widehat{y})^2\\)\nwhere,\n\\(\\widehat{y} = mx_i + b\\), hence\n\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))^2\\)"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html#optimal-m-and-b",
    "href": "posts/Linear-Regression/Linear_Regression.html#optimal-m-and-b",
    "title": "Linear Regression",
    "section": "",
    "text": "\\(\\frac{∂E}{∂m} = -\\frac{2}{N}\\sum_{i = 1}^{N}(x_i \\times (y_i - (mx_i + b)) )\\)\n\\(\\frac{∂E}{∂b} = -\\frac{2}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))\\)"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html#gradient-descent",
    "href": "posts/Linear-Regression/Linear_Regression.html#gradient-descent",
    "title": "Linear Regression",
    "section": "",
    "text": "Arrive at the desired \\(m\\) and \\(b\\) by updating these values following the direction of greatest descent of this function. The learning rate \\(L\\) has to be specified.\n\\(\\bar{m} = m - L\\frac{\\partial E}{\\partial m}\\)\n\\(\\bar{b} = b - L\\frac{\\partial E}{\\partial b}\\)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib widget\n\n\ndef loss_func(m, b, data):\n  N = len(data)\n  E = 0\n  for i in range(N):\n    E += (data[i][1] - (m * data[i][0] + b))**2\n  return E/N\n\n\ndef gradient_descent(data, m_now, b_now, L):\n  N = len(data)\n  E_m, E_b = 0, 0\n  for i in range(N):\n    E_m += -2/N * (data[i][0] * (data[i][1] - m_now * data[i][0] - b_now))\n    E_b += -2/N * (data[i][1] - m_now * data[i][0] - b_now)\n\n  m_cur = m_now - L * E_m\n  b_cur = b_now - L * E_b\n\n  return m_cur, b_cur\n\n\ndf = pd.read_csv(\"Data.csv\")\ndata = df.to_numpy()\nm, b, L, epochs = 0, 0, 0.00001, 100\nfor i in range(epochs):\n  m, b = gradient_descent(data, m, b, L)\n\nprint(f\"m = {m}, b = {b}\")\nplt.scatter(data[:, 0], data[:, 1], color = \"black\")\nX = range(100)\nplt.plot(X, m*X + b, color = \"red\")\nplt.show()\n\nm = 2.4306905668231544, b = 0.045763347379328585\n\n\n\n\n\n\nm_x = np.linspace(-1, 8, 100)\nb_y = np.linspace(-20, 20, 100)\nm_mesh, b_mesh = np.meshgrid(m_x, b_y)\n\nE = loss_func(m_mesh, b_mesh, data)\n\n# The Loss Function\nfig = go.Figure(data = [go.Surface(x = m_mesh, y = b_mesh, z = E)])\n\n# The Minima point\nfig.add_trace(go.Scatter3d(x = [m], y = [b], z = [loss_func(m, b, data)], mode = \"markers\", marker = dict(size = 10, color = \"red\"), name = \"Minima\"))\n\nfig.update_layout(scene = dict(xaxis_title = \"Slope (m)\", yaxis_title = \"Intercept (b)\", zaxis_title = \"Loss Function E(m ,b)\",))\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(111, projection = \"3d\")\nax.plot_surface(m_mesh, b_mesh, E, cmap = \"viridis\")\nax.scatter(m, b, loss_func(m, b, data), c = \"red\", s = 100, label = \"Minima\")\n\nax.set_xlabel(\"Slope (m)\")\nax.set_ylabel(\"Intercept (b)\")\nax.set_zlabel(\"Loss Function E(m ,b)\")\nplt.show()"
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html",
    "href": "posts/Gradient-Descent/Gradient_Descent.html",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "For a multivariable function, in \\(N\\) dimensions let’s say, \\(F(\\mathbf{v})\\) which is differentiable at a point \\(\\mathbf{v}\\), we say that \\(F(\\mathbf{v})\\) decreases fastest in the direction of negative of the gradient at that point \\(\\mathbf{v}\\) denoted by \\(-∇F(\\mathbf{v})\\).\n\n\n\\[\\begin{equation}\n\\mathbf{v}_{i + 1} = \\mathbf{v}_{i} - L \\nabla F(\\mathbf{v}_{i})\n\\end{equation}\\] where, \\(L\\) is the learning rate and \\(L \\in \\mathbb{R}_{+}\\) and \\(\\mathbf{v} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}\\) and \\(\\nabla F(\\mathbf{v}) = \\begin{bmatrix}\n    \\frac{\\partial F}{\\partial x_1} \\\\\n    \\frac{\\partial F}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\\). Hence the equation becomes:\n\\[\\begin{equation}\n\\begin{bmatrix} x_1^{i + 1} \\\\ x_2^{i + 1} \\\\ \\vdots \\\\ x_N^{i + 1} \\end{bmatrix} = \\begin{bmatrix} x_1^i \\\\ x_2^i \\\\ \\vdots \\\\ x_N^i \\end{bmatrix} - L \\begin{bmatrix}\\frac{\\partial F}{\\partial x_1} \\\\ \\frac{\\partial F}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\n\\end{equation}\\]\n\n\n\nNotice that the decrease in \\(F(\\mathbf{v})\\) is guaranteed only to the nearest well, which may or may not be the global minima. We may run for a specified number of epochs or terminate at a set tolerance too.\n\n\n\nLet \\(F(\\mathbf{v}) = F\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\right) = \\sin(x)\\cos(y)\\), then the gradient \\(∇F(\\mathbf{v}) = \\begin{bmatrix}\\frac{\\partial F}{\\partial x} \\\\ \\frac{\\partial F}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\\) and starting from an initial point \\(\\mathbf{v}_0\\), we may reach the nearest local minima as:\n\\[\\begin{equation}\n\\begin{bmatrix} \\bar x \\\\ \\bar y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} - L \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\n\\end{equation}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n!pip3 install ipympl\n%matplotlib widget\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndef gradient_func2d(x, y):\n  return np.cos(x) * np.cos(y), - np.sin(x) * np.sin(y)\n\n\ndef func2d(x, y):\n  return np.sin(x) * np.cos(y)\n\n\n\n\nx = np.arange(-5, 5, 0.1)\ny = np.arange(-5, 5, 0.1)\nX, Y = np.meshgrid(x, y)\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder=False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\")\n\nplt.show()\n\n\n\n\n\n\n\n\np1 = (1, 0.5)\np2 = (-1, 3)\np3 = (-4, -0.5)\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\nax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\nax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\nax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\nplt.show()\n\n\n\n\n\n\n\n\ndef iterate_gradient(point, L):\n  Xgrad, Ygrad = gradient_func2d(point[0], point[1])\n  Xnew, Ynew = point[0] - L * Xgrad, point[1] - L * Ygrad\n  return (Xnew, Ynew, func2d(Xnew, Ynew))\n\n\nL, epochs = 0.1, 100\n\nax = plt.subplot(projection=\"3d\", computed_zorder = False)\n\nfor i in range(epochs):\n  p1 = iterate_gradient(p1, L)\n  p2 = iterate_gradient(p2, L)\n  p3 = iterate_gradient(p3, L)\n\n\n  ax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\n  ax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\n  ax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\n  ax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\n\n  ax.set_title(f\"Epoch No: {i}\")\n\n  plt.pause(0.001)\n  ax.clear()"
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html#evaluating-the-descent",
    "href": "posts/Gradient-Descent/Gradient_Descent.html#evaluating-the-descent",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "\\[\\begin{equation}\n\\mathbf{v}_{i + 1} = \\mathbf{v}_{i} - L \\nabla F(\\mathbf{v}_{i})\n\\end{equation}\\] where, \\(L\\) is the learning rate and \\(L \\in \\mathbb{R}_{+}\\) and \\(\\mathbf{v} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}\\) and \\(\\nabla F(\\mathbf{v}) = \\begin{bmatrix}\n    \\frac{\\partial F}{\\partial x_1} \\\\\n    \\frac{\\partial F}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\\). Hence the equation becomes:\n\\[\\begin{equation}\n\\begin{bmatrix} x_1^{i + 1} \\\\ x_2^{i + 1} \\\\ \\vdots \\\\ x_N^{i + 1} \\end{bmatrix} = \\begin{bmatrix} x_1^i \\\\ x_2^i \\\\ \\vdots \\\\ x_N^i \\end{bmatrix} - L \\begin{bmatrix}\\frac{\\partial F}{\\partial x_1} \\\\ \\frac{\\partial F}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html#descent-to-a-minima",
    "href": "posts/Gradient-Descent/Gradient_Descent.html#descent-to-a-minima",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "Notice that the decrease in \\(F(\\mathbf{v})\\) is guaranteed only to the nearest well, which may or may not be the global minima. We may run for a specified number of epochs or terminate at a set tolerance too."
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html#dimensional-example",
    "href": "posts/Gradient-Descent/Gradient_Descent.html#dimensional-example",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "Let \\(F(\\mathbf{v}) = F\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\right) = \\sin(x)\\cos(y)\\), then the gradient \\(∇F(\\mathbf{v}) = \\begin{bmatrix}\\frac{\\partial F}{\\partial x} \\\\ \\frac{\\partial F}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\\) and starting from an initial point \\(\\mathbf{v}_0\\), we may reach the nearest local minima as:\n\\[\\begin{equation}\n\\begin{bmatrix} \\bar x \\\\ \\bar y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} - L \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\n\\end{equation}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n!pip3 install ipympl\n%matplotlib widget\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndef gradient_func2d(x, y):\n  return np.cos(x) * np.cos(y), - np.sin(x) * np.sin(y)\n\n\ndef func2d(x, y):\n  return np.sin(x) * np.cos(y)\n\n\n\n\nx = np.arange(-5, 5, 0.1)\ny = np.arange(-5, 5, 0.1)\nX, Y = np.meshgrid(x, y)\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder=False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\")\n\nplt.show()\n\n\n\n\n\n\n\n\np1 = (1, 0.5)\np2 = (-1, 3)\np3 = (-4, -0.5)\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\nax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\nax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\nax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\nplt.show()\n\n\n\n\n\n\n\n\ndef iterate_gradient(point, L):\n  Xgrad, Ygrad = gradient_func2d(point[0], point[1])\n  Xnew, Ynew = point[0] - L * Xgrad, point[1] - L * Ygrad\n  return (Xnew, Ynew, func2d(Xnew, Ynew))\n\n\nL, epochs = 0.1, 100\n\nax = plt.subplot(projection=\"3d\", computed_zorder = False)\n\nfor i in range(epochs):\n  p1 = iterate_gradient(p1, L)\n  p2 = iterate_gradient(p2, L)\n  p3 = iterate_gradient(p3, L)\n\n\n  ax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\n  ax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\n  ax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\n  ax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\n\n  ax.set_title(f\"Epoch No: {i}\")\n\n  plt.pause(0.001)\n  ax.clear()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Time Series data of UCI-HAR Dataset\n\n\n\nClassfication\n\n\n\n\nGuntas Singh Saran, Hrriday V. Ruparel\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature Extraction using TSFEL on UCI-HAR Dataset\n\n\n\nClassfication\n\n\n\n\nGuntas Singh Saran, Hrriday V. Ruparel, Jiya Desai\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Deployment on Feature Extracted Data\n\n\n\nClassfication\n\n\n\n\nGuntas Singh Saran, Hrriday V. Ruparel\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Deployment on Raw Time Series Data\n\n\n\nClassfication\n\n\n\n\nGuntas Singh Saran, Hrriday V. Ruparel\n\n\nJan 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage and Max Pooling built from scratch\n\n\n\nConvolution\n\n\n\n\nGuntas Singh Saran\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelling of Particles using Object-Oriented Programming\n\n\n\nOOP\n\n\n\n\nGuntas Singh Saran\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Detecting Filters using Sobel Operators\n\n\n\nConvolution\n\n\n\n\nGuntas Singh Saran\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvolution operations built from scratch\n\n\n\nConvolution\n\n\n\n\nGuntas Singh Saran\n\n\nDec 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Gradient Descent for two variable case\n\n\n\nGradient Descent\n\n\n\n\nGuntas Singh Saran\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\nLinear Regression\n\n\n\n\nGuntas Singh Saran\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]