[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Here’s the compilation of the blog posts written by me in the interest of exploring or learning new concepts!"
  },
  {
    "objectID": "posts/Convolution/ML_Scratch_Convolution.html",
    "href": "posts/Convolution/ML_Scratch_Convolution.html",
    "title": "Convolution operations built from scratch",
    "section": "",
    "text": "Convolution\nFor two functions \\(f(x)\\) and \\(g(x)\\), the convolution function \\(f(x) * g(x)\\) is defined as:\n\\[\\begin{equation}\n(f * g) (t) = \\int_{-\\infty}^{\\infty} f(τ) ⋅ g(t - τ) dτ\n\\end{equation}\\]\nfor discrete samples that we deal with: \\[\\begin{equation}\ny[n] = f[n] * g[n] = \\sum_{k = -∞}^{∞} f[k] ⋅ g[n - k]\n\\end{equation}\\]\nif \\(f\\) has \\(N\\) samples and \\(g\\) has \\(M\\) samples, then the convolved function has \\(N + M - 1\\) samples. A basic rule: “flip any one of the functions, overlap it with the stationary one, multiply and add, and then traverse over.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef triangle(x):\n  if x &gt;= 0 and x &lt;= 5:\n    return 5 - x\n  elif x &lt; 0 and x &gt;= -5:\n    return 5 + x\n  else:\n    return 0\n\ndef rect(x):\n  if -5 &lt;= x &lt;= 5:\n    return 5\n  return 0\n\ndef signal(f, x):\n  return [f(i) for i in x]\n\n\nx = np.arange(-8, 8, 0.1)\nf_x = signal(rect, x)\nplt.plot(x, f_x, color = \"black\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\\(O(N^2)\\) complexity algorithm total \\((N + M - 1) \\cdot M\\) or \\((N + M - 1) \\cdot N\\) computations\n\nA_signal, B_kernel = signal(rect, x), signal(rect, x)\nN_signal, M_kernel = len(A_signal), len(B_kernel)\nY = np.zeros(N_signal + M_kernel - 1)\nfor i in range(len(Y)):\n  for j in range(M_kernel):\n    if i - j &gt;= 0 and i - j &lt; N_signal:\n      Y[i] += A_signal[i - j] * B_kernel[j]\n\n\nConvolution of two rect(\\(x\\))\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"green\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"green\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nprint(f\"Signal Length: {N_signal}, Convolved Signal Lenght: {len(Y)}\")\nplt.plot(Y, color = \"red\")\nplt.xlabel(\"Index Number (k)\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignal Length: 160, Convolved Signal Lenght: 319\n\n\n\n\n\n\n\n\n\n\nA_signal, B_kernel = signal(triangle, x), signal(triangle, x)\nN_signal, M_kernel = len(A_signal), len(B_kernel)\nY = np.zeros(N_signal + M_kernel - 1)\nfor i in range(len(Y)):\n  for j in range(M_kernel):\n    if i - j &gt;= 0 and i - j &lt; N_signal:\n      Y[i] += A_signal[i - j] * B_kernel[j]\n\n\n\nConvolution of two triangle(\\(x\\))\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"cyan\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nplt.figure(figsize = (3, 3))\nplt.plot(x, A_signal, color = \"cyan\")\nplt.xlabel(\"Time sample (N)\")\nplt.grid()\nplt.show()\n\nprint(f\"Signal Length: {N_signal}, Convolved Signal Lenght: {len(Y)}\")\nplt.plot(Y, color = \"blue\")\nplt.xlabel(\"Index Number (k)\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSignal Length: 160, Convolved Signal Lenght: 319"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html",
    "href": "posts/Linear-Regression/Linear_Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "A set of \\(N\\) data points \\((x_i, y_i)\\), the goal is to find the find the best linear map \\(f: \\mathbb{R}^2 \\to \\mathbb{R}^2\\) such that \\(f(x) = mx + b\\) fits the data points. In simpler terms, we assume the relation between the dependenet variable \\(y\\) and independent variable \\(x\\) is linear and try finding the optimal \\(m\\) and \\(b\\) such that some error function is minimised.\n\n\n\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - \\widehat{y})^2\\)\nwhere,\n\\(\\widehat{y} = mx_i + b\\), hence\n\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))^2\\)\n\n\n\n\\(\\frac{∂E}{∂m} = -\\frac{2}{N}\\sum_{i = 1}^{N}(x_i \\times (y_i - (mx_i + b)) )\\)\n\\(\\frac{∂E}{∂b} = -\\frac{2}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))\\)\n\n\n\nArrive at the desired \\(m\\) and \\(b\\) by updating these values following the direction of greatest descent of this function. The learning rate \\(L\\) has to be specified.\n\\(\\bar{m} = m - L\\frac{\\partial E}{\\partial m}\\)\n\\(\\bar{b} = b - L\\frac{\\partial E}{\\partial b}\\)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib widget\n\n\ndef loss_func(m, b, data):\n  N = len(data)\n  E = 0\n  for i in range(N):\n    E += (data[i][1] - (m * data[i][0] + b))**2\n  return E/N\n\n\ndef gradient_descent(data, m_now, b_now, L):\n  N = len(data)\n  E_m, E_b = 0, 0\n  for i in range(N):\n    E_m += -2/N * (data[i][0] * (data[i][1] - m_now * data[i][0] - b_now))\n    E_b += -2/N * (data[i][1] - m_now * data[i][0] - b_now)\n\n  m_cur = m_now - L * E_m\n  b_cur = b_now - L * E_b\n\n  return m_cur, b_cur\n\n\ndf = pd.read_csv(\"Data.csv\")\ndata = df.to_numpy()\nm, b, L, epochs = 0, 0, 0.00001, 100\nfor i in range(epochs):\n  m, b = gradient_descent(data, m, b, L)\n\nprint(f\"m = {m}, b = {b}\")\nplt.scatter(data[:, 0], data[:, 1], color = \"black\")\nX = range(100)\nplt.plot(X, m*X + b, color = \"red\")\nplt.show()\n\nm = 2.4306905668231544, b = 0.045763347379328585\n\n\n\n\n\n\nm_x = np.linspace(-1, 8, 100)\nb_y = np.linspace(-20, 20, 100)\nm_mesh, b_mesh = np.meshgrid(m_x, b_y)\n\nE = loss_func(m_mesh, b_mesh, data)\n\n# The Loss Function\nfig = go.Figure(data = [go.Surface(x = m_mesh, y = b_mesh, z = E)])\n\n# The Minima point\nfig.add_trace(go.Scatter3d(x = [m], y = [b], z = [loss_func(m, b, data)], mode = \"markers\", marker = dict(size = 10, color = \"red\"), name = \"Minima\"))\n\nfig.update_layout(scene = dict(xaxis_title = \"Slope (m)\", yaxis_title = \"Intercept (b)\", zaxis_title = \"Loss Function E(m ,b)\",))\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(111, projection = \"3d\")\nax.plot_surface(m_mesh, b_mesh, E, cmap = \"viridis\")\nax.scatter(m, b, loss_func(m, b, data), c = \"red\", s = 100, label = \"Minima\")\n\nax.set_xlabel(\"Slope (m)\")\nax.set_ylabel(\"Intercept (b)\")\nax.set_zlabel(\"Loss Function E(m ,b)\")\nplt.show()"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html#losserror-function",
    "href": "posts/Linear-Regression/Linear_Regression.html#losserror-function",
    "title": "Linear Regression",
    "section": "",
    "text": "\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - \\widehat{y})^2\\)\nwhere,\n\\(\\widehat{y} = mx_i + b\\), hence\n\\(E = \\frac{1}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))^2\\)"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html#optimal-m-and-b",
    "href": "posts/Linear-Regression/Linear_Regression.html#optimal-m-and-b",
    "title": "Linear Regression",
    "section": "",
    "text": "\\(\\frac{∂E}{∂m} = -\\frac{2}{N}\\sum_{i = 1}^{N}(x_i \\times (y_i - (mx_i + b)) )\\)\n\\(\\frac{∂E}{∂b} = -\\frac{2}{N}\\sum_{i = 1}^{N}(y_i - (mx_i + b))\\)"
  },
  {
    "objectID": "posts/Linear-Regression/Linear_Regression.html#gradient-descent",
    "href": "posts/Linear-Regression/Linear_Regression.html#gradient-descent",
    "title": "Linear Regression",
    "section": "",
    "text": "Arrive at the desired \\(m\\) and \\(b\\) by updating these values following the direction of greatest descent of this function. The learning rate \\(L\\) has to be specified.\n\\(\\bar{m} = m - L\\frac{\\partial E}{\\partial m}\\)\n\\(\\bar{b} = b - L\\frac{\\partial E}{\\partial b}\\)\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib widget\n\n\ndef loss_func(m, b, data):\n  N = len(data)\n  E = 0\n  for i in range(N):\n    E += (data[i][1] - (m * data[i][0] + b))**2\n  return E/N\n\n\ndef gradient_descent(data, m_now, b_now, L):\n  N = len(data)\n  E_m, E_b = 0, 0\n  for i in range(N):\n    E_m += -2/N * (data[i][0] * (data[i][1] - m_now * data[i][0] - b_now))\n    E_b += -2/N * (data[i][1] - m_now * data[i][0] - b_now)\n\n  m_cur = m_now - L * E_m\n  b_cur = b_now - L * E_b\n\n  return m_cur, b_cur\n\n\ndf = pd.read_csv(\"Data.csv\")\ndata = df.to_numpy()\nm, b, L, epochs = 0, 0, 0.00001, 100\nfor i in range(epochs):\n  m, b = gradient_descent(data, m, b, L)\n\nprint(f\"m = {m}, b = {b}\")\nplt.scatter(data[:, 0], data[:, 1], color = \"black\")\nX = range(100)\nplt.plot(X, m*X + b, color = \"red\")\nplt.show()\n\nm = 2.4306905668231544, b = 0.045763347379328585\n\n\n\n\n\n\nm_x = np.linspace(-1, 8, 100)\nb_y = np.linspace(-20, 20, 100)\nm_mesh, b_mesh = np.meshgrid(m_x, b_y)\n\nE = loss_func(m_mesh, b_mesh, data)\n\n# The Loss Function\nfig = go.Figure(data = [go.Surface(x = m_mesh, y = b_mesh, z = E)])\n\n# The Minima point\nfig.add_trace(go.Scatter3d(x = [m], y = [b], z = [loss_func(m, b, data)], mode = \"markers\", marker = dict(size = 10, color = \"red\"), name = \"Minima\"))\n\nfig.update_layout(scene = dict(xaxis_title = \"Slope (m)\", yaxis_title = \"Intercept (b)\", zaxis_title = \"Loss Function E(m ,b)\",))\nfig.show()\n\n\n\n\n                                \n                                            \n\n\n\n\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(111, projection = \"3d\")\nax.plot_surface(m_mesh, b_mesh, E, cmap = \"viridis\")\nax.scatter(m, b, loss_func(m, b, data), c = \"red\", s = 100, label = \"Minima\")\n\nax.set_xlabel(\"Slope (m)\")\nax.set_ylabel(\"Intercept (b)\")\nax.set_zlabel(\"Loss Function E(m ,b)\")\nplt.show()"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\nimg = plt.imread(\"Image.jpeg\")\nNx, Ny, Nz = np.shape(img)\nprint(f\"Height: {Nx}, Width: {Ny}, RGB: {Nz}\")\nplt.imshow(img)\nplt.show()\n\nHeight: 3264, Width: 4928, RGB: 3\n\n\n\n\n\n\n\n\n\n\nprint(img)\n\n[[[182 202 227]\n  [172 192 217]\n  [174 194 219]\n  ...\n  [216 229 246]\n  [206 219 236]\n  [196 209 226]]\n\n [[189 209 234]\n  [179 199 224]\n  [179 199 224]\n  ...\n  [213 226 243]\n  [211 224 241]\n  [210 223 240]]\n\n [[191 211 236]\n  [182 202 227]\n  [180 200 225]\n  ...\n  [208 221 238]\n  [214 227 244]\n  [219 232 249]]\n\n ...\n\n [[ 60 107 137]\n  [ 59 106 136]\n  [ 63 110 140]\n  ...\n  [ 57  89 110]\n  [ 59  91 116]\n  [ 57  89 114]]\n\n [[ 60 107 137]\n  [ 58 105 135]\n  [ 60 105 136]\n  ...\n  [ 59  91 112]\n  [ 59  89 115]\n  [ 55  85 111]]\n\n [[ 61 108 138]\n  [ 57 104 134]\n  [ 57 102 133]\n  ...\n  [ 45  77  98]\n  [ 54  81 108]\n  [ 57  84 111]]]\n\n\n\n\n\nimgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nrgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(3264, 4928)\n[[198.8518 188.8528 190.8526 ... 227.0294 217.0304 207.0314]\n [205.8511 195.8521 195.8521 ... 224.0297 222.0299 221.03  ]\n [207.8509 198.8518 196.852  ... 219.0302 225.0296 230.0291]\n ...\n [ 96.361   95.3611  99.3607 ...  81.8203  84.2761  82.2763]\n [ 96.361   94.3612  95.073  ...  83.8201  82.9881  78.9885]\n [ 97.3609  93.3613  92.0733 ...  69.8215  75.9996  78.9993]]\n\n\n\n\n\n\\(G_x = \\begin{bmatrix}1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix}\\) and \\(G_y = \\begin{bmatrix}1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}\\)\n\nGx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\nGy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])\n\n\n\n\n\\(A = \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\) when zero padded by 1 pixel gives: \\(A' = \\begin{bmatrix}0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 2 & 3 & 0 \\\\ 0 & 4 & 5 & 6 & 0 \\\\ 0 & 7 & 8 & 9 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\end{bmatrix}\\)  This is achieved using the NumPy’s .pad() function.\nA_padded = np.pad(A, padding = 1, mode = \"constant\")\nAlso before proceeding with the convolution, the kernel must be flipped Left-Right and then Upside-Down  \\(ker = \\begin{bmatrix}a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} ⟶ \\begin{bmatrix}c & b & a \\\\ f & e & d \\\\ i & h & g \\end{bmatrix} ⟶ \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = ker'\\) \nThis is achieved as:\nker_flipped = np.flipud(np.fliplr(ker))\nfliplr denoting a left-right flip and flipud denoting a up-down flip. Choose a stride of length 1 and perform the convolution as the dot product of kernel sized chunks of \\(A\\) with the \\(ker\\):\n\\(\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 4 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_1\\)  \\(\\begin{bmatrix}0 & 0 & 0 \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_2\\)  \\(\\vdots\\) \n\\(\\begin{bmatrix}5 & 6 & 0 \\\\ 8 & 9 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_N\\)  Notice the dimensions of the final output matrix:\n\\[\\begin{equation}\nR_{\\text{height}} = \\frac{A_{\\text{height}} + 2\\cdot\\text{padding} - ker_{\\text{height}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\\[\\begin{equation}\nR_{\\text{width}} = \\frac{A_{\\text{width}} + 2\\cdot\\text{padding} - ker_{\\text{width}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\nprint(Gx); print()\nprint(np.fliplr(Gx)); print()\nprint(np.flipud(Gx))\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n[[-1.  0.  1.]\n [-2.  0.  2.]\n [-1.  0.  1.]]\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n\n\ndef convolve2d(image, kernel, padding, stride):\n    image_height, image_width = image.shape\n    kernel_height, kernel_width = kernel.shape\n\n    output_height = (image_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (image_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    kernel = np.flipud(np.fliplr(kernel))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            output[i, j] = np.sum(padded_image[i : i + kernel_height, j : j+kernel_width] * kernel)\n\n    return output\n\n\nimgX = convolve2d(grayscale_image, Gx, 1, 1)\nimgY = convolve2d(grayscale_image, Gy, 1, 1)\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\nax[0].imshow(imgX, cmap = \"gray\")\nax[0].set_title(\"X-Gradient\")\nax[1].imshow(imgY, cmap = \"gray\")\nax[1].set_title(\"Y-Gradient\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsobel_final = np.sqrt(imgX**2 + imgY**2)\nplt.imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(img)\nax[1].imshow(grayscale_image, cmap = \"gray\")\nax[2].imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.imsave(\"Sobel.jpeg\", sobel_final, cmap = \"gray\")\n\n\n\n\n\ndef edge_detect(image_org):\n    padding, stride = 1, 1\n\n    rgb_weights = [0.2989, 0.5870, 0.1140]\n    image = np.dot(image_org, rgb_weights)\n\n    Gx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\n    Gy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])\n\n    image_height, image_width = image.shape\n\n    output_height = (image_height + 2 * padding - 3) // stride + 1\n    output_width = (image_width + 2 * padding - 3) // stride + 1\n    A_sobel = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    Gx = np.flipud(np.fliplr(Gx))\n    Gy = np.flipud(np.fliplr(Gy))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            A_sobel[i, j] = (np.sum(padded_image[i : i + 3, j : j + 3] * Gx)**2 + np.sum(padded_image[i : i + 3, j : j + 3] * Gy)**2)**0.5\n\n    plt.imsave(\"Edge.jpeg\", A_sobel, cmap = \"gray\")\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\n    ax[0].imshow(image_org)\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(A_sobel, cmap = \"gray\")\n    ax[1].set_title(\"Edge-Detected\")\n    plt.show()\n\n\nedge_detect(img)"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-rgb-channels",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-rgb-channels",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "imgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-grayscale-image",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#the-grayscale-image",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "rgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(3264, 4928)\n[[198.8518 188.8528 190.8526 ... 227.0294 217.0304 207.0314]\n [205.8511 195.8521 195.8521 ... 224.0297 222.0299 221.03  ]\n [207.8509 198.8518 196.852  ... 219.0302 225.0296 230.0291]\n ...\n [ 96.361   95.3611  99.3607 ...  81.8203  84.2761  82.2763]\n [ 96.361   94.3612  95.073  ...  83.8201  82.9881  78.9885]\n [ 97.3609  93.3613  92.0733 ...  69.8215  75.9996  78.9993]]"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#sobel-operators---the-edge-detecting-kernels",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#sobel-operators---the-edge-detecting-kernels",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "\\(G_x = \\begin{bmatrix}1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix}\\) and \\(G_y = \\begin{bmatrix}1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix}\\)\n\nGx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\nGy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#implementing-the-2d-convolution",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#implementing-the-2d-convolution",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "\\(A = \\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}\\) when zero padded by 1 pixel gives: \\(A' = \\begin{bmatrix}0 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 2 & 3 & 0 \\\\ 0 & 4 & 5 & 6 & 0 \\\\ 0 & 7 & 8 & 9 & 0 \\\\ 0 & 0 & 0 & 0 & 0\\end{bmatrix}\\)  This is achieved using the NumPy’s .pad() function.\nA_padded = np.pad(A, padding = 1, mode = \"constant\")\nAlso before proceeding with the convolution, the kernel must be flipped Left-Right and then Upside-Down  \\(ker = \\begin{bmatrix}a & b & c \\\\ d & e & f \\\\ g & h & i \\end{bmatrix} ⟶ \\begin{bmatrix}c & b & a \\\\ f & e & d \\\\ i & h & g \\end{bmatrix} ⟶ \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = ker'\\) \nThis is achieved as:\nker_flipped = np.flipud(np.fliplr(ker))\nfliplr denoting a left-right flip and flipud denoting a up-down flip. Choose a stride of length 1 and perform the convolution as the dot product of kernel sized chunks of \\(A\\) with the \\(ker\\):\n\\(\\begin{bmatrix}0 & 0 & 0 \\\\ 0 & 1 & 2 \\\\ 0 & 4 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_1\\)  \\(\\begin{bmatrix}0 & 0 & 0 \\\\ 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_2\\)  \\(\\vdots\\) \n\\(\\begin{bmatrix}5 & 6 & 0 \\\\ 8 & 9 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\cdot \\begin{bmatrix}i & h & g \\\\ f & e & d \\\\ c & b & a \\end{bmatrix} = elt_N\\)  Notice the dimensions of the final output matrix:\n\\[\\begin{equation}\nR_{\\text{height}} = \\frac{A_{\\text{height}} + 2\\cdot\\text{padding} - ker_{\\text{height}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\\[\\begin{equation}\nR_{\\text{width}} = \\frac{A_{\\text{width}} + 2\\cdot\\text{padding} - ker_{\\text{width}}}{\\text{stride}} + 1\n\\end{equation}\\]\n\nprint(Gx); print()\nprint(np.fliplr(Gx)); print()\nprint(np.flipud(Gx))\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n[[-1.  0.  1.]\n [-2.  0.  2.]\n [-1.  0.  1.]]\n\n[[ 1.  0. -1.]\n [ 2.  0. -2.]\n [ 1.  0. -1.]]\n\n\n\ndef convolve2d(image, kernel, padding, stride):\n    image_height, image_width = image.shape\n    kernel_height, kernel_width = kernel.shape\n\n    output_height = (image_height + 2 * padding - kernel_height) // stride + 1\n    output_width = (image_width + 2 * padding - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    kernel = np.flipud(np.fliplr(kernel))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            output[i, j] = np.sum(padded_image[i : i + kernel_height, j : j+kernel_width] * kernel)\n\n    return output\n\n\nimgX = convolve2d(grayscale_image, Gx, 1, 1)\nimgY = convolve2d(grayscale_image, Gy, 1, 1)\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\nax[0].imshow(imgX, cmap = \"gray\")\nax[0].set_title(\"X-Gradient\")\nax[1].imshow(imgY, cmap = \"gray\")\nax[1].set_title(\"Y-Gradient\")\nplt.show()\n\n\n\n\n\n\n\n\n\nsobel_final = np.sqrt(imgX**2 + imgY**2)\nplt.imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(img)\nax[1].imshow(grayscale_image, cmap = \"gray\")\nax[2].imshow(sobel_final, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.imsave(\"Sobel.jpeg\", sobel_final, cmap = \"gray\")"
  },
  {
    "objectID": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#wrapped-up-function",
    "href": "posts/2D-Convolution/ML_Scratch_2D_Convolution.html#wrapped-up-function",
    "title": "Edge Detecting Filters using Sobel Operators",
    "section": "",
    "text": "def edge_detect(image_org):\n    padding, stride = 1, 1\n\n    rgb_weights = [0.2989, 0.5870, 0.1140]\n    image = np.dot(image_org, rgb_weights)\n\n    Gx = np.array([[1.0, 0.0, -1.0], [2.0, 0.0, -2.0], [1.0, 0.0, -1.0]])\n    Gy = np.array([[1.0, 2.0, 1.0], [0.0, 0.0, 0.0], [-1.0, -2.0, -1.0]])\n\n    image_height, image_width = image.shape\n\n    output_height = (image_height + 2 * padding - 3) // stride + 1\n    output_width = (image_width + 2 * padding - 3) // stride + 1\n    A_sobel = np.zeros((output_height, output_width))\n\n    padded_image = np.pad(image, padding, mode = \"constant\")\n    Gx = np.flipud(np.fliplr(Gx))\n    Gy = np.flipud(np.fliplr(Gy))\n\n    for i in range(0, output_height, stride):\n        for j in range(0, output_width, stride):\n            A_sobel[i, j] = (np.sum(padded_image[i : i + 3, j : j + 3] * Gx)**2 + np.sum(padded_image[i : i + 3, j : j + 3] * Gy)**2)**0.5\n\n    plt.imsave(\"Edge.jpeg\", A_sobel, cmap = \"gray\")\n    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15, 15))\n    ax[0].imshow(image_org)\n    ax[0].set_title(\"Original Image\")\n    ax[1].imshow(A_sobel, cmap = \"gray\")\n    ax[1].set_title(\"Edge-Detected\")\n    plt.show()\n\n\nedge_detect(img)"
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n# !pip3 install ipympl\n%matplotlib widget\nfrom google.colab import output\noutput.enable_custom_widget_manager()"
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#particle-as-an-object",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#particle-as-an-object",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "Particle as an Object",
    "text": "Particle as an Object\nIn order to create an object, we create a class - which acts as the blueprint for creating several new objects. The class defines the attributes and the behaviour of the methods that our instances of the class (our objects) will inherit.\nNew categories of objects may be created by extending the attributes and methods of some standard class. This allows us to add on deeper functionality while still preserving the fundamental structure of our objects.\nFor Example: The Particle class may be extended by Electron class."
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#attributes-and-methods-of-the-particle-class",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#attributes-and-methods-of-the-particle-class",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "Attributes and Methods of the Particle class",
    "text": "Attributes and Methods of the Particle class\n\nAttributes\n\n\n\nMass\n\n\nPosition in 3D space \\((x, y, z)\\)\n\n\nVelocity in 3D space \\((v_x, v_y, v_z)\\)\n\n\n\nMethods\n\n\n\nmove(\\(t\\))\n\n\ndistance_from_orgin(\\(x, y, z\\))\n\n\n\nclass Particle:\n  def __init__(self, m, x, y, z, vx, vy, vz):\n    self.m = m\n    self.x = x\n    self.y = y\n    self.z = z\n    self.vx = vx\n    self.vy = vy\n    self.vz = vz\n\n  def move(self, t):\n    self.x += self.vx * t\n    self.y += self.vy * t\n    self.z += self.vz * t\n\n  def origin(self):\n    return np.sqrt(self.x**2 + self.y**2 + self.z**2)\n\n  def plot(self):\n    fig = plt.figure(figsize = (5, 3))\n    ax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\n    ax.scatter(self.x, self.y, self.z, c = \"red\", s = 50, zorder = 1)\n    plt.show()\n\n  def compare_plot(self, point):\n    fig = plt.figure(figsize = (5, 3))\n    ax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\n    ax.scatter(self.x, self.y, self.z, c = \"red\", s = 50, zorder = 1)\n    ax.scatter(point[0], point[1], point[2], c = \"blue\", s = 50, zorder = 1)\n    plt.show()\n\n\np1 = Particle(5, 1, 1, 0, -1, 0, 1)\n\n\np1.origin()\n\n1.4142135623730951\n\n\n\np1.plot()\n\n\n\n\n\np1.move(10)\n\n\nprint(p1.x, p1.y, p1.z)\n\n-9 1 10\n\n\n\np1.origin()\n\n13.490737563232042\n\n\n\np1.plot()"
  },
  {
    "objectID": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#inheritence-of-class",
    "href": "posts/Object-Oriented-Programming/ML_Scratch_OOP.html#inheritence-of-class",
    "title": "Modelling of Particles using Object-Oriented Programming",
    "section": "Inheritence of Class",
    "text": "Inheritence of Class\nElectron inherits all the characteristics of the Particle class and in addition, adds on more functionality to it. 1. Additional Attributes - * mass \\(m = 9.11 \\times 10^{-31} kg\\) - * charge \\(q = -1.602 \\times 10^{-27} C\\)\n\nChanges to the move() method\n\n\n\nMay move in an Electric field \\(\\mathbf{E} = (E_x, E_y, E_z)\\) with an acceleration \\(\\mathbf{a} = \\left(\\frac{q\\mathbf{E}}{m}\\right)\\)\n\n\nMotion now described as:\n\n\n\\(\\bar{x} = x + v_xt + \\frac{1}{2}a_xt^2\\)\n\n\n\\(\\bar{y} = y + v_yt + \\frac{1}{2}a_yt^2\\)\n\n\n\\(\\bar{z} = z + v_zt + \\frac{1}{2}a_zt^2\\)\n\n\n\\(\\bar{v_x} = v_x + a_xt\\)\n\n\n\\(\\bar{v_y} = v_y + a_yt\\)\n\n\n\\(\\bar{v_z} = v_z + a_zt\\)\n\n\n\nclass Electron(Particle):\n  def __init__(self, x, y, z, vx, vy, vz):\n    self.m = 9.11e-31\n    self.q = -1.602e-19\n    super(Electron, self).__init__(self.m, x, y, z, vx, vy, vz)\n\n  def move(self, t, Ex, Ey, Ez):\n    self.x += self.vx * t + (1/2) * ((self.q * Ex)/self.m) * t**2\n    self.y += self.vy * t + (1/2) * ((self.q * Ey)/self.m) * t**2\n    self.z += self.vz * t + (1/2) * ((self.q * Ez)/self.m) * t**2\n    self.vx += ((self.q * Ex)/self.m) * t\n    self.vy += ((self.q * Ey)/self.m) * t\n    self.vz += ((self.q * Ez)/self.m) * t\n\n  def position(self):\n    return np.array([self.x, self.y, self.z])\n\n  def velocity(self):\n    return np.array([self.vx, self.vy, self.vz])\n\n\ne1 = Electron(0, 1, 2, 1, 0, 0)\n\n\ne1.origin()\n\n2.23606797749979\n\n\n\ne1.plot()\n\n\n\n\n\ne1.velocity()\n\narray([1, 0, 0])\n\n\n\ne1.position()\n\narray([0, 1, 2])\n\n\n\ne1.move(2, 10e-10, 0, 1.2e-6)\n\n\ne1.position()\n\narray([-3.49701427e+02,  1.00000000e+00, -4.22039712e+05])\n\n\n\ne1.velocity()\n\narray([-3.50701427e+02,  0.00000000e+00, -4.22041712e+05])\n\n\n\ne1.compare_plot([0, 1, 2])"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html",
    "href": "posts/Pooling/ML_Scratch_Pooling.html",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import gaussian_filter\n\n\nimg = plt.imread(\"ImageSq.jpeg\")\nNx, Ny, Nz = np.shape(img)\nprint(f\"Height: {Nx}, Width: {Ny}, RGB: {Nz}\")\nplt.imshow(img)\nplt.show()\n\nHeight: 6069, Width: 4855, RGB: 3\n\n\n\n\n\n\n\n\n\n\nprint(img)\n\n[[[ 86  62  38]\n  [ 86  62  38]\n  [ 86  62  38]\n  ...\n  [164 132 109]\n  [164 132 109]\n  [164 132 109]]\n\n [[ 86  62  38]\n  [ 86  62  38]\n  [ 86  62  38]\n  ...\n  [164 132 109]\n  [164 132 109]\n  [164 132 109]]\n\n [[ 86  62  38]\n  [ 86  62  38]\n  [ 86  62  38]\n  ...\n  [164 132 109]\n  [164 132 109]\n  [164 132 109]]\n\n ...\n\n [[156 129 100]\n  [156 129 100]\n  [156 129 100]\n  ...\n  [146 118  94]\n  [146 118  94]\n  [146 118  94]]\n\n [[156 129 100]\n  [156 129 100]\n  [156 129 100]\n  ...\n  [146 118  94]\n  [146 118  94]\n  [146 118  94]]\n\n [[156 129 100]\n  [156 129 100]\n  [156 129 100]\n  ...\n  [146 118  94]\n  [146 118  94]\n  [146 118  94]]]\n\n\n\n\n\nimgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nrgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(6069, 4855)\n[[ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n ...\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]]\n\n\n\n\n\n\ndef max_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.max(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgnew = max_pool(img, (3, 3), 3)\n\n\nimgnew\n\narray([[[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       ...,\n\n       [[160, 130, 102],\n        [160, 130, 102],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[159, 129, 101],\n        [159, 129, 101],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[156, 129, 100],\n        [156, 129, 100],\n        [156, 129, 100],\n        ...,\n        [148, 120,  96],\n        [146, 118,  94],\n        [146, 118,  94]]], dtype=uint8)\n\n\n\nimgnew.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgnew)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"MaxPooled.jpeg\", imgnew)\n\n\nimgnew1 = max_pool(imgnew, (3, 3), 3)\n\n\nimgnew1.shape\n\n(674, 539, 3)\n\n\n\n\n\n\nplt.imshow(imgnew1)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"FurtherMaxPooled.jpeg\", imgnew1)\n\n\n\n\n\n\ndef avg_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.mean(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgavg = avg_pool(img, (3, 3), 3)\nimgavg.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgavg)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"AvgPooled.jpeg\", imgavg)"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#the-rgb-channels",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#the-rgb-channels",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "imgR, imgG, imgB = img.copy(), img.copy(), img.copy()\nimgR[:, :, (1, 2)] = 0\nimgG[:, :, (0, 2)] = 0\nimgB[:, :, (0, 1)] = 0\nfig, ax = plt.subplots(nrows = 1, ncols = 3, figsize=(15, 15))\nax[0].imshow(imgR)\nax[1].imshow(imgG)\nax[2].imshow(imgB)\nplt.show()"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#the-grayscale-image",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#the-grayscale-image",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "rgb_weights = [0.2989, 0.5870, 0.1140]\ngrayscale_image = np.dot(img, rgb_weights)\nplt.imshow(grayscale_image, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(np.shape(grayscale_image))\nprint(grayscale_image)\n\n(6069, 4855)\n[[ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n [ 66.4314  66.4314  66.4314 ... 138.9296 138.9296 138.9296]\n ...\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]\n [133.7514 133.7514 133.7514 ... 123.6214 123.6214 123.6214]]"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#max-pooling",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#max-pooling",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "def max_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.max(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgnew = max_pool(img, (3, 3), 3)\n\n\nimgnew\n\narray([[[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       [[ 86,  62,  38],\n        [ 86,  62,  38],\n        [ 86,  62,  38],\n        ...,\n        [164, 132, 109],\n        [164, 132, 109],\n        [164, 132, 109]],\n\n       ...,\n\n       [[160, 130, 102],\n        [160, 130, 102],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[159, 129, 101],\n        [159, 129, 101],\n        [160, 130, 102],\n        ...,\n        [147, 119,  95],\n        [147, 119,  95],\n        [147, 119,  95]],\n\n       [[156, 129, 100],\n        [156, 129, 100],\n        [156, 129, 100],\n        ...,\n        [148, 120,  96],\n        [146, 118,  94],\n        [146, 118,  94]]], dtype=uint8)\n\n\n\nimgnew.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgnew)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"MaxPooled.jpeg\", imgnew)\n\n\nimgnew1 = max_pool(imgnew, (3, 3), 3)\n\n\nimgnew1.shape\n\n(674, 539, 3)\n\n\n\n\n\n\nplt.imshow(imgnew1)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"FurtherMaxPooled.jpeg\", imgnew1)"
  },
  {
    "objectID": "posts/Pooling/ML_Scratch_Pooling.html#average-pooling",
    "href": "posts/Pooling/ML_Scratch_Pooling.html#average-pooling",
    "title": "Average and Max Pooling built from scratch",
    "section": "",
    "text": "def avg_pool(image, kernel_size, stride):\n    image_height, image_width, channels = image.shape\n    kernel_height, kernel_width = kernel_size[0], kernel_size[1]\n\n    output_height = (image_height - kernel_height) // stride + 1\n    output_width = (image_width - kernel_width) // stride + 1\n    output = np.zeros((output_height, output_width, 3))\n\n    for c in range(channels):\n        for i in range(0, output_height * stride, stride):\n            for j in range(0, output_width * stride, stride):\n                output[i // stride, j // stride, c] = np.mean(image[i : i + kernel_height, j : j + kernel_width, c])\n\n    final = output.astype(np.uint8)\n    return final\n\n\nimgavg = avg_pool(img, (3, 3), 3)\nimgavg.shape\n\n(2023, 1618, 3)\n\n\n\n\n\nplt.imshow(imgavg)\n\n\n\n\n\n\n\n\n\nplt.imsave(\"AvgPooled.jpeg\", imgavg)"
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html",
    "href": "posts/Gradient-Descent/Gradient_Descent.html",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "For a multivariable function, in \\(N\\) dimensions let’s say, \\(F(\\mathbf{v})\\) which is differentiable at a point \\(\\mathbf{v}\\), we say that \\(F(\\mathbf{v})\\) decreases fastest in the direction of negative of the gradient at that point \\(\\mathbf{v}\\) denoted by \\(-∇F(\\mathbf{v})\\).\n\n\n\\[\\begin{equation}\n\\mathbf{v}_{i + 1} = \\mathbf{v}_{i} - L \\nabla F(\\mathbf{v}_{i})\n\\end{equation}\\] where, \\(L\\) is the learning rate and \\(L \\in \\mathbb{R}_{+}\\) and \\(\\mathbf{v} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}\\) and \\(\\nabla F(\\mathbf{v}) = \\begin{bmatrix}\n    \\frac{\\partial F}{\\partial x_1} \\\\\n    \\frac{\\partial F}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\\). Hence the equation becomes:\n\\[\\begin{equation}\n\\begin{bmatrix} x_1^{i + 1} \\\\ x_2^{i + 1} \\\\ \\vdots \\\\ x_N^{i + 1} \\end{bmatrix} = \\begin{bmatrix} x_1^i \\\\ x_2^i \\\\ \\vdots \\\\ x_N^i \\end{bmatrix} - L \\begin{bmatrix}\\frac{\\partial F}{\\partial x_1} \\\\ \\frac{\\partial F}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\n\\end{equation}\\]\n\n\n\nNotice that the decrease in \\(F(\\mathbf{v})\\) is guaranteed only to the nearest well, which may or may not be the global minima. We may run for a specified number of epochs or terminate at a set tolerance too.\n\n\n\nLet \\(F(\\mathbf{v}) = F\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\right) = \\sin(x)\\cos(y)\\), then the gradient \\(∇F(\\mathbf{v}) = \\begin{bmatrix}\\frac{\\partial F}{\\partial x} \\\\ \\frac{\\partial F}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\\) and starting from an initial point \\(\\mathbf{v}_0\\), we may reach the nearest local minima as:\n\\[\\begin{equation}\n\\begin{bmatrix} \\bar x \\\\ \\bar y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} - L \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\n\\end{equation}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n!pip3 install ipympl\n%matplotlib widget\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndef gradient_func2d(x, y):\n  return np.cos(x) * np.cos(y), - np.sin(x) * np.sin(y)\n\n\ndef func2d(x, y):\n  return np.sin(x) * np.cos(y)\n\n\n\n\nx = np.arange(-5, 5, 0.1)\ny = np.arange(-5, 5, 0.1)\nX, Y = np.meshgrid(x, y)\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder=False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\")\n\nplt.show()\n\n\n\n\n\n\n\n\np1 = (1, 0.5)\np2 = (-1, 3)\np3 = (-4, -0.5)\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\nax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\nax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\nax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\nplt.show()\n\n\n\n\n\n\n\n\ndef iterate_gradient(point, L):\n  Xgrad, Ygrad = gradient_func2d(point[0], point[1])\n  Xnew, Ynew = point[0] - L * Xgrad, point[1] - L * Ygrad\n  return (Xnew, Ynew, func2d(Xnew, Ynew))\n\n\nL, epochs = 0.1, 100\n\nax = plt.subplot(projection=\"3d\", computed_zorder = False)\n\nfor i in range(epochs):\n  p1 = iterate_gradient(p1, L)\n  p2 = iterate_gradient(p2, L)\n  p3 = iterate_gradient(p3, L)\n\n\n  ax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\n  ax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\n  ax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\n  ax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\n\n  ax.set_title(f\"Epoch No: {i}\")\n\n  plt.pause(0.001)\n  ax.clear()"
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html#evaluating-the-descent",
    "href": "posts/Gradient-Descent/Gradient_Descent.html#evaluating-the-descent",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "\\[\\begin{equation}\n\\mathbf{v}_{i + 1} = \\mathbf{v}_{i} - L \\nabla F(\\mathbf{v}_{i})\n\\end{equation}\\] where, \\(L\\) is the learning rate and \\(L \\in \\mathbb{R}_{+}\\) and \\(\\mathbf{v} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix}\\) and \\(\\nabla F(\\mathbf{v}) = \\begin{bmatrix}\n    \\frac{\\partial F}{\\partial x_1} \\\\\n    \\frac{\\partial F}{\\partial x_2} \\\\\n    \\vdots \\\\\n    \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\\). Hence the equation becomes:\n\\[\\begin{equation}\n\\begin{bmatrix} x_1^{i + 1} \\\\ x_2^{i + 1} \\\\ \\vdots \\\\ x_N^{i + 1} \\end{bmatrix} = \\begin{bmatrix} x_1^i \\\\ x_2^i \\\\ \\vdots \\\\ x_N^i \\end{bmatrix} - L \\begin{bmatrix}\\frac{\\partial F}{\\partial x_1} \\\\ \\frac{\\partial F}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial F}{\\partial x_n}\n\\end{bmatrix}\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html#descent-to-a-minima",
    "href": "posts/Gradient-Descent/Gradient_Descent.html#descent-to-a-minima",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "Notice that the decrease in \\(F(\\mathbf{v})\\) is guaranteed only to the nearest well, which may or may not be the global minima. We may run for a specified number of epochs or terminate at a set tolerance too."
  },
  {
    "objectID": "posts/Gradient-Descent/Gradient_Descent.html#dimensional-example",
    "href": "posts/Gradient-Descent/Gradient_Descent.html#dimensional-example",
    "title": "Visualising Gradient Descent for two variable case",
    "section": "",
    "text": "Let \\(F(\\mathbf{v}) = F\\left(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\right) = \\sin(x)\\cos(y)\\), then the gradient \\(∇F(\\mathbf{v}) = \\begin{bmatrix}\\frac{\\partial F}{\\partial x} \\\\ \\frac{\\partial F}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\\) and starting from an initial point \\(\\mathbf{v}_0\\), we may reach the nearest local minima as:\n\\[\\begin{equation}\n\\begin{bmatrix} \\bar x \\\\ \\bar y \\end{bmatrix} = \\begin{bmatrix} x \\\\ y \\end{bmatrix} - L \\begin{bmatrix} \\cos(x)\\cos(y) \\\\ -\\sin(x)\\sin(y) \\end{bmatrix}\n\\end{equation}\\]\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n!pip3 install ipympl\n%matplotlib widget\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\ndef gradient_func2d(x, y):\n  return np.cos(x) * np.cos(y), - np.sin(x) * np.sin(y)\n\n\ndef func2d(x, y):\n  return np.sin(x) * np.cos(y)\n\n\n\n\nx = np.arange(-5, 5, 0.1)\ny = np.arange(-5, 5, 0.1)\nX, Y = np.meshgrid(x, y)\n\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder=False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\")\n\nplt.show()\n\n\n\n\n\n\n\n\np1 = (1, 0.5)\np2 = (-1, 3)\np3 = (-4, -0.5)\nfig = plt.figure(figsize = (10, 8))\nax = fig.add_subplot(projection=\"3d\", computed_zorder = False)\nax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\nax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\nax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\nax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\nplt.show()\n\n\n\n\n\n\n\n\ndef iterate_gradient(point, L):\n  Xgrad, Ygrad = gradient_func2d(point[0], point[1])\n  Xnew, Ynew = point[0] - L * Xgrad, point[1] - L * Ygrad\n  return (Xnew, Ynew, func2d(Xnew, Ynew))\n\n\nL, epochs = 0.1, 100\n\nax = plt.subplot(projection=\"3d\", computed_zorder = False)\n\nfor i in range(epochs):\n  p1 = iterate_gradient(p1, L)\n  p2 = iterate_gradient(p2, L)\n  p3 = iterate_gradient(p3, L)\n\n\n  ax.plot_surface(X, Y, func2d(X, Y), cmap=\"viridis\", zorder = 0)\n\n  ax.scatter(p1[0], p1[1], func2d(p1[0], p1[1]), c = \"red\", s = 75, zorder = 1)\n  ax.scatter(p2[0], p2[1], func2d(p2[0], p2[1]), c = \"green\", s = 75, zorder = 1)\n  ax.scatter(p3[0], p3[1], func2d(p3[0], p3[1]), c = \"cyan\", s = 75, zorder = 1)\n\n  ax.set_title(f\"Epoch No: {i}\")\n\n  plt.pause(0.001)\n  ax.clear()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Average and Max Pooling built from scratch\n\n\n\nConvolution\n\n\n\n\nGuntas Singh Saran\n\n\nDec 19, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModelling of Particles using Object-Oriented Programming\n\n\n\nOOP\n\n\n\n\nGuntas Singh Saran\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEdge Detecting Filters using Sobel Operators\n\n\n\nConvolution\n\n\n\n\nGuntas Singh Saran\n\n\nDec 18, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvolution operations built from scratch\n\n\n\nConvolution\n\n\n\n\nGuntas Singh Saran\n\n\nDec 17, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualising Gradient Descent for two variable case\n\n\n\nGradient Descent\n\n\n\n\nGuntas Singh Saran\n\n\nDec 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\nLinear Regression\n\n\n\n\nGuntas Singh Saran\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]