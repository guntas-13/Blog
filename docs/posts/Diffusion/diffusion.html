<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Guntas Singh Saran">
<meta name="dcterms.date" content="2024-06-08">
<meta name="description" content="Dive deep into the world of Diffusion Models">

<title>Guntas Blog - Maths behind Diffusion Probabilistic Models</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../tabicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Guntas Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://guntas-13.github.io"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guntas-13"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/guntas-singh-saran-2b8811179/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/guntas.saran13/"> <i class="bi bi-instagram" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guntassingh.saran@iitgn.ac.in"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Maths behind Diffusion Probabilistic Models</h1>
                  <div>
        <div class="description">
          Dive deep into the world of Diffusion Models
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Generative Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Guntas Singh Saran </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 8, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="denoising-diffusion-probabilistic-models" class="level1">
<h1>Denoising Diffusion Probabilistic Models</h1>
<p>As seen in the case of <a href="https://guntas-13.github.io/Blog/posts/Generative/Maths.html">Variational Autoencoders</a>, it all boils down to learning the probability distributions - <span class="math inline">\(p(\textbf{z} | \textbf{x})\)</span> the posterior abstraction of obtaining a hidden representation <span class="math inline">\(\textbf{z}\)</span> given some input image <span class="math inline">\(\textbf{x}\)</span> and the likelihood <span class="math inline">\(p(\textbf{x} | \textbf{z})\)</span> of generating the image samples given some hidden representation <span class="math inline">\(\textbf{z}\)</span>.</p>
<p>Now the most crucial task in all these generative models is trying to understand to relate the objective that we are trying to acheive and what the model actually learns. We’ll see the same confusing conclusion being established by the end of this blog and then we’ll realise how beautifully all the mathematics and the tasks laid out make sense.</p>
<section id="throwback-to-variational-autoencoders" class="level2">
<h2 class="anchored" data-anchor-id="throwback-to-variational-autoencoders">Throwback to Variational Autoencoders</h2>
<p>Like in the case of VAEs<span class="citation" data-cites="weng2018VAE"><a href="#ref-weng2018VAE" role="doc-biblioref">[1]</a></span>, we started off by approximating the actual <span class="math inline">\(P(\textbf{z} | \textbf{x})\)</span> through our probabilistic Encoder <span class="math inline">\(Q_{\phi}(\textbf{z} | \textbf{x})\)</span> and minimising the KL divergence between these two. But in order to establish the knowledge of the actual <span class="math inline">\(P(\textbf{z} | \textbf{x})\)</span>, we went into maximising the log-likelihood of data samples <span class="math inline">\(\textbf{x}\)</span> and eventually made the encoder learn this distribution <span class="math inline">\(Q_{\phi}(\textbf{z} | \textbf{x})\)</span> to be as close to the standard normal <span class="math inline">\(\mathcal{N}(\textbf{0}, \mathbb{I})\)</span> as possible. Hence now drawing any <span class="math inline">\(\textbf{z} \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span> we are sure of it being close to the <span class="math inline">\(\textbf{z}\)</span>’s seen during training, allowing us to discard off the encoder entirely at inference. The way we setup the objective of making the actual and approximated distribution close to each other will stay same for Diffusion Models too and this would allow us to uncover more truth about the actual distribution itself.</p>
<img src="./VAE1.png" class="img-fluid">
<p style="text-align: center; color: #5f9ea0;">
Graphical Model of a Variation Autoencoder.<br>Adapted from <span class="citation" data-cites="weng2021diffusion"><a href="#ref-weng2021diffusion" role="doc-biblioref">[2]</a></span>.
</p>
</section>
<section id="what-are-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="what-are-diffusion-models">What are Diffusion Models?</h2>
<p>For Diffusion Models instead of one latent variable <span class="math inline">\(\textbf{z}\)</span>, we have <span class="math inline">\(T\)</span> latent variables of the form <span class="math inline">\(\textbf{x}_1, \textbf{x}_2, \cdots , \textbf{x}_T\)</span> of same dimension as the input image <span class="math inline">\(\textbf{x}_0\)</span>, and the most interesting point is that the forward noising process is a deterministic Markov Chain, wherein Gaussian noise is added in gradual <span class="math inline">\(T\)</span> steps, defined as:</p>
<p><span class="math display">\[ \boxed{q(\textbf{x}_t | \textbf{x}_{t - 1}) = \mathcal{N}(\textbf{x}_{t}; \sqrt{1 - \beta_{t}} \textbf{x}_{t - 1}, \beta_{t} \mathbb{I})} \]</span></p>
<p>Here the variances are controlled by a scheduler <span class="math inline">\(\left \{ \beta_t \in (0, 1) \right \}_{t = 1}^T\)</span>, which means for each noisy sample <span class="math inline">\(\textbf{x}_t\)</span> is sampled from a Gaussian with <span class="math inline">\(\boldsymbol{\mu}_q = \sqrt{1 - \beta_{t}} \textbf{x}_{t - 1}\)</span> and covariance matrix <span class="math inline">\(\mathbf{\Sigma}_q = \beta_{t} \mathbb{I}\)</span>. The idea is then to learn the reverse denoising diffusion distribution <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span>, which is also a <strong>Markov Chain with learned Gaussian transitions</strong><span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span> starting at <span class="math inline">\(p(\textbf{x}_T) \sim \mathcal{N}(\textbf{x}_T; \textbf{0}, \mathbb{I})\)</span>. Therefore it then becomes really important to understand the entire joint distribution <span class="math inline">\(p(\textbf{x}_1, \textbf{x}_2, \cdots, \textbf{x}_T)\)</span> denoted in shorthand as <span class="math inline">\(p(\textbf{x}_{0:T})\)</span></p>
<img src="./Diff.png" class="img-fluid">
<p style="text-align: center; color: #5f9ea0;">
Graphical Model of a Diffusion Process.<br>Adapted from <span class="citation" data-cites="weng2021diffusion"><a href="#ref-weng2021diffusion" role="doc-biblioref">[2]</a></span>.
</p>
</section>
<section id="prerequisites" class="level2">
<h2 class="anchored" data-anchor-id="prerequisites">Prerequisites</h2>
<section id="joint-conditional-distribution-of-n-rvs-and-bayes-rule" class="level3">
<h3 class="anchored" data-anchor-id="joint-conditional-distribution-of-n-rvs-and-bayes-rule">Joint &amp; Conditional Distribution of <span class="math inline">\(N\)</span> RVs and Bayes’ Rule</h3>
<p>A joint distribution over <span class="math inline">\(N\)</span> random variables assigns probabilities to all the events involving these <span class="math inline">\(N\)</span> random variables<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, denoted as</p>
<p><span class="math display">\[ P(X_1, X_2, X_3, \cdots, X_N) \]</span></p>
<p>Now starting off with just two RVs, the conditional probabilities <span class="math inline">\(P(X_1 | X_2)\)</span> and <span class="math inline">\(P(X_2 | X_1)\)</span> can be calculated from the joint distribution as:</p>
<p><span class="math display">\[
\begin{align}
P(X_1 | X_2) = \frac{P(X_1, X_2)}{P(X_2)} &amp;&amp; P(X_2 | X_1) = \frac{P(X_1, X_2)}{P(X_1)}
\end{align}
\]</span></p>
<p>Conveniently this allows us to write <span class="math inline">\(P(X_1, X_2) = P(X_2 | X_1) P(X_1)\)</span>. And similarly for <span class="math inline">\(N\)</span> random variables</p>
<p><span class="math display">\[
\begin{align}
P(X_1, X_2, X_3, \cdots, X_N) &amp;= P(X_2, X_3, \cdots, X_N | X_1) \cdot P(X_1) \\
&amp;= P(X_3, \cdots, X_N | X_1, X_2) \cdot P(X_2 | X_1) \cdot P(X_1) \\
&amp;= P(X_4, \cdots, X_N | X_1, X_2, X_3) \cdot P(X_3 | X_2, X_1) \cdot P(X_2 | X_1) \cdot P(X_1)
\end{align}
\]</span></p>
<p>Expanding by chain rule we get <span class="math display">\[ \boxed{P(X_1, X_2, X_3, \cdots, X_N) = P(X_1) \cdot \prod_{i = 2}^{N} P(X_i | X_1^{i - 1})} \]</span></p>
<p>where <span class="math inline">\(X_1^{i - 1} = X_1, X_2, X_3, \cdots, X_{i - 1}\)</span> and then using the joint-conditional-marginal formula, we get the <strong>Bayes’ Rule</strong> as</p>
<p><span class="math display">\[ P(X_2 | X_1) = \frac{P(X_1 | X_2) \cdot P(X_2)}{P(X_1)} \]</span></p>
</section>
<section id="markov-diffusion-process-and-reparametrization" class="level3">
<h3 class="anchored" data-anchor-id="markov-diffusion-process-and-reparametrization">Markov-Diffusion Process and Reparametrization</h3>
<p>The special property of a Markov Process is that the future state is dependent <strong>only on previous state</strong>, which means</p>
<p><span class="math display">\[ P(\textbf{x}_t | \textbf{x}_{t - 1}, \cdots, \textbf{x}_0) = P(\textbf{x}_t | \textbf{x}_{t - 1}) \]</span></p>
<p>the utility of this is that using the chain rule of joint probability, we may simply write for all our diffusion process forward steps <span class="math inline">\(q(\textbf{x}_{1 : T} | \textbf{x}_0)\)</span> as</p>
<p><span class="math display">\[
\begin{align}
q(\textbf{x}_{1 : T} | \textbf{x}_0) = \prod_{t = 1}^{T} q(\textbf{x}_t | \textbf{x}_{t - 1}) &amp;&amp; q(\textbf{x}_t | \textbf{x}_{t - 1}) = \mathcal{N}(\textbf{x}_{t}; \sqrt{1 - \beta_{t}} \textbf{x}_{t - 1}, \beta_{t} \mathbb{I})
\end{align}
\]</span></p>
<p><strong>Diffusion</strong> is the process of converting samples from a <strong>complex distribution</strong> (the data here) <span class="math inline">\(\textbf{x}_0 \sim q(\textbf{x}_0)\)</span> to samples of a <strong>simple distribution</strong> (isotropic Gaussian noise) <span class="math inline">\(\textbf{x}_T \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span>. One can also observe that there is a <span class="math inline">\(\color{purple}{\text{deterministic}}\)</span> and a <span class="math inline">\(\color{blue}{\text{stochastic}}\)</span> component even in our case. Since any RV can be reparametrized as <span class="math inline">\(Z = \sigma X + \mu\)</span>, hence we denote the <span class="math inline">\(\textbf{x}_t\)</span> being drawn from <span class="math inline">\(q(\textbf{x}_t | \textbf{x}_{t - 1})\)</span> as</p>
<p><span class="math display">\[ \boxed{\textbf{x}_t = \color{purple}{\sqrt{1 - \beta_t} \textbf{x}_{t - 1}} + \color{blue}{\sqrt{\beta_t} \boldsymbol{\epsilon}_{t - 1}}} \]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\epsilon}_{t - 1}, \boldsymbol{\epsilon}_{t - 2}, \cdots \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span></p>
<img src="./Diff1.png" class="img-fluid">
<p style="text-align: center; color: #5f9ea0;">
Forward and Reverse Diffusion Processes.<br>Adapted from <span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span>.
</p>
</section>
</section>
<section id="understanding-the-forward-markov-process" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-forward-markov-process">Understanding the Forward Markov Process</h2>
<p>One might wonder why does following the above said markov chain of gaussians lead to <span class="math inline">\(\textbf{x}_T \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span>. To understand this let’s take arbitrary constants for the above</p>
<p><span class="math display">\[ \textbf{x}_t = \sqrt{\alpha} \textbf{x}_{t - 1} + \sqrt{\beta} \boldsymbol{\epsilon}_{t - 1} \]</span></p>
<p>Since we know that <span class="math inline">\(\textbf{x}_T \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span> so let’s open up the formula from this end</p>
<p><span class="math display">\[
\begin{align}
\textbf{x}_T &amp;= \sqrt{\alpha} \textbf{x}_{T - 1} + \sqrt{\beta} \mathcal{N}(\textbf{0}, \mathbb{I}) \\
&amp;= \sqrt{\alpha} (\sqrt{\alpha} \textbf{x}_{T - 2} + \sqrt{\beta} \mathcal{N}(\textbf{0}, \mathbb{I})) + \sqrt{\beta} \mathcal{N}(\textbf{0}, \mathbb{I}) \\
&amp;= (\sqrt{\alpha})^2 \textbf{x}_{T - 2} + \sqrt{\alpha} \sqrt{\beta} \mathcal{N}(\textbf{0}, \mathbb{I}) + \sqrt{\beta} \mathcal{N}(\textbf{0}, \mathbb{I}) \\
\cdots \\
&amp;= (\sqrt{\alpha})^T \textbf{x}_0 + \sqrt{\beta} ((\sqrt{\alpha})^{T - 1} \mathcal{N}(\textbf{0}, \mathbb{I}) + (\sqrt{\alpha})^{T - 2} \mathcal{N}(\textbf{0}, \mathbb{I}) + \cdots + \mathcal{N}(\textbf{0}, \mathbb{I}))
\end{align}
\]</span></p>
<p>We can combine the independent Gaussians into one Gaussian<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> as they have <strong>variances</strong> as <span class="math inline">\((\beta \alpha^{T - 1}, \beta \alpha^{T - 2}, \cdots, \beta \alpha, \beta)\)</span> with <span class="math inline">\(\sigma^2 = \beta \frac{1 - \alpha^T}{1 - \alpha}\)</span>. Notice that as <span class="math inline">\(T \to \infty, (\sqrt{\alpha})^T \to 0\)</span> and <span class="math inline">\(\textbf{x}_T \to \mathcal{N}(\textbf{0}, \mathbb{I})\)</span> only when <span class="math inline">\(\alpha = 1 - \beta\)</span>.</p>
<section id="do-we-traverse-for-all-t-steps" class="level3">
<h3 class="anchored" data-anchor-id="do-we-traverse-for-all-t-steps">Do we traverse for all <span class="math inline">\(T\)</span> steps?</h3>
<p>Certainly Not! Here’s how the Markov Process allows us to reach any <span class="math inline">\(\textbf{x}_t\)</span> from the image <span class="math inline">\(\textbf{x}_0\)</span>. Let <span class="math inline">\(\alpha_t = 1 - \beta_t\)</span></p>
<p><span class="math display">\[
\begin{align}
\textbf{x}_t &amp;= \sqrt{\alpha_t} \textbf{x}_{t - 1} + \sqrt{1 - \alpha_t} \mathcal{N}(\textbf{0}, \mathbb{I}) \\
&amp;= \sqrt{\alpha_t} (\sqrt{\alpha_{t - 1}} \textbf{x}_{t - 2} + \sqrt{1 - \alpha_{t - 1}} \mathcal{N}(\textbf{0}, \mathbb{I})) + \sqrt{1 - \alpha_t} \mathcal{N}(\textbf{0}, \mathbb{I}) \\
&amp;= (\sqrt{\alpha_t \alpha_{t - 1}}) \textbf{x}_{t - 2} + (\sqrt{\alpha_t} \sqrt{1 - \alpha_{t - 1}}) \mathcal{N}(\textbf{0}, \mathbb{I}) + (\sqrt{1 - \alpha_t}) \mathcal{N}(\textbf{0}, \mathbb{I})
\end{align}
\]</span></p>
<p>Combine the independent Gaussians with <span class="math inline">\(\sigma^2 = \alpha_t (1 - \alpha_{t - 1}) + (1 - \alpha_t) = 1 - \alpha_t \alpha_{t - 1}\)</span>, hence</p>
<p><span class="math display">\[
\begin{align}
\textbf{x}_t &amp;= (\sqrt{\alpha_t \alpha_{t - 1}}) \textbf{x}_{t - 2} + (\sqrt{1 - \alpha_t \alpha_{t - 1}})\mathcal{N}(\textbf{0}, \mathbb{I}) \\
\cdots \\
&amp;= (\sqrt{\alpha_t \alpha_{t - 1} \cdots \alpha_2 \alpha_1}) \textbf{x}_0 + (\sqrt{1 - \alpha_t \alpha_{t - 1} \cdots \alpha_2 \alpha_1}) \mathcal{N}(\textbf{0}, \mathbb{I})
\end{align}
\]</span></p>
<p>With <span class="math inline">\(\bar{\alpha_t} = \prod_{i = 1}^{t} \alpha_i\)</span>, we finally get</p>
<p><span class="math display">\[ \boxed{\textbf{x}_t = (\sqrt{\bar{\alpha_t}}) \textbf{x}_0 + (\sqrt{1 - \bar{\alpha_t}}) \mathcal{N}(\textbf{0}, \mathbb{I})} \]</span></p>
<p><span class="math display">\[ \boxed{q(\textbf{x}_t | \textbf{x}_0) = \mathcal{N}(\textbf{x}_t; (\sqrt{\bar{\alpha_t}}) \textbf{x}_0, (1 - \bar{\alpha_t}) \mathbb{I})} \]</span></p>
</section>
</section>
<section id="the-crucial-reverse-diffusion-process" class="level2">
<h2 class="anchored" data-anchor-id="the-crucial-reverse-diffusion-process">The Crucial Reverse Diffusion Process</h2>
<p>If we can reverse the above process and sample from <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span>, we will be able to recreate the true sample from a Gaussian noise input, <span class="math inline">\(\textbf{x}_T \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span> . Note that if <span class="math inline">\(\beta_t\)</span> is small enough, <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span> will also be Gaussian. Unfortunately, we cannot easily estimate <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span> because it needs to use the entire dataset and therefore we need to learn a model <span class="math inline">\(p_{\theta}\)</span> to approximate these conditional probabilities in order to run the reverse diffusion process. The actual reverse distribution</p>
<p><span class="math display">\[ q(\textbf{x}_{t - 1} | \textbf{x}_t) = \mathcal{N}(\textbf{x}_{t - 1}; \boldsymbol{\mu}_q, \mathbf{\Sigma}_q) \]</span></p>
<p>And the approximated distribution can be represented as</p>
<p><span class="math display">\[
\begin{align}
p_{\theta}(\textbf{x}_{0 : T}) = p(\textbf{x}_T) \prod_{t = 1}^T p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t) &amp;&amp; p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t) = \mathcal{N}(\textbf{x}_{t - 1}; \boldsymbol{\mu}_{\theta}(\textbf{x}_t, t), \mathbf{\Sigma}_{\theta}(\textbf{x}_t, t))
\end{align}
\]</span></p>
<p>Before moving onto defining the objective to find the approximate <span class="math inline">\(p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span>, its noteworthy to understand the actual reverse process distribution <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span>. As stated by <span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span>, the reverse conditional distribution is tractable when condition on <span class="math inline">\(\textbf{x}_0\)</span> and since this a Markov Process, we can safely introduce this <span class="math inline">\(\textbf{x}_0\)</span> in the joint conditional part and then we may expand it by <strong>Bayes’ Rule</strong></p>
<p><span class="math display">\[ q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) = \frac{q(\textbf{x}_t | \textbf{x}_{t - 1}, \textbf{x}_0) \cdot q(\textbf{x}_{t - 1} | \textbf{x}_0)}{q(\textbf{x}_t | \textbf{x}_0)} \]</span></p>
<p>Notice that all of these are forward processes, and using<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p><span class="math display">\[ q(\textbf{x}_t | \textbf{x}_{t - 1}, \textbf{x}_0) = q(\textbf{x}_t | \textbf{x}_{t - 1}) = \mathcal{N}(\textbf{x}_t; \sqrt{\alpha_t}\textbf{x}_{t - 1}, (1 - \alpha_t) \mathbb{I}) \propto \text{exp} \left( -\frac{1}{2} \frac{(\textbf{x}_t - \sqrt{\alpha_t}\textbf{x}_{t - 1})^2}{(1 - \alpha_t)} \right) \]</span></p>
<p><span class="math display">\[ q(\textbf{x}_{t - 1} | \textbf{x}_0) = \mathcal{N}(\textbf{x}_{t - 1}; \sqrt{\bar{\alpha}_{t - 1}}\textbf{x}_0, (1 - \bar{\alpha}_{t - 1}) \mathbb{I}) \propto\text{exp} \left( -\frac{1}{2} \frac{(\textbf{x}_{t - 1} - \sqrt{\bar{\alpha}_{t - 1}}\textbf{x}_0)^2}{(1 - \bar{\alpha}_{t - 1})} \right) \]</span></p>
<p><span class="math display">\[ q(\textbf{x}_t | \textbf{x}_0) = \mathcal{N}(\textbf{x}_t; \sqrt{\bar{\alpha}_t}\textbf{x}_0, (1 - \bar{\alpha}_t) \mathbb{I}) \propto\text{exp} \left( -\frac{1}{2} \frac{(\textbf{x}_t - \sqrt{\bar{\alpha}_t}\textbf{x}_0)^2}{(1 - \bar{\alpha}_t)} \right) \]</span></p>
<p>Hence, we may combine all these to get a single Gaussian for <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0)\)</span> as</p>
<p><span class="math display">\[
\begin{align}
q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) &amp;\propto \textbf{exp} \left(-\frac{1}{2} \left( \frac{(\textbf{x}_t - \sqrt{\alpha_t}\textbf{x}_{t - 1})^2}{(1 - \alpha_t)} + \frac{(\textbf{x}_{t - 1} - \sqrt{\bar{\alpha}_{t - 1}}\textbf{x}_0)^2}{(1 - \bar{\alpha}_{t - 1})} + \frac{(\textbf{x}_t - \sqrt{\bar{\alpha}_t}\textbf{x}_0)^2}{(1 - \bar{\alpha}_t)} \right) \right) \\
&amp;= \textbf{exp} \left(-\frac{1}{2} \left( \frac{\textbf{x}_t^2 - 2 \sqrt{\alpha_t} \textbf{x}_t \color{blue}{\textbf{x}_{t - 1}} \color{black}{+ \alpha_t} \color{green}{\textbf{x}^2_{t - 1}}}{(1 - \alpha_t)} + \frac{\bar{\alpha}_{t - 1}\textbf{x}_0^2 - \sqrt{\bar{\alpha}_{t - 1}}\textbf{x}_0 \color{blue}{\textbf{x}_{t - 1}} \color{black}{+} \color{green}{\textbf{x}_{t - 1}^2}}{(1 - \bar{\alpha}_{t - 1})} + \frac{(\textbf{x}_t - \sqrt{\bar{\alpha}_t}\textbf{x}_0)^2}{(1 - \bar{\alpha}_t)} \right) \right) \\
&amp;= \textbf{exp} \left(-\frac{1}{2} \left( \left( \frac{\alpha_t}{1 - \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t - 1}} \right) \color{green}{\textbf{x}^2_{t - 1}} \color{black}{-2 \left( \frac{\sqrt{\alpha_t} \textbf{x}_t}{1 - \alpha_t} + \frac{\sqrt{\bar{\alpha}_{t - 1}} \textbf{x}_0}{1 - \bar{\alpha}_{t - 1}} \right)} \color{blue}{\textbf{x}_{t - 1}} \color{black}{+ F(\textbf{x}_t, \textbf{x}_0)} \right) \right) \\
&amp;\propto \textbf{exp} \left(-\frac{1}{2 \boldsymbol{\sigma}^2} \left( \color{green}{\textbf{x}^2_{t - 1}} \color{black}{-2 \boldsymbol{\mu}} \color{blue}{\textbf{x}_{t - 1}} \color{black}{+ \boldsymbol{\mu}^2} \right) \right) \\
&amp;\propto \textbf{exp} \left( -\frac{1}{2 \boldsymbol{\sigma}^2} \left( \textbf{x}_{t - 1} - \boldsymbol{\mu} \right)^2 \right)
\end{align}
\]</span></p>
<p>where the variance can be written as</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\sigma}^2 &amp;= 1 / \left( \frac{\alpha_t}{1 - \alpha_t} + \frac{1}{1 - \bar{\alpha}_{t - 1}} \right) \\
&amp;= \frac{(1 - \alpha_t) \cdot (1 - \bar{\alpha}_{t - 1})}{(1 - \bar{\alpha}_t)}
\end{align}
\]</span></p>
<p>and the mean as the weighted mean of <span class="math inline">\(\textbf{x}_t\)</span> and <span class="math inline">\(\textbf{x}_0\)</span> be written as</p>
<p><span class="math display">\[
\begin{align}
\boldsymbol{\mu} &amp;= \frac{(1 - \alpha_t) \cdot (1 - \bar{\alpha}_{t - 1})}{(1 - \bar{\alpha}_t)} \left( \frac{\sqrt{\alpha_t}}{1 - \alpha_t} \textbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t - 1}}}{1 - \bar{\alpha}_{t - 1}}\textbf{x}_0 \right) \\
&amp;= \frac{(1 - \bar{\alpha}_{t - 1}) \sqrt{\alpha_t}}{1 - \bar{\alpha}_t} \textbf{x}_t + \frac{(1 - \alpha_t)\sqrt{\bar{\alpha}_{t - 1}}}{1 - \bar{\alpha}_t}\textbf{x}_0 \\
&amp;= \frac{(1 - \bar{\alpha}_{t - 1}) \sqrt{\alpha_t}}{1 - \bar{\alpha}_t} \textbf{x}_t + \frac{(1 - \alpha_t)\sqrt{\bar{\alpha}_{t - 1}}}{1 - \bar{\alpha}_t} \left( \frac{1}{\sqrt{\bar{\alpha}_t}} (\textbf{x}_t - \sqrt{1 - \bar{\alpha}_t} \boldsymbol{\epsilon}_t) \right) \\
&amp; (\because \textbf{x}_t = \sqrt{\bar{\alpha_t}} \textbf{x}_0 + (\sqrt{1 - \bar{\alpha_t}}) \boldsymbol{\epsilon}_t ) \\
&amp;= \frac{1}{\sqrt{\alpha_t}} \left(\textbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \right)
\end{align}
\]</span></p>
<p>Finally we have the original reverse distribution <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0)\)</span> as</p>
<p><span class="math display">\[ \color{MidnightBlue}{\boxed{q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) = q(\textbf{x}_{t - 1} | \textbf{x}_t) = \mathcal{N}(\textbf{x}_{t - 1}; \boldsymbol{\mu}_q(\textbf{x}_0, \textbf{x}_t), \mathbf{\Sigma}_q(t))}} \]</span></p>
<p><span class="math display">\[ \color{RubineRed}{\boxed{\mathbf{\Sigma}_q(t) = \frac{(1 - \alpha_t) \cdot (1 - \bar{\alpha}_{t - 1})}{(1 - \bar{\alpha}_t)}}} \]</span> <span class="math display">\[ \color{teal}{\boxed{\boldsymbol{\mu}_q(\textbf{x}_t, \textbf{x}_0) = \frac{(1 - \bar{\alpha}_{t - 1}) \sqrt{\alpha_t}}{1 - \bar{\alpha}_t} \textbf{x}_t + \frac{(1 - \alpha_t)\sqrt{\bar{\alpha}_{t - 1}}}{1 - \bar{\alpha}_t}\textbf{x}_0}} \]</span></p>
<p><span class="math display">\[ \color{teal}{\boxed{\boldsymbol{\mu}_q(t) = \frac{1}{\sqrt{\alpha_t}} \left(\textbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \right)}} \]</span></p>
</section>
<section id="the-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="the-loss-function">The Loss Function</h2>
<section id="defining-the-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-loss-function">Defining the Loss Function</h3>
<p>As discussed before we will follow the same methodology as done in VAEs of learning the approximate reverse distribution <span class="math inline">\(p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span> by maximizing the expected log-likelihood of the observed data <span class="math inline">\(p_{\theta}(\textbf{x}_0)\)</span> for <span class="math inline">\(\textbf{x}_0 \sim q(\textbf{x}_0)\)</span></p>
<img src="./Jensen.png" class="img-fluid">
<p style="text-align: center; color: #5f9ea0;">
Jenson’s Inequality. Adapted from <span class="citation" data-cites="jensen"><a href="#ref-jensen" role="doc-biblioref">[4]</a></span>.
</p>
<p>Using Jensen’s Inequality over the <span class="math inline">\(\log\)</span> function (convex function), hence the expectation of <span class="math inline">\(\log\)</span> is lesser than equal to the <span class="math inline">\(\log\)</span> of expectation.</p>
<p><span class="math display">\[
\begin{align}
L &amp;= \mathbb{E}_{\textbf{x}_0 \sim q(\textbf{x}_0)} \left[ \log p_{\theta}(\textbf{x}_0) \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_0)} \left[ \log \int p_{\theta}(\textbf{x}_{0 : T}) d\textbf{x}_{1 : T} \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_0)} \left[ \log \int q(\textbf{x}_{1 : T} | \textbf{x}_0) \times \frac{p_{\theta}(\textbf{x}_{0 : T})}{q(\textbf{x}_{1 : T} | \textbf{x}_0)} d\textbf{x}_{1 : T} \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_0)} \left[ \log \left ( \mathbb{E}_{q(\textbf{x}_{1 : T} | \textbf{x}_0)} \left[ \frac{p_{\theta}(\textbf{x}_{0 : T})}{q(\textbf{x}_{1 : T} | \textbf{x}_0)} \right) \right] \right] \\
&amp;\ge \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[\color{OrangeRed}{ \log \frac{p_{\theta}(\textbf{x}_{0 : T})}{q(\textbf{x}_{1 : T} | \textbf{x}_0)}} \color{black}{} \right] \\
\end{align}
\]</span></p>
<p>Notice that both these terms are joint probability distributions with <span class="math inline">\(\color{OrangeRed}{q(\textbf{x}_{1 : T} | \textbf{x}_0)}\)</span> being the actual forward process and <span class="math inline">\(\color{OrangeRed}{p_{\theta}(\textbf{x}_{0 : T})}\)</span> the approximate reverse process. Exapanding these terms out</p>
<p><span class="math display">\[ p_{\theta}(\textbf{x}_{0 : T}) = p_{\theta}(\textbf{x}_T) \cdot \prod_{t = 1}^T p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t) \]</span></p>
<p>Further we’ll condition the forward process on <span class="math inline">\(\textbf{x}_0\)</span> as it would later allow us to expand terms using <strong>Bayes’ Rule</strong><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p><span class="math display">\[
\begin{align}
q(\textbf{x}_{1 : T} | \textbf{x}_0) &amp;= \prod_{t = 1}^T q(\textbf{x}_t | \textbf{x}_{t - 1}) \\
&amp;= q(\textbf{x}_1 | \textbf{x}_0) \cdot \prod_{t = 2}^T q(\textbf{x}_t | \textbf{x}_{t - 1}, \color{orange}{\textbf{x}_0} \color{black}{}) \\
&amp;= q(\textbf{x}_1 | \textbf{x}_0) \cdot \prod_{t = 2}^T \frac{q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) \cdot q(\textbf{x}_t | \textbf{x}_0)}{q(\textbf{x}_{t - 1} | \textbf{x}_0)} \\
&amp;= q(\textbf{x}_1 | \textbf{x}_0) \cdot \frac{q(\textbf{x}_{T - 1} | \textbf{x}_T, \textbf{x}_0)q(\textbf{x}_T | \textbf{x}_0) \cdot q(\textbf{x}_{T - 2} | \textbf{x}_{T - 1}, \textbf{x}_0)q(\textbf{x}_{T - 1} | \textbf{x}_0) \cdots q(\textbf{x}_2 | \textbf{x}_3, \textbf{x}_0)q(\textbf{x}_3 | \textbf{x}_0) \cdot q(\textbf{x}_1 | \textbf{x}_2, \textbf{x}_0)q(\textbf{x}_2 | \textbf{x}_0)}{q(\textbf{x}_{T - 1} | \textbf{x}_0) \cdot q(\textbf{x}_{T - 2} | \textbf{x}_0) \cdots  q(\textbf{x}_2 | \textbf{x}_0) \cdot q(\textbf{x}_1 | \textbf{x}_0)} \\
&amp;= \cancel{q(\textbf{x}_1 | \textbf{x}_0)} \cdot \frac{q(\textbf{x}_{T - 1} | \textbf{x}_T, \textbf{x}_0)q(\textbf{x}_T | \textbf{x}_0) \cdot q(\textbf{x}_{T - 2} | \textbf{x}_{T - 1}, \textbf{x}_0)\cancel{q(\textbf{x}_{T - 1} | \textbf{x}_0)} \cdots q(\textbf{x}_2 | \textbf{x}_3, \textbf{x}_0)\cancel{q(\textbf{x}_3 | \textbf{x}_0)} \cdot q(\textbf{x}_1 | \textbf{x}_2, \textbf{x}_0)\cancel{q(\textbf{x}_2 | \textbf{x}_0)}}{\cancel{q(\textbf{x}_{T - 1} | \textbf{x}_0)} \cdot \cancel{q(\textbf{x}_{T - 2} | \textbf{x}_0)} \cdots  \cancel{q(\textbf{x}_2 | \textbf{x}_0)} \cdot \cancel{q(\textbf{x}_1 | \textbf{x}_0)}} \\
\Aboxed{q(\textbf{x}_{1 : T} | \textbf{x}_0) &amp;= q(\textbf{x}_T | \textbf{x}_0) \cdot \prod_{t = 2}^T q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0)}
\end{align}
\]</span></p>
<p>Substituting it all for the new lower bound loss</p>
<p><span class="math display">\[
\begin{align}
L_{N} &amp;= \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[\log \frac{p_{\theta}(\textbf{x}_{0 : T})}{q(\textbf{x}_{1 : T} | \textbf{x}_0)} \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[\log \frac{p_{\theta}(\textbf{x}_T) \cdot \prod_{t = 1}^T p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)}{q(\textbf{x}_1 | \textbf{x}_0) \cdot \prod_{t = 2}^T q(\textbf{x}_t | \textbf{x}_{t - 1}, \textbf{x}_0)} \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[\log \frac{p_{\theta}(\textbf{x}_T) \cdot \prod_{t = 1}^T p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)}{q(\textbf{x}_1 | \textbf{x}_0) \cdot \prod_{t = 2}^T \frac{q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) \cdot q(\textbf{x}_t | \textbf{x}_0)}{q(\textbf{x}_{t - 1} | \textbf{x}_0)} } \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[\log \frac{p_{\theta}(\textbf{x}_T) \cdot \prod_{t = 1}^T p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)}{q(\textbf{x}_T | \textbf{x}_0) \cdot \prod_{t = 2}^T q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) } \right] \\
&amp;= \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[\log \frac{p_{\theta}(\textbf{x}_T)}{q(\textbf{x}_T | \textbf{x}_0)} + \log p_{\theta}(\textbf{x}_0 | \textbf{x}_1) + \sum_{t = 2}^T \log \frac{p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)}{q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0)} \right] \\
&amp;= \log p_{\theta}(\textbf{x}_0 | \textbf{x}_1) + \mathbb{E}_{q(\textbf{x}_{0 : T})} \left[ \sum_{t = 2}^T \log \frac{p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)}{q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0)} \right] \\
\Aboxed{L_N &amp;= \sum_{t = 2}^T D_{KL} \left(p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t) \parallel q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) \right) + \log p_{\theta}(\textbf{x}_0 | \textbf{x}_1)}
\end{align}
\]</span></p>
</section>
<section id="reparametrization-of-the-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="reparametrization-of-the-loss-function">Reparametrization of the Loss Function</h3>
<p>The above loss function aims to bring the actual reverse <span class="math inline">\(q(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span> and approximated reverse distributions <span class="math inline">\(p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t)\)</span> as close as possible by the means of the <span class="math inline">\(T - 1\)</span> <span class="math inline">\(KL\)</span> Divergence<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> terms. Since we approximate the reverse process using a neural network, the Divergence terms would imply that we want their means to be as close as possible<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="math display">\[ p_{\theta}(\textbf{x}_{t - 1} | \textbf{x}_t) = \mathcal{N}(\textbf{x}_{t - 1}; \boldsymbol{\mu}_{\theta}(\textbf{x}_t, t), \mathbf{\Sigma}_{\theta}(\textbf{x}_t, t)) \]</span></p>
<p><span class="math display">\[ q(\textbf{x}_{t - 1} | \textbf{x}_t) = \mathcal{N}(\textbf{x}_{t - 1}; \left( \frac{(1 - \bar{\alpha}_{t - 1}) \sqrt{\alpha_t}}{1 - \bar{\alpha}_t} \textbf{x}_t + \frac{(1 - \alpha_t)\sqrt{\bar{\alpha}_{t - 1}}}{1 - \bar{\alpha}_t}\textbf{x}_0 \right), \frac{(1 - \alpha_t) \cdot (1 - \bar{\alpha}_{t - 1})}{(1 - \bar{\alpha}_t)} \mathbb{I}) \]</span></p>
<p>As stated in the paper<span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span>, take both their covariances to be same, hence each term of the loss function <span class="math inline">\(L_t\)</span> can be written as</p>
<p><span class="math display">\[ \color{BlueViolet}{\boxed{L_t = \mathbb{E}_{\textbf{x}_0, \boldsymbol{\epsilon}} \left[ \frac{1}{2 \lVert \mathbf{\Sigma}_{\theta}(\textbf{x}_t, t) \rVert^2_2} \lVert \boldsymbol{\mu}_{\theta}(\textbf{x}_t, t) - \boldsymbol{\mu}_t(\textbf{x}_t, \textbf{x}_0) \rVert^2 \right] }} \]</span></p>
<p>The authors<span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span> however define this in terms of the noise prediction. Since <span class="math inline">\(\boldsymbol{\mu}_q(\textbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(\textbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \right)\)</span>, hence for the approximate reverse process distribution, we may write</p>
<p><span class="math display">\[ \boldsymbol{\mu}_{\theta}(\textbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left(\textbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_{\theta}(\textbf{x}_t, t) \right) \]</span></p>
<p>Hence the objective can be re-written as</p>
<p><span class="math display">\[
\begin{align}
L_t &amp;= \mathbb{E}_{\textbf{x}_0, \boldsymbol{\epsilon}} \left[ \frac{1}{2 \lVert \mathbf{\Sigma}_{\theta} \rVert^2_2} \lVert \frac{1}{\sqrt{\alpha_t}} \left(\textbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_{\theta}(\textbf{x}_t, t) \right) - \frac{1}{\sqrt{\alpha_t}} \left(\textbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}} \boldsymbol{\epsilon}_t \right) \rVert^2 \right] \\
&amp;= \mathbb{E}_{\textbf{x}_0, \boldsymbol{\epsilon}} \left[ \frac{(1 - \alpha_t)^2}{2 \alpha_t (1 - \bar{\alpha}_t) \lVert \mathbf{\Sigma}_{\theta} \rVert^2_2} \lVert \boldsymbol{\epsilon}_{\theta}(\textbf{x}_t, t) - \boldsymbol{\epsilon}_t \rVert^2 \right]
\end{align}
\]</span></p>
<p><span class="math display">\[ \color{PineGreen}{\boxed{L_t = \mathbb{E}_{\textbf{x}_0, \boldsymbol{\epsilon}} \left[ \frac{(1 - \alpha_t)^2}{2 \alpha_t (1 - \bar{\alpha}_t) \lVert \mathbf{\Sigma}_{\theta} \rVert^2_2} \lVert \boldsymbol{\epsilon}_{\theta}((\sqrt{\bar{\alpha_t}}) \textbf{x}_0 + (\sqrt{1 - \bar{\alpha_t}})\boldsymbol{\epsilon}_t , t) - \boldsymbol{\epsilon}_t \rVert^2 \right]}} \]</span></p>
<p>Notice how beautifully it wraps down to just making the model to learn to approximate the noising <span class="math inline">\(\boldsymbol{\epsilon}_{\theta}(\textbf{x}_t, t)\)</span> process over any <span class="math inline">\(\textbf{x}_t\)</span> to the actual noise <span class="math inline">\(\boldsymbol{\epsilon}_t \sim \mathcal{N}(\textbf{0}, \mathbb{I})\)</span>. Hence this quite so weird learning process lets us learn the denoising reverse distribution.</p>
</section>
<section id="modified-objective" class="level3">
<h3 class="anchored" data-anchor-id="modified-objective">Modified Objective</h3>
<p>The authors<span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span> further found that training works better by dropping off the constant term entirely, so the final objective is</p>
<p><span class="math display">\[ \color{OrangeRed}{\boxed{L_t^{\text{Simple}} = \mathbb{E}_{t \sim [1, T], \textbf{x}_0, \boldsymbol{\epsilon}_t} \left[\lVert \boldsymbol{\epsilon}_{\theta}((\sqrt{\bar{\alpha_t}}) \textbf{x}_0 + (\sqrt{1 - \bar{\alpha_t}})\boldsymbol{\epsilon}_t , t) - \boldsymbol{\epsilon}_t \rVert^2 \right]}} \]</span></p>
<img src="./DiffAlgo.png" class="img-fluid">
<p style="text-align: center; color: #5f9ea0;">
Training and Inference Algorithms. Adapted from <span class="citation" data-cites="ho2020denoising"><a href="#ref-ho2020denoising" role="doc-biblioref">[3]</a></span>.
</p>
<p>Great <a href="https://www.youtube.com/watch?v=H45lF4sUgiE">YouTube tutorial</a> by <strong>ExplainingAI</strong> really helped understand the whole concept!</p>



</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-weng2018VAE" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">L. Weng, <span>“From autoencoder to beta-VAE,”</span> <em>lilianweng.github.io</em>, 2018, Available: <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">https://lilianweng.github.io/posts/2018-08-12-vae/</a></div>
</div>
<div id="ref-weng2021diffusion" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">L. Weng, <span>“What are diffusion models?”</span> <em>lilianweng.github.io</em>, Jul. 2021, Available: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></div>
</div>
<div id="ref-ho2020denoising" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">J. Ho, A. Jain, and P. Abbeel, <span>“Denoising diffusion probabilistic models.”</span> 2020. Available: <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a></div>
</div>
<div id="ref-jensen" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">F. Bach, <span>“Revisiting the classics: Jensen’s inequality,”</span> <em>https://francisbach.com/</em>, 2023, Available: <a href="https://francisbach.com/jensen-inequality/">https://francisbach.com/jensen-inequality/</a></div>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p><span class="math inline">\(k^N\)</span> values if each RV can take <span class="math inline">\(k\)</span> values<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Two Gaussians with different variances, <span class="math inline">\(\mathcal{N}(\textbf{0}, \sigma_1^2\mathbb{I})\)</span> and <span class="math inline">\(\mathcal{N}(\textbf{0}, \sigma_2^2\mathbb{I})\)</span> can be merged to a new Gaussian distribution <span class="math inline">\(\mathcal{N}(\textbf{0}, (\sigma_1^2 + \sigma_2^2)\mathbb{I})\)</span><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p><span class="math inline">\(\mathcal{N}(\textbf{x}; \boldsymbol{\mu}, \boldsymbol{\sigma}^2) \propto \text{exp}(-\frac{1}{2} \frac{(\textbf{x} - \boldsymbol{\mu})^2}{\boldsymbol{\sigma}^2})\)</span><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p><span class="math inline">\(q(\textbf{x}_t | \textbf{x}_{t - 1}, \textbf{x}_0) = \frac{q(\textbf{x}_{t - 1} | \textbf{x}_t, \textbf{x}_0) \cdot q(\textbf{x}_t | \textbf{x}_0)}{q(\textbf{x}_{t - 1} | \textbf{x}_0)}\)</span><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p><span class="math inline">\(D_{KL}(p \parallel q) = \underset{x \sim p(x)}{\int} p(x) \log \frac{p(x)}{q(x)}\)</span><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>For two Gaussians with same covariance matrices <span class="math inline">\(p = \mathcal{N}(\textbf{x}; \boldsymbol{\mu}_1, \mathbf{\Sigma})\)</span> and <span class="math inline">\(q = \mathcal{N}(\textbf{x}; \boldsymbol{\mu}_2, \mathbf{\Sigma})\)</span>, their <span class="math inline">\(D_{KL}(p \parallel q) = \frac{1}{2 \lVert \mathbf{\Sigma} \rVert^2_2} \lVert \boldsymbol{\mu}_1 - \boldsymbol{\mu}_2 \rVert^2\)</span><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Website made with <a href="https://quarto.org/">Quarto</a>, by Guntas Singh Saran.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guntas-13/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/guntas.saran13/">
      <i class="bi bi-instagram" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/guntas-singh-saran-2b8811179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:guntassingh.saran@iitgn.ac.in">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>