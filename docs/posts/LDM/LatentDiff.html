<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Guntas Singh Saran">
<meta name="dcterms.date" content="2024-07-04">

<title>Guntas Blog - Implementing Latent Diffusion Models over CelebAHQ</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../tabicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Guntas Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://guntas-13.github.io"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guntas-13"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/guntas-singh-saran-2b8811179/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/guntas.saran13/"> <i class="bi bi-instagram" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guntassingh.saran@iitgn.ac.in"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Implementing Latent Diffusion Models over CelebAHQ</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Generative Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Guntas Singh Saran </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 4, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#final-generation" id="toc-final-generation" class="nav-link active" data-scroll-target="#final-generation">Final Generation</a>
  <ul class="collapse">
  <li><a href="#latent-diffusion-models" id="toc-latent-diffusion-models" class="nav-link" data-scroll-target="#latent-diffusion-models"><strong>Latent Diffusion Models</strong></a>
  <ul class="collapse">
  <li><a href="#the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image." id="toc-the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image." class="nav-link" data-scroll-target="#the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image.">The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.</a></li>
  <li><a href="#the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy" id="toc-the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy" class="nav-link" data-scroll-target="#the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy">The downside is that although the <span class="math inline">\(L1/L2\)</span> reconstruction loss might be low, the perceptual features in the reconstructed image still might be <strong>fuzzy</strong></a></li>
  </ul></li>
  <li><a href="#perceptual-retention-textlpips-as-the-metric" id="toc-perceptual-retention-textlpips-as-the-metric" class="nav-link" data-scroll-target="#perceptual-retention-textlpips-as-the-metric">Perceptual Retention &amp; <span class="math inline">\(\text{LPIPS}\)</span> as the metric</a></li>
  <li><a href="#discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes" id="toc-discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes" class="nav-link" data-scroll-target="#discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes">Discretizing the Latent Space using the <span class="math inline">\(\text{CodeBooks}\)</span> from <span class="math inline">\(\text{VQVAEs}\)</span></a>
  <ul class="collapse">
  <li><a href="#textvqvae-as-the-textautoencoder" id="toc-textvqvae-as-the-textautoencoder" class="nav-link" data-scroll-target="#textvqvae-as-the-textautoencoder"><span class="math inline">\(\text{VQVAE}\)</span> as the <span class="math inline">\(\text{AutoEncoder}\)</span></a></li>
  </ul></li>
  <li><a href="#the-autoencoder-architecture" id="toc-the-autoencoder-architecture" class="nav-link" data-scroll-target="#the-autoencoder-architecture">The AutoEncoder Architecture</a>
  <ul class="collapse">
  <li><a href="#the-model-blocks" id="toc-the-model-blocks" class="nav-link" data-scroll-target="#the-model-blocks">The Model Blocks</a></li>
  <li><a href="#vqvae-implementation" id="toc-vqvae-implementation" class="nav-link" data-scroll-target="#vqvae-implementation">VQVAE Implementation</a></li>
  <li><a href="#discriminator" id="toc-discriminator" class="nav-link" data-scroll-target="#discriminator">Discriminator</a></li>
  </ul></li>
  <li><a href="#loading-dataset---celebahq" id="toc-loading-dataset---celebahq" class="nav-link" data-scroll-target="#loading-dataset---celebahq">Loading Dataset - CelebAHQ</a>
  <ul class="collapse">
  <li><a href="#textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w" id="toc-textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w" class="nav-link" data-scroll-target="#textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w"><span class="math inline">\(\texttt{im\_dataset}\)</span> contains <span class="math inline">\(30000\)</span> images of shape <span class="math inline">\((3 \times 256 \times 256) \to (C \times H \times W)\)</span></a></li>
  <li><a href="#textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects" id="toc-textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects" class="nav-link" data-scroll-target="#textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects"><span class="math inline">\(\texttt{celebALoader}\)</span> creates a dataloader of <span class="math inline">\(\texttt{batch\_size} = 8\)</span> i.e.&nbsp;each object in this loader is of shape <span class="math inline">\((8 \times 3 \times 256 \times 256) \to (B \times C \times H \times W)\)</span> and there are <span class="math inline">\(30000 / 8 = 3750\)</span> such objects</a></li>
  </ul></li>
  <li><a href="#training-of-autoencoder" id="toc-training-of-autoencoder" class="nav-link" data-scroll-target="#training-of-autoencoder">Training of AutoEncoder</a></li>
  </ul></li>
  <li><a href="#infer-from-vqvae-to-get-latents" id="toc-infer-from-vqvae-to-get-latents" class="nav-link" data-scroll-target="#infer-from-vqvae-to-get-latents">Infer from VQVAE to get latents</a>
  <ul class="collapse">
  <li><a href="#latent-diffusion-model-architecture" id="toc-latent-diffusion-model-architecture" class="nav-link" data-scroll-target="#latent-diffusion-model-architecture">Latent Diffusion Model Architecture</a>
  <ul class="collapse">
  <li><a href="#unet" id="toc-unet" class="nav-link" data-scroll-target="#unet">UNet</a></li>
  <li><a href="#linear-noise-scheduler" id="toc-linear-noise-scheduler" class="nav-link" data-scroll-target="#linear-noise-scheduler">Linear Noise Scheduler</a></li>
  </ul></li>
  <li><a href="#latent-diffusion-model-training" id="toc-latent-diffusion-model-training" class="nav-link" data-scroll-target="#latent-diffusion-model-training">Latent Diffusion Model Training</a></li>
  <li><a href="#use-latents-for-setting-up-the-data" id="toc-use-latents-for-setting-up-the-data" class="nav-link" data-scroll-target="#use-latents-for-setting-up-the-data">Use Latents for setting up the data</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="final-generation" class="level1">
<h1>Final Generation</h1>
<p><img src="./x0_999ldm.png" style="float: left; width: 50%;"> <img src="./x0_0ldm.png" style="width: 50%;"></p>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.parallel</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.utils <span class="im">as</span> vutils</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms, models</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataset <span class="im">import</span> Dataset</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> glob</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">"retina"</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cuda</code></pre>
</div>
</div>
<section id="latent-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="latent-diffusion-models"><strong>Latent Diffusion Models</strong></h2>
<section id="the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image." class="level3">
<h3 class="anchored" data-anchor-id="the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image.">The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.</h3>
<p align="center">
<img src="./Latent2.png" style="width:60%;border:0;" alt="image">
</p>
</section>
<section id="the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy" class="level3">
<h3 class="anchored" data-anchor-id="the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy">The downside is that although the <span class="math inline">\(L1/L2\)</span> reconstruction loss might be low, the perceptual features in the reconstructed image still might be <strong>fuzzy</strong></h3>
</section>
</section>
<section id="perceptual-retention-textlpips-as-the-metric" class="level2">
<h2 class="anchored" data-anchor-id="perceptual-retention-textlpips-as-the-metric">Perceptual Retention &amp; <span class="math inline">\(\text{LPIPS}\)</span> as the metric</h2>
<p align="center">
<img src="./Percept.png" style="width:80%;border:0;" alt="image">
</p>
<p>CLearly as said that although the <span class="math inline">\(L_1\)</span> or <span class="math inline">\(L_2\)</span> reconstruction loss might be low for the image, yet the perceptual features in the image perceived by a human are still <strong>blurry</strong>. Now in order to understand how a model would perceive the image, there is no better place to dig into pretrained classification <strong>CNNs</strong> <span class="math inline">\(\to\)</span> <strong>VGGs</strong>. The goal is to bring the feature map extracted at each VGG layer to be very similar to the original image’s feature maps at each VGG layer. This distance metric between the feature maps extracted from the layers of a pretrained VGG is called the <strong>perceptual loss</strong>.</p>
<p align="center">
<img src="./LPIPS1.png" style="width:80%;border:0;" alt="image">
</p>
<p>Check out the original implementation too at <a href="https://github.com/richzhang/PerceptualSimilarity">Perceptual Similarity</a>.</p>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># lpips.py implementation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> spatial_average(in_tens, keepdim <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> in_tens.mean([<span class="dv">2</span>, <span class="dv">3</span>], keepdim <span class="op">=</span> keepdim)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> vgg16(nn.Module):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, requires_grad <span class="op">=</span> <span class="va">False</span>, pretrained <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        vgg_pretrained_features <span class="op">=</span> models.vgg16(pretrained <span class="op">=</span> pretrained).features</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice1 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice2 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice3 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice4 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice5 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N_slices <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice1.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>, <span class="dv">9</span>):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice2.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>, <span class="dv">16</span>):</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice3.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">16</span>, <span class="dv">23</span>):</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice4.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">23</span>, <span class="dv">30</span>):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice5.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Freeze the model</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> requires_grad <span class="op">==</span> <span class="va">False</span>:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>                param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice1(X)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        h_relu1_2 <span class="op">=</span> h</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice2(h)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        h_relu2_2 <span class="op">=</span> h</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice3(h)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        h_relu3_3 <span class="op">=</span> h</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice4(h)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        h_relu4_3 <span class="op">=</span> h</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice5(h)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        h_relu5_3 <span class="op">=</span> h</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        vgg_outputs <span class="op">=</span> namedtuple(<span class="st">"VggOutputs"</span>, [<span class="st">"relu1_2"</span>, <span class="st">"relu2_2"</span>, <span class="st">"relu3_3"</span>, <span class="st">"relu4_3"</span>, <span class="st">"relu5_3"</span>])</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScalingLayer(nn.Module):</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mean = [0.485, 0.456, 0.406]</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># std = [0.229, 0.224, 0.225]</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"shift"</span>, torch.tensor([<span class="op">-</span><span class="fl">.030</span>, <span class="op">-</span><span class="fl">.088</span>, <span class="op">-</span><span class="fl">.188</span>])[<span class="va">None</span>, :, <span class="va">None</span>, <span class="va">None</span>])</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"scale"</span>, torch.tensor([<span class="fl">.458</span>, <span class="fl">.448</span>, <span class="fl">.450</span>])[<span class="va">None</span>, :, <span class="va">None</span>, <span class="va">None</span>])</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inp):</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (inp <span class="op">-</span> <span class="va">self</span>.shift) <span class="op">/</span> <span class="va">self</span>.scale</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NetLinLayer(nn.Module):</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chn_in, chn_out <span class="op">=</span> <span class="dv">1</span>, use_dropout <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [nn.Dropout(), ] <span class="cf">if</span> (use_dropout) <span class="cf">else</span> []</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">+=</span> [nn.Conv2d(chn_in, chn_out, kernel_size <span class="op">=</span> <span class="dv">1</span>, stride <span class="op">=</span> <span class="dv">1</span>, padding <span class="op">=</span> <span class="dv">0</span>, bias <span class="op">=</span> <span class="va">False</span>), ]</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x)</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LPIPS(nn.Module):</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, use_dropout <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scaling_layer <span class="op">=</span> ScalingLayer()</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.chns <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">512</span>, <span class="dv">512</span>]</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.chns)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> vgg16(pretrained <span class="op">=</span> <span class="va">True</span>, requires_grad <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin0 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">0</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">1</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">2</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin3 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">3</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin4 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">4</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lins <span class="op">=</span> [<span class="va">self</span>.lin0, <span class="va">self</span>.lin1, <span class="va">self</span>.lin2, <span class="va">self</span>.lin3, <span class="va">self</span>.lin4]</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lins <span class="op">=</span> nn.ModuleList(<span class="va">self</span>.lins)</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net.load_state_dict(torch.load(<span class="st">"./vgg.pth"</span>, map_location <span class="op">=</span> device), strict <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">eval</span>()</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, in0, in1, normalize <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> normalize:</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>            in0 <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> in0 <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>            in1 <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> in1 <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        in0_input, in1_input <span class="op">=</span> <span class="va">self</span>.scaling_layer(in0), <span class="va">self</span>.scaling_layer(in1)</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>        outs0, outs1 <span class="op">=</span> <span class="va">self</span>.net.forward(in0_input), <span class="va">self</span>.net.forward(in1_input)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>        feats0, feats1, diffs <span class="op">=</span> {}, {}, {}</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L):</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>            feats0[kk], feats1[kk] <span class="op">=</span> F.normalize(outs0[kk], dim <span class="op">=</span> <span class="dv">1</span>), F.normalize(outs1[kk])</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>            diffs[kk] <span class="op">=</span> (feats0[kk] <span class="op">-</span> feats1[kk]) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> [spatial_average(<span class="va">self</span>.lins[kk](diffs[kk]), keepdim <span class="op">=</span> <span class="va">True</span>) <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L)]</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        val <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L):</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>            val <span class="op">+=</span> res[l]</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> val</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes" class="level2">
<h2 class="anchored" data-anchor-id="discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes">Discretizing the Latent Space using the <span class="math inline">\(\text{CodeBooks}\)</span> from <span class="math inline">\(\text{VQVAEs}\)</span></h2>
<section id="textvqvae-as-the-textautoencoder" class="level3">
<h3 class="anchored" data-anchor-id="textvqvae-as-the-textautoencoder"><span class="math inline">\(\text{VQVAE}\)</span> as the <span class="math inline">\(\text{AutoEncoder}\)</span></h3>
<p><span class="math inline">\(k\)</span> vectors, each of <span class="math inline">\(d\)</span> dimensions <span class="math inline">\((k \times d)\)</span> help us encode the data.</p>
<p align="center">
<img src="./VQVAE1.png" style="width:70%;border:0;" alt="image">
</p>
The encoder generates a feature map of <span class="math inline">\(H \times W\)</span> features each of <span class="math inline">\(d\)</span> dimension.
<p align="center">
<img src="./VQVAE2.png" style="width:70%;border:0;" alt="image">
</p>
<p>For each of the features, we find the nearest <span class="math inline">\(d\)</span> dimensional encoding to it and replace it with that.</p>
<p><span class="math display">\[ z_q(x) = e_k \]</span> <span class="math display">\[ k = \argmin_j || z_e(x) - e_j ||_2 \]</span></p>
<p align="center">
<img src="./VQVAE3.png" style="width:70%;border:0;" alt="image">
</p>
The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.
<p align="center">
<img src="./VQVAE4.png" style="width:70%;border:0;" alt="image">
</p>
The issue is we have to define the gradients for the <span class="math inline">\(\argmin\)</span> step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input <span class="math inline">\(z_q(x)\)</span> to encoder output <span class="math inline">\(z_e(x)\)</span>
<p align="center">
<img src="./VQVAE5.png" style="width:70%;border:0;" alt="image">
</p>
<p><span class="math display">\[ L = \log p(x | z_q(x)) + || \text{sg}[z_e(x)] - e ||_2^2 + \beta || z_e(x) - \text{sg}[e] ||_2^2 \]</span></p>
</section>
</section>
<section id="the-autoencoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-autoencoder-architecture">The AutoEncoder Architecture</h2>
<p align="center">
<img src="./Latent3.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./Latent4.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc1.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc2.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc3.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc4.png" style="width:80%;border:0;" alt="image">
</p>
<section id="the-model-blocks" class="level3">
<h3 class="anchored" data-anchor-id="the-model-blocks">The Model Blocks</h3>
<p>Adapted from <a href="https://github.com/explainingai-code/StableDiffusion-PyTorch/blob/main/models/blocks.py">ExplainingAI</a></p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Embedding</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_time_embedding(T, d_model):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    factor <span class="op">=</span> <span class="dv">10000</span> <span class="op">**</span> ((torch.arange(start <span class="op">=</span> <span class="dv">0</span>, end <span class="op">=</span> d_model <span class="op">//</span> <span class="dv">2</span>, dtype <span class="op">=</span> torch.float32, device <span class="op">=</span> T.device)) <span class="op">/</span> (d_model <span class="op">//</span> <span class="dv">2</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    t_emb <span class="op">=</span> T[:, <span class="va">None</span>].repeat(<span class="dv">1</span>, d_model <span class="op">//</span> <span class="dv">2</span>) <span class="op">/</span> factor</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    t_emb <span class="op">=</span> torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t_emb</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Blocks</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DownBlock(nn.Module):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Down conv block with attention.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following block</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Resnet block with time embedding</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Attention block</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Downsample</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn<span class="op">=</span><span class="va">False</span>, context_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> down_sample</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> attn</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dim <span class="op">=</span> context_dim</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> cross_attn</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels,</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>                              kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(<span class="va">self</span>.t_emb_dim, out_channels)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels,</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>                              kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> context_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"Context Dimension must be passed for cross attention"</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_proj <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>                [nn.Linear(context_dim, out_channels)</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample_conv <span class="op">=</span> nn.Conv2d(out_channels, out_channels,</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>                                          <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="cf">if</span> <span class="va">self</span>.down_sample <span class="cf">else</span> nn.Identity()</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t_emb<span class="op">=</span><span class="va">None</span>, context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet block of Unet</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i](out)</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i](out)</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i](resnet_input)</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Attention block of Unet</span></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"context cannot be None if cross attention layers are used"</span></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.cross_attention_norms[i](in_attn)</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context.shape[<span class="dv">0</span>] <span class="op">==</span> x.shape[<span class="dv">0</span>] <span class="kw">and</span> context.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.context_dim</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>                context_proj <span class="op">=</span> <span class="va">self</span>.context_proj[i](context)</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.cross_attentions[i](in_attn, context_proj, context_proj)</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Downsample</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.down_sample_conv(out)</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MidBlock(nn.Module):</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a><span class="co">    Mid conv block with attention.</span></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following blocks</span></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Resnet block with time embedding</span></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Attention block</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Resnet block with time embedding</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn<span class="op">=</span><span class="va">None</span>, context_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dim <span class="op">=</span> context_dim</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> cross_attn</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(t_emb_dim, out_channels)</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>            [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>             <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>            [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>             <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> context_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"Context Dimension must be passed for cross attention"</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_proj <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>                [nn.Linear(context_dim, out_channels)</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t_emb<span class="op">=</span><span class="va">None</span>, context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First resnet block</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>        resnet_input <span class="op">=</span> out</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[<span class="dv">0</span>](out)</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[<span class="dv">0</span>](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[<span class="dv">0</span>](out)</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[<span class="dv">0</span>](resnet_input)</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Attention Block</span></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>            batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>            out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>            out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"context cannot be None if cross attention layers are used"</span></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.cross_attention_norms[i](in_attn)</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context.shape[<span class="dv">0</span>] <span class="op">==</span> x.shape[<span class="dv">0</span>] <span class="kw">and</span> context.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.context_dim</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>                context_proj <span class="op">=</span> <span class="va">self</span>.context_proj[i](context)</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.cross_attentions[i](in_attn, context_proj, context_proj)</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet Block</span></span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i <span class="op">+</span> <span class="dv">1</span>](out)</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i <span class="op">+</span> <span class="dv">1</span>](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i <span class="op">+</span> <span class="dv">1</span>](out)</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i <span class="op">+</span> <span class="dv">1</span>](resnet_input)</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UpBlock(nn.Module):</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a><span class="co">    Up conv block with attention.</span></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following blocks</span></span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Upsample</span></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Concatenate Down block output</span></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Resnet block with time embedding</span></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Attention Block</span></span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim,</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>                 up_sample, num_heads, num_layers, attn, norm_channels):</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> up_sample</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> attn</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(t_emb_dim, out_channels)</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>                    nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample_conv <span class="op">=</span> nn.ConvTranspose2d(in_channels, in_channels,</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>                                                 <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">\</span></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.up_sample <span class="cf">else</span> nn.Identity()</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, out_down<span class="op">=</span><span class="va">None</span>, t_emb<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Upsample</span></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up_sample_conv(x)</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat with Downblock output</span></span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> out_down <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, out_down], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet Block</span></span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i](out)</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i](out)</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i](resnet_input)</span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Self Attention</span></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UpBlockUnet(nn.Module):</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a><span class="co">    Up conv block with attention.</span></span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following blocks</span></span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Upsample</span></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Concatenate Down block output</span></span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Resnet block with time embedding</span></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Attention Block</span></span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim, up_sample,</span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a>                 num_heads, num_layers, norm_channels, cross_attn<span class="op">=</span><span class="va">False</span>, context_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> up_sample</span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> cross_attn</span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dim <span class="op">=</span> context_dim</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(t_emb_dim, out_channels)</span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>                nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>                nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> context_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"Context Dimension must be passed for cross attention"</span></span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_proj <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a>                [nn.Linear(context_dim, out_channels)</span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample_conv <span class="op">=</span> nn.ConvTranspose2d(in_channels <span class="op">//</span> <span class="dv">2</span>, in_channels <span class="op">//</span> <span class="dv">2</span>,</span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a>                                                 <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">\</span></span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.up_sample <span class="cf">else</span> nn.Identity()</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, out_down<span class="op">=</span><span class="va">None</span>, t_emb<span class="op">=</span><span class="va">None</span>, context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up_sample_conv(x)</span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> out_down <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, out_down], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet</span></span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i](out)</span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i](out)</span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i](resnet_input)</span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Self Attention</span></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a>            batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a>            out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a>            out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Cross Attention</span></span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"context cannot be None if cross attention layers are used"</span></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.cross_attention_norms[i](in_attn)</span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> <span class="bu">len</span>(context.shape) <span class="op">==</span> <span class="dv">3</span>, <span class="op">\</span></span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Context shape does not match B,_,CONTEXT_DIM"</span></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context.shape[<span class="dv">0</span>] <span class="op">==</span> x.shape[<span class="dv">0</span>] <span class="kw">and</span> context.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.context_dim,<span class="op">\</span></span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Context shape does not match B,_,CONTEXT_DIM"</span></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a>                context_proj <span class="op">=</span> <span class="va">self</span>.context_proj[i](context)</span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.cross_attentions[i](in_attn, context_proj, context_proj)</span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vqvae-implementation" class="level3">
<h3 class="anchored" data-anchor-id="vqvae-implementation">VQVAE Implementation</h3>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VQVAE(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        im_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_channels <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">256</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mid_channels <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">256</span>]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_down_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_mid_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_up_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_channels <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># To disable attention in the DownBlock of Encoder and UpBlock of Decoder</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attns <span class="op">=</span> [<span class="va">False</span>, <span class="va">False</span>, <span class="va">False</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Latent Dimension</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.codebook_size <span class="op">=</span> <span class="dv">8192</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wherever we use downsampling in encoder correspondingly use</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># upsampling in decoder</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> <span class="bu">list</span>(<span class="bu">reversed</span>(<span class="va">self</span>.down_sample))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">##################### Encoder ######################</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_conv_in <span class="op">=</span> nn.Conv2d(im_channels, <span class="va">self</span>.down_channels[<span class="dv">0</span>], kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Downblock + Midblock</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_layers <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.down_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder_layers.append(DownBlock(<span class="va">self</span>.down_channels[i], <span class="va">self</span>.down_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>                                                 t_emb_dim<span class="op">=</span><span class="va">None</span>, down_sample<span class="op">=</span><span class="va">self</span>.down_sample[i],</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>                                                 num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>                                                 num_layers<span class="op">=</span><span class="va">self</span>.num_down_layers,</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                                                 attn<span class="op">=</span><span class="va">self</span>.attns[i],</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                                                 norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_mids <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.mid_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder_mids.append(MidBlock(<span class="va">self</span>.mid_channels[i], <span class="va">self</span>.mid_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>                                              t_emb_dim<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>                                              num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>                                              num_layers<span class="op">=</span><span class="va">self</span>.num_mid_layers,</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>                                              norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_norm_out <span class="op">=</span> nn.GroupNorm(<span class="va">self</span>.norm_channels, <span class="va">self</span>.down_channels[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_conv_out <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.down_channels[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.z_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre Quantization Convolution</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pre_quant_conv <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.z_channels, <span class="va">self</span>.z_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Codebook</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(<span class="va">self</span>.codebook_size, <span class="va">self</span>.z_channels)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">####################################################</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">##################### Decoder ######################</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Post Quantization Convolution</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_quant_conv <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.z_channels, <span class="va">self</span>.z_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_conv_in <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.z_channels, <span class="va">self</span>.mid_channels[<span class="op">-</span><span class="dv">1</span>], kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Midblock + Upblock</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_mids <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(<span class="va">self</span>.mid_channels))):</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_mids.append(MidBlock(<span class="va">self</span>.mid_channels[i], <span class="va">self</span>.mid_channels[i <span class="op">-</span> <span class="dv">1</span>],</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>                                              t_emb_dim<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>                                              num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>                                              num_layers<span class="op">=</span><span class="va">self</span>.num_mid_layers,</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>                                              norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_layers <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(<span class="va">self</span>.down_channels))):</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_layers.append(UpBlock(<span class="va">self</span>.down_channels[i], <span class="va">self</span>.down_channels[i <span class="op">-</span> <span class="dv">1</span>],</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>                                               t_emb_dim<span class="op">=</span><span class="va">None</span>, up_sample<span class="op">=</span><span class="va">self</span>.down_sample[i <span class="op">-</span> <span class="dv">1</span>],</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>                                               num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>                                               num_layers<span class="op">=</span><span class="va">self</span>.num_up_layers,</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>                                               attn<span class="op">=</span><span class="va">self</span>.attns[i<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>                                               norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_norm_out <span class="op">=</span> nn.GroupNorm(<span class="va">self</span>.norm_channels, <span class="va">self</span>.down_channels[<span class="dv">0</span>])</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_conv_out <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.down_channels[<span class="dv">0</span>], im_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> quantize(<span class="va">self</span>, x):</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>        B, C, H, W <span class="op">=</span> x.shape</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B, C, H, W -&gt; B, H, W, C</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B, H, W, C -&gt; B, H*W, C</span></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>, x.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find nearest embedding/codebook vector</span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dist between (B, H*W, C) and (B, K, C) -&gt; (B, H*W, K)</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        dist <span class="op">=</span> torch.cdist(x, <span class="va">self</span>.embedding.weight[<span class="va">None</span>, :].repeat((x.size(<span class="dv">0</span>), <span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (B, H*W)</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>        min_encoding_indices <span class="op">=</span> torch.argmin(dist, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Replace encoder output with nearest codebook</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># quant_out -&gt; B*H*W, C</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>        quant_out <span class="op">=</span> torch.index_select(<span class="va">self</span>.embedding.weight, <span class="dv">0</span>, min_encoding_indices.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x -&gt; B*H*W, C</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape((<span class="op">-</span><span class="dv">1</span>, x.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>        commmitment_loss <span class="op">=</span> torch.mean((quant_out.detach() <span class="op">-</span> x) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>        codebook_loss <span class="op">=</span> torch.mean((quant_out <span class="op">-</span> x.detach()) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>        quantize_losses <span class="op">=</span> {</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>            <span class="st">'codebook_loss'</span>: codebook_loss,</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>            <span class="st">'commitment_loss'</span>: commmitment_loss</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Straight through estimation</span></span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>        quant_out <span class="op">=</span> x <span class="op">+</span> (quant_out <span class="op">-</span> x).detach()</span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># quant_out -&gt; B, C, H, W</span></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>        quant_out <span class="op">=</span> quant_out.reshape((B, H, W, C)).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>        min_encoding_indices <span class="op">=</span> min_encoding_indices.reshape((<span class="op">-</span><span class="dv">1</span>, quant_out.size(<span class="op">-</span><span class="dv">2</span>), quant_out.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quant_out, quantize_losses, min_encoding_indices</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.encoder_conv_in(x)</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, down <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.encoder_layers):</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> down(out)</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mid <span class="kw">in</span> <span class="va">self</span>.encoder_mids:</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> mid(out)</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.encoder_norm_out(out)</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.SiLU()(out)</span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.encoder_conv_out(out)</span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.pre_quant_conv(out)</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a>        out, quant_losses, _ <span class="op">=</span> <span class="va">self</span>.quantize(out)</span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, quant_losses</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> z</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.post_quant_conv(out)</span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decoder_conv_in(out)</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mid <span class="kw">in</span> <span class="va">self</span>.decoder_mids:</span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> mid(out)</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, up <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.decoder_layers):</span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> up(out)</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decoder_norm_out(out)</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.SiLU()(out)</span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decoder_conv_out(out)</span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>        z, quant_losses <span class="op">=</span> <span class="va">self</span>.encode(x)</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decode(z)</span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, z, quant_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="discriminator" class="level3">
<h3 class="anchored" data-anchor-id="discriminator">Discriminator</h3>
<div id="cell-13" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    PatchGAN Discriminator.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Rather than taking IMG_CHANNELSxIMG_HxIMG_W all the way to</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    1 scalar value , we instead predict grid of values.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Where each grid is prediction of how likely</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    the discriminator thinks that the image patch corresponding</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    to the grid cell is real</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, im_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                 conv_channels<span class="op">=</span>[<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>],</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>                 kernels<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>],</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                 strides<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                 paddings<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_channels <span class="op">=</span> im_channels</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        activation <span class="op">=</span> nn.LeakyReLU(<span class="fl">0.2</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        layers_dim <span class="op">=</span> [<span class="va">self</span>.im_channels] <span class="op">+</span> conv_channels <span class="op">+</span> [<span class="dv">1</span>]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            nn.Sequential(</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(layers_dim[i], layers_dim[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>                          kernel_size<span class="op">=</span>kernels[i],</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>                          stride<span class="op">=</span>strides[i],</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>                          padding<span class="op">=</span>paddings[i],</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>                          bias<span class="op">=</span><span class="va">False</span> <span class="cf">if</span> i <span class="op">!=</span><span class="dv">0</span> <span class="cf">else</span> <span class="va">True</span>),</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>                nn.BatchNorm2d(layers_dim[i <span class="op">+</span> <span class="dv">1</span>]) <span class="cf">if</span> i <span class="op">!=</span> <span class="bu">len</span>(layers_dim) <span class="op">-</span> <span class="dv">2</span> <span class="kw">and</span> i <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> nn.Identity(),</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>                activation <span class="cf">if</span> i <span class="op">!=</span> <span class="bu">len</span>(layers_dim) <span class="op">-</span> <span class="dv">2</span> <span class="cf">else</span> nn.Identity()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layers_dim) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> layer(out)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="loading-dataset---celebahq" class="level2">
<h2 class="anchored" data-anchor-id="loading-dataset---celebahq">Loading Dataset - CelebAHQ</h2>
<div id="cell-15" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_latents(latent_path):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple utility to save latents to speed up ldm training</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param latent_path:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    latent_maps <span class="op">=</span> {}</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fname <span class="kw">in</span> glob.glob(os.path.join(latent_path, <span class="st">'*.pkl'</span>)):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> pickle.load(<span class="bu">open</span>(fname, <span class="st">'rb'</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> s.items():</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            latent_maps[k] <span class="op">=</span> v[<span class="dv">0</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> latent_maps</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CelebDataset(Dataset):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Celeb dataset will by default centre crop and resize the images.</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    This can be replaced by any other dataset. As long as all the images</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">    are under one directory.</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, split, im_path, im_size<span class="op">=</span><span class="dv">256</span>, im_channels<span class="op">=</span><span class="dv">3</span>, im_ext<span class="op">=</span><span class="st">'jpg'</span>,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                 use_latents<span class="op">=</span><span class="va">False</span>, latent_path<span class="op">=</span><span class="va">None</span>, condition_config<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.split <span class="op">=</span> split</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_size <span class="op">=</span> im_size</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_channels <span class="op">=</span> im_channels</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_ext <span class="op">=</span> im_ext</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_path <span class="op">=</span> im_path</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latent_maps <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_latents <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.condition_types <span class="op">=</span> [] <span class="cf">if</span> condition_config <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> condition_config[<span class="st">'condition_types'</span>]</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idx_to_cls_map <span class="op">=</span> {}</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_to_idx_map <span class="op">=</span>{}</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mask_channels <span class="op">=</span> condition_config[<span class="st">'image_condition_config'</span>][<span class="st">'image_condition_input_channels'</span>]</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mask_h <span class="op">=</span> condition_config[<span class="st">'image_condition_config'</span>][<span class="st">'image_condition_h'</span>]</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mask_w <span class="op">=</span> condition_config[<span class="st">'image_condition_config'</span>][<span class="st">'image_condition_w'</span>]</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.images, <span class="va">self</span>.texts, <span class="va">self</span>.masks <span class="op">=</span> <span class="va">self</span>.load_images(im_path)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Whether to load images or to load latents</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_latents <span class="kw">and</span> latent_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>            latent_maps <span class="op">=</span> load_latents(latent_path)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(latent_maps) <span class="op">==</span> <span class="bu">len</span>(<span class="va">self</span>.images):</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.use_latents <span class="op">=</span> <span class="va">True</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.latent_maps <span class="op">=</span> latent_maps</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> latents'</span>.<span class="bu">format</span>(<span class="bu">len</span>(<span class="va">self</span>.latent_maps)))</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Latents not found'</span>)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_images(<span class="va">self</span>, im_path):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Gets all images from the path specified</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="co">        and stacks them all up</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> os.path.exists(im_path), <span class="st">"images path </span><span class="sc">{}</span><span class="st"> does not exist"</span>.<span class="bu">format</span>(im_path)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> []</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        fnames <span class="op">=</span> glob.glob(os.path.join(im_path, <span class="st">'CelebA-HQ-img/*.</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'png'</span>)))</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        fnames <span class="op">+=</span> glob.glob(os.path.join(im_path, <span class="st">'CelebA-HQ-img/*.</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'jpg'</span>)))</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        fnames <span class="op">+=</span> glob.glob(os.path.join(im_path, <span class="st">'CelebA-HQ-img/*.</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'jpeg'</span>)))</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        texts <span class="op">=</span> []</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        masks <span class="op">=</span> []</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>            label_list <span class="op">=</span> [<span class="st">'skin'</span>, <span class="st">'nose'</span>, <span class="st">'eye_g'</span>, <span class="st">'l_eye'</span>, <span class="st">'r_eye'</span>, <span class="st">'l_brow'</span>, <span class="st">'r_brow'</span>, <span class="st">'l_ear'</span>, <span class="st">'r_ear'</span>, <span class="st">'mouth'</span>,</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>                          <span class="st">'u_lip'</span>, <span class="st">'l_lip'</span>, <span class="st">'hair'</span>, <span class="st">'hat'</span>, <span class="st">'ear_r'</span>, <span class="st">'neck_l'</span>, <span class="st">'neck'</span>, <span class="st">'cloth'</span>]</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.idx_to_cls_map <span class="op">=</span> {idx: label_list[idx] <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(label_list))}</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cls_to_idx_map <span class="op">=</span> {label_list[idx]: idx <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(label_list))}</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> fname <span class="kw">in</span> tqdm(fnames):</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>            ims.append(fname)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>                im_name <span class="op">=</span> os.path.split(fname)[<span class="dv">1</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>]</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>                captions_im <span class="op">=</span> []</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> <span class="bu">open</span>(os.path.join(im_path, <span class="st">'celeba-caption/</span><span class="sc">{}</span><span class="st">.txt'</span>.<span class="bu">format</span>(im_name))) <span class="im">as</span> f:</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> line <span class="kw">in</span> f.readlines():</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>                        captions_im.append(line.strip())</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>                texts.append(captions_im)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>                im_name <span class="op">=</span> <span class="bu">int</span>(os.path.split(fname)[<span class="dv">1</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>])</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>                masks.append(os.path.join(im_path, <span class="st">'CelebAMask-HQ-mask'</span>, <span class="st">'</span><span class="sc">{}</span><span class="st">.png'</span>.<span class="bu">format</span>(im_name)))</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="bu">len</span>(texts) <span class="op">==</span> <span class="bu">len</span>(ims), <span class="st">"Condition Type Text but could not find captions for all images"</span></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="bu">len</span>(masks) <span class="op">==</span> <span class="bu">len</span>(ims), <span class="st">"Condition Type Image but could not find masks for all images"</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> images'</span>.<span class="bu">format</span>(<span class="bu">len</span>(ims)))</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> masks'</span>.<span class="bu">format</span>(<span class="bu">len</span>(masks)))</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> captions'</span>.<span class="bu">format</span>(<span class="bu">len</span>(texts)))</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ims, texts, masks</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_mask(<span class="va">self</span>, index):</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="co">        Method to get the mask of WxH</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="co">        for given index and convert it into</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a><span class="co">        Classes x W x H mask image</span></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="co">        :param index:</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="co">        :return:</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>        mask_im <span class="op">=</span> Image.<span class="bu">open</span>(<span class="va">self</span>.masks[index])</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>        mask_im <span class="op">=</span> np.array(mask_im)</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>        im_base <span class="op">=</span> np.zeros((<span class="va">self</span>.mask_h, <span class="va">self</span>.mask_w, <span class="va">self</span>.mask_channels))</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> orig_idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.idx_to_cls_map)):</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>            im_base[mask_im <span class="op">==</span> (orig_idx<span class="op">+</span><span class="dv">1</span>), orig_idx] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> torch.from_numpy(im_base).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mask</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.images)</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>        <span class="co">######## Set Conditioning Info ########</span></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>        cond_inputs <span class="op">=</span> {}</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>            cond_inputs[<span class="st">'text'</span>] <span class="op">=</span> random.sample(<span class="va">self</span>.texts[index], k<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> <span class="va">self</span>.get_mask(index)</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>            cond_inputs[<span class="st">'image'</span>] <span class="op">=</span> mask</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">#######################################</span></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_latents:</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>            latent <span class="op">=</span> <span class="va">self</span>.latent_maps[<span class="va">self</span>.images[index]]</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.condition_types) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> latent</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> latent, cond_inputs</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>            im <span class="op">=</span> Image.<span class="bu">open</span>(<span class="va">self</span>.images[index])</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>            im_tensor <span class="op">=</span> transforms.Compose([</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>                transforms.Resize(<span class="va">self</span>.im_size),</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>                transforms.CenterCrop(<span class="va">self</span>.im_size),</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>                transforms.ToTensor(),</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>            ])(im)</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>            im.close()</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert input to -1 to 1 range.</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a>            im_tensor <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> im_tensor) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.condition_types) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> im_tensor</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> im_tensor, cond_inputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Parameters</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>batch_size_autoenc <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>num_epochs_autoenc <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>batch_size_ldm <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>num_epochs_ldm <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Parameters</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>nc <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">256</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w" class="level3">
<h3 class="anchored" data-anchor-id="textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w"><span class="math inline">\(\texttt{im\_dataset}\)</span> contains <span class="math inline">\(30000\)</span> images of shape <span class="math inline">\((3 \times 256 \times 256) \to (C \times H \times W)\)</span></h3>
</section>
<section id="textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects" class="level3">
<h3 class="anchored" data-anchor-id="textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects"><span class="math inline">\(\texttt{celebALoader}\)</span> creates a dataloader of <span class="math inline">\(\texttt{batch\_size} = 8\)</span> i.e.&nbsp;each object in this loader is of shape <span class="math inline">\((8 \times 3 \times 256 \times 256) \to (B \times C \times H \times W)\)</span> and there are <span class="math inline">\(30000 / 8 = 3750\)</span> such objects</h3>
<div id="cell-18" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"../CelebAMask-HQ"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>im_dataset <span class="op">=</span> CelebDataset(split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                                im_path<span class="op">=</span>path,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                                im_size<span class="op">=</span>image_size,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                                im_channels<span class="op">=</span>nc)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>celebALoader <span class="op">=</span> DataLoader(im_dataset,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                             batch_size<span class="op">=</span>batch_size_autoenc,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                             shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 30000/30000 [00:00&lt;00:00, 3045603.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 30000 images
Found 0 masks
Found 0 captions</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.random.choice(<span class="dv">30000</span>, <span class="dv">16</span>, replace <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> torch.stack([im_dataset[i] <span class="cf">for</span> i <span class="kw">in</span> indices], dim <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> vutils.make_grid(images, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.transpose(grid, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="training-of-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="training-of-autoencoder">Training of AutoEncoder</h2>
<div id="cell-21" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lpips_model <span class="op">=</span> LPIPS().<span class="bu">eval</span>().to(device)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> Discriminator().to(device)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>recon_criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>disc_criterion <span class="op">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>optimizer_d <span class="op">=</span> torch.optim.Adam(discriminator.parameters(), lr <span class="op">=</span> <span class="fl">1e-5</span>, betas <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>optimizer_g <span class="op">=</span> torch.optim.Adam(model.parameters(), lr <span class="op">=</span> <span class="fl">1e-5</span>, betas <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>disc_step_start <span class="op">=</span> <span class="dv">15000</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>step_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>acc_steps <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>real_images <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>reconstructed_images <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>quantized_images <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch_idx <span class="kw">in</span> <span class="bu">range</span>(num_epochs_autoenc):</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    recon_losses <span class="op">=</span> []</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    codebook_losses <span class="op">=</span> []</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    perceptual_losses <span class="op">=</span> []</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    disc_losses <span class="op">=</span> []</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    gen_losses <span class="op">=</span> []</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    optimizer_d.zero_grad()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    optimizer_g.zero_grad()</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx, im <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(celebALoader)):</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        step_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> im.<span class="bu">float</span>().to(device)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generator</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        model_output <span class="op">=</span> model(im)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        output, z, quantize_losses <span class="op">=</span> model_output</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        recon_loss <span class="op">=</span> recon_criterion(output, im)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        recon_losses.append(recon_loss.item())</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        recon_loss <span class="op">=</span> recon_loss <span class="op">/</span> acc_steps</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        g_loss <span class="op">=</span> recon_loss <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> quantize_losses[<span class="st">"codebook_loss"</span>] <span class="op">/</span> acc_steps) <span class="op">+</span> (<span class="fl">0.2</span> <span class="op">*</span> quantize_losses[<span class="st">"commitment_loss"</span>] <span class="op">/</span> acc_steps)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        codebook_losses.append(quantize_losses[<span class="st">"codebook_loss"</span>].item())</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adversarial Loss</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step_count <span class="op">&gt;</span> disc_step_start:</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>            disc_fake_pred <span class="op">=</span> discriminator(output)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>            disc_fake_loss <span class="op">=</span> disc_criterion(disc_fake_pred, torch.ones(disc_fake_pred.shape, device <span class="op">=</span> disc_fake_pred.device))</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>            gen_losses.append(<span class="fl">0.5</span> <span class="op">*</span> disc_fake_loss.item())</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>            g_loss <span class="op">+=</span> <span class="fl">0.5</span> <span class="op">*</span> disc_fake_loss.item()</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        lpips_loss <span class="op">=</span> torch.mean(lpips_model(output, im)) <span class="op">/</span> acc_steps</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>        perceptual_losses.append(lpips_loss.item())</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        g_loss <span class="op">+=</span> lpips_loss <span class="op">/</span> acc_steps</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>        losses.append(g_loss.item())</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        g_loss.backward()</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        optimizer_g.step()</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discriminator</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step_count <span class="op">&gt;</span> disc_step_start:</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>            fake <span class="op">=</span> output</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>            disc_fake_pred <span class="op">=</span> discriminator(fake.detach())</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>            disc_real_pred <span class="op">=</span> discriminator(im)</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>            disc_fake_loss <span class="op">=</span> disc_criterion(disc_fake_pred, torch.zeros(disc_fake_pred.shape, device <span class="op">=</span> disc_fake_pred.device))</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>            disc_real_loss <span class="op">=</span> disc_criterion(disc_real_pred, torch.ones(disc_real_pred.shape, device <span class="op">=</span> disc_real_pred.device))</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>            disc_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (disc_fake_loss <span class="op">+</span> disc_real_loss) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>            disc_losses.append(disc_loss.item())</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>            disc_loss <span class="op">=</span> disc_loss <span class="op">/</span> acc_steps</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>            disc_loss.backward()</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> step_count <span class="op">%</span> acc_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>                optimizer_d.step()</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>                optimizer_d.zero_grad()</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step_count <span class="op">%</span> acc_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>            optimizer_g.step()</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>            optimizer_g.zero_grad()</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> batch_idx <span class="op">%</span> <span class="dv">2</span></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        real_images[idx] <span class="op">=</span> im</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>        reconstructed_images[idx] <span class="op">=</span> output</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>        quantized_images[idx] <span class="op">=</span> z</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>    optimizer_d.step()</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>    optimizer_d.zero_grad()</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>    optimizer_g.step()</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>    optimizer_g.zero_grad()</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(disc_losses) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Finished epoch: </span><span class="sc">{}</span><span class="st"> | Recon Loss : </span><span class="sc">{:.4f}</span><span class="st"> | Perceptual Loss : </span><span class="sc">{:.4f}</span><span class="st"> | '</span></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Codebook : </span><span class="sc">{:.4f}</span><span class="st"> | G Loss : </span><span class="sc">{:.4f}</span><span class="st"> | D Loss </span><span class="sc">{:.4f}</span><span class="st">'</span>.</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>            <span class="bu">format</span>(epoch_idx <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>                    np.mean(recon_losses),</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>                    np.mean(perceptual_losses),</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>                    np.mean(codebook_losses),</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>                    np.mean(gen_losses),</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>                    np.mean(disc_losses)))</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Finished epoch: </span><span class="sc">{}</span><span class="st"> | Recon Loss : </span><span class="sc">{:.4f}</span><span class="st"> | Perceptual Loss : </span><span class="sc">{:.4f}</span><span class="st"> | Codebook : </span><span class="sc">{:.4f}</span><span class="st">'</span>.</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>                <span class="bu">format</span>(epoch_idx <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>                        np.mean(recon_losses),</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a>                        np.mean(perceptual_losses),</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a>                        np.mean(codebook_losses)))</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), <span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>)</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>    torch.save(discriminator.state_dict(), <span class="st">"../vqvaeCeleb/vqvae_discriminator.pth"</span>)</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>        real_images1 <span class="op">=</span> torch.cat(real_images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>        reconstructed_images1 <span class="op">=</span> torch.cat(reconstructed_images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>        quantized_images1 <span class="op">=</span> torch.cat(quantized_images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each of these lists have 2 tensors of shape (8, 3, 256, 256)</span></span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># So stacking them gives (16, 3, 256, 256)</span></span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and we plot it in a grid of 4x4</span></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>        grid_real <span class="op">=</span> vutils.make_grid(real_images1, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a>        grid_reconstructed <span class="op">=</span> vutils.make_grid(reconstructed_images1, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>        grid_quantized <span class="op">=</span> vutils.make_grid(quantized_images1, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Real Images"</span>)</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>        plt.imshow(np.transpose(grid_real.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Reconstructed Images"</span>)</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>        plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Quantized Images"</span>)</span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>        plt.imshow(np.transpose(grid_quantized.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a>        plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/cvig/miniconda3/envs/PyTorchG/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/cvig/miniconda3/envs/PyTorchG/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
100%|██████████| 3750/3750 [16:43&lt;00:00,  3.74it/s]
100%|██████████| 3750/3750 [16:46&lt;00:00,  3.72it/s]
100%|██████████| 3750/3750 [16:39&lt;00:00,  3.75it/s]
100%|██████████| 3750/3750 [16:41&lt;00:00,  3.74it/s]
100%|██████████| 3750/3750 [23:59&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:01&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:04&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:01&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:04&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:01&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:00&lt;00:00,  2.60it/s]
100%|██████████| 3750/3750 [24:03&lt;00:00,  2.60it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Finished epoch: 1 | Recon Loss : 0.0444 | Perceptual Loss : -0.0079 | Codebook : 0.0024
Finished epoch: 2 | Recon Loss : 0.0274 | Perceptual Loss : -0.0099 | Codebook : 0.0012
Finished epoch: 3 | Recon Loss : 0.0187 | Perceptual Loss : -0.0102 | Codebook : 0.0016
Finished epoch: 4 | Recon Loss : 0.0153 | Perceptual Loss : -0.0104 | Codebook : 0.0013
Finished epoch: 5 | Recon Loss : 0.0135 | Perceptual Loss : -0.0107 | Codebook : 0.0011 | G Loss : 0.9127 | D Loss 0.1442
Finished epoch: 6 | Recon Loss : 0.0126 | Perceptual Loss : -0.0110 | Codebook : 0.0009 | G Loss : 1.9146 | D Loss 0.0166
Finished epoch: 7 | Recon Loss : 0.0119 | Perceptual Loss : -0.0113 | Codebook : 0.0008 | G Loss : 2.4890 | D Loss 0.0052
Finished epoch: 8 | Recon Loss : 0.0117 | Perceptual Loss : -0.0118 | Codebook : 0.0008 | G Loss : 2.8359 | D Loss 0.0040
Finished epoch: 9 | Recon Loss : 0.0111 | Perceptual Loss : -0.0124 | Codebook : 0.0008 | G Loss : 3.0382 | D Loss 0.0018
Finished epoch: 10 | Recon Loss : 0.0108 | Perceptual Loss : -0.0131 | Codebook : 0.0007 | G Loss : 3.1862 | D Loss 0.0014
Finished epoch: 11 | Recon Loss : 0.0105 | Perceptual Loss : -0.0139 | Codebook : 0.0007 | G Loss : 3.3179 | D Loss 0.0012
Finished epoch: 12 | Recon Loss : 0.0102 | Perceptual Loss : -0.0146 | Codebook : 0.0007 | G Loss : 3.5116 | D Loss 0.0008
Finished epoch: 13 | Recon Loss : 0.0100 | Perceptual Loss : -0.0150 | Codebook : 0.0007 | G Loss : 3.7793 | D Loss 0.0005
Finished epoch: 14 | Recon Loss : 0.0098 | Perceptual Loss : -0.0153 | Codebook : 0.0007 | G Loss : 3.9513 | D Loss 0.0041
Finished epoch: 15 | Recon Loss : 0.0096 | Perceptual Loss : -0.0156 | Codebook : 0.0006 | G Loss : 3.6160 | D Loss 0.0009
Finished epoch: 16 | Recon Loss : 0.0095 | Perceptual Loss : -0.0158 | Codebook : 0.0006 | G Loss : 3.9615 | D Loss 0.0004
Finished epoch: 17 | Recon Loss : 0.0094 | Perceptual Loss : -0.0160 | Codebook : 0.0006 | G Loss : 4.0736 | D Loss 0.0003
Finished epoch: 18 | Recon Loss : 0.0093 | Perceptual Loss : -0.0161 | Codebook : 0.0006 | G Loss : 4.1628 | D Loss 0.0003
Finished epoch: 19 | Recon Loss : 0.0091 | Perceptual Loss : -0.0162 | Codebook : 0.0006 | G Loss : 4.2656 | D Loss 0.0002
Finished epoch: 20 | Recon Loss : 0.0090 | Perceptual Loss : -0.0164 | Codebook : 0.0006 | G Loss : 4.3928 | D Loss 0.0002</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-16.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-17.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-18.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-19.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-20.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-21.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-22.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="infer-from-vqvae-to-get-latents" class="level1">
<h1>Infer from VQVAE to get latents</h1>
<div id="cell-23" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>im_dataset <span class="op">=</span> CelebDataset(split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                                im_path<span class="op">=</span>path,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                                im_size<span class="op">=</span>image_size,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                                im_channels<span class="op">=</span>nc)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(im_dataset,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                            batch_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                            shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>num_images <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>ngrid <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(im_dataset) <span class="op">-</span> <span class="dv">1</span>, (num_images,))</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>ims <span class="op">=</span> torch.cat([im_dataset[idx][<span class="va">None</span>, :] <span class="cf">for</span> idx <span class="kw">in</span> idxs]).<span class="bu">float</span>()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>ims <span class="op">=</span> ims.to(device)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(<span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>, map_location <span class="op">=</span> device))</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    encoded_output, _ <span class="op">=</span> model.encode(ims)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    decoded_output <span class="op">=</span> model.decode(encoded_output)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    encoded_output <span class="op">=</span> torch.clamp(encoded_output, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    encoded_output <span class="op">=</span> (encoded_output <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    decoded_output <span class="op">=</span> torch.clamp(decoded_output, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    decoded_output <span class="op">=</span> (decoded_output <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    ims <span class="op">=</span> (ims <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    encoder_grid <span class="op">=</span> vutils.make_grid(encoded_output.cpu(), nrow<span class="op">=</span>ngrid)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>    decoder_grid <span class="op">=</span> vutils.make_grid(decoded_output.cpu(), nrow<span class="op">=</span>ngrid)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    input_grid <span class="op">=</span> vutils.make_grid(ims.cpu(), nrow<span class="op">=</span>ngrid)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    encoder_grid <span class="op">=</span> transforms.ToPILImage()(encoder_grid)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    decoder_grid <span class="op">=</span> transforms.ToPILImage()(decoder_grid)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    input_grid <span class="op">=</span> transforms.ToPILImage()(input_grid)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    input_grid.save(<span class="st">'../CelebAHQ/input_samples.png'</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    encoder_grid.save(<span class="st">'../CelebAHQ/encoded_samples.png'</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    decoder_grid.save(<span class="st">"../CelebAHQ/reconstructed_samples.png"</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    latent_path <span class="op">=</span> <span class="st">"../vqvaelatents"</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    latent_fnames <span class="op">=</span> glob.glob(os.path.join(<span class="st">"../vqvaelatents"</span>, <span class="st">'*.pkl'</span>))</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(latent_fnames) <span class="op">==</span> <span class="dv">0</span>, <span class="st">'Latents already present. Delete all latent files and re-run'</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(latent_path):</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        os.mkdir(latent_path)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    fname_latent_map <span class="op">=</span> {}</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    part_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, im <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(data_loader)):</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        encoded_output, _ <span class="op">=</span> model.encode(im.<span class="bu">float</span>().to(device))</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        fname_latent_map[im_dataset.images[idx]] <span class="op">=</span> encoded_output.cpu()</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save latents every 1000 images</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (count<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>            pickle.dump(fname_latent_map, <span class="bu">open</span>(os.path.join(latent_path,</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>                                                            <span class="st">'</span><span class="sc">{}</span><span class="st">.pkl'</span>.<span class="bu">format</span>(part_count)), <span class="st">'wb'</span>))</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>            part_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>            fname_latent_map <span class="op">=</span> {}</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(fname_latent_map) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        pickle.dump(fname_latent_map, <span class="bu">open</span>(os.path.join(latent_path,</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>                                            <span class="st">'</span><span class="sc">{}</span><span class="st">.pkl'</span>.<span class="bu">format</span>(part_count)), <span class="st">'wb'</span>))</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Done saving latents"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 30000/30000 [00:00&lt;00:00, 6612145.03it/s]
100%|██████████| 30000/30000 [10:07&lt;00:00, 49.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 30000 images
Found 0 masks
Found 0 captions
Done saving latents</code></pre>
</div>
</div>
<section id="latent-diffusion-model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="latent-diffusion-model-architecture">Latent Diffusion Model Architecture</h2>
<section id="unet" class="level3">
<h3 class="anchored" data-anchor-id="unet">UNet</h3>
<div id="cell-25" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Unet(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, im_channels <span class="op">=</span> <span class="dv">3</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_channels <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">384</span>, <span class="dv">512</span>, <span class="dv">768</span>]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mid_channels <span class="op">=</span> [<span class="dv">768</span>, <span class="dv">512</span>]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>]</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_down_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_mid_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_up_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attns <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>]</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_channels <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_out_channels <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial projection from sinusoidal time embedding</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_proj <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.t_emb_dim, <span class="va">self</span>.t_emb_dim),</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            nn.SiLU(),</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.t_emb_dim, <span class="va">self</span>.t_emb_dim)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> <span class="bu">list</span>(<span class="bu">reversed</span>(<span class="va">self</span>.down_sample))</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_in <span class="op">=</span> nn.Conv2d(im_channels, <span class="va">self</span>.down_channels[<span class="dv">0</span>], kernel_size <span class="op">=</span> <span class="dv">3</span>, padding <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downs <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.down_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.downs.append(DownBlock(<span class="va">self</span>.down_channels[i], <span class="va">self</span>.down_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>                                       t_emb_dim <span class="op">=</span> <span class="va">self</span>.t_emb_dim, down_sample <span class="op">=</span> <span class="va">self</span>.down_sample[i],</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>                                       num_heads <span class="op">=</span> <span class="va">self</span>.num_heads, num_layers <span class="op">=</span> <span class="va">self</span>.num_down_layers,</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>                                       attn <span class="op">=</span> <span class="va">self</span>.attns[i], norm_channels <span class="op">=</span> <span class="va">self</span>.norm_channels))</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mids <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.mid_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mids.append(MidBlock(<span class="va">self</span>.mid_channels[i], <span class="va">self</span>.mid_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>                                    t_emb_dim <span class="op">=</span> <span class="va">self</span>.t_emb_dim, num_heads <span class="op">=</span> <span class="va">self</span>.num_heads,</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>                                    num_layers <span class="op">=</span> <span class="va">self</span>.num_mid_layers, norm_channels <span class="op">=</span> <span class="va">self</span>.norm_channels))</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ups <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.down_channels) <span class="op">-</span> <span class="dv">1</span>)):</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.ups.append(UpBlockUnet(<span class="va">self</span>.down_channels[i] <span class="op">*</span> <span class="dv">2</span>, <span class="va">self</span>.down_channels[i <span class="op">-</span> <span class="dv">1</span>] <span class="cf">if</span> i <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">self</span>.conv_out_channels,</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>                                    <span class="va">self</span>.t_emb_dim, up_sample <span class="op">=</span> <span class="va">self</span>.down_sample[i],</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>                                        num_heads <span class="op">=</span> <span class="va">self</span>.num_heads,</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>                                        num_layers <span class="op">=</span> <span class="va">self</span>.num_up_layers,</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>                                        norm_channels <span class="op">=</span> <span class="va">self</span>.norm_channels))</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_out <span class="op">=</span> nn.GroupNorm(<span class="va">self</span>.norm_channels, <span class="va">self</span>.conv_out_channels)</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_out <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.conv_out_channels, im_channels, kernel_size <span class="op">=</span> <span class="dv">3</span>, padding <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t):</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shapes assuming downblocks are [C1, C2, C3, C4]</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shapes assuming midblocks are [C4, C4, C3]</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shapes assuming downsamples are [True, True, False]</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B x C x H x W</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_in(x)</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B x C1 x H x W</span></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># t_emb -&gt; B x t_emb_dim</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        t_emb <span class="op">=</span> get_time_embedding(torch.as_tensor(t).<span class="bu">long</span>(), <span class="va">self</span>.t_emb_dim)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>        t_emb <span class="op">=</span> <span class="va">self</span>.t_proj(t_emb)</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>        down_outs <span class="op">=</span> []</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, down <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.downs):</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>            down_outs.append(out)</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> down(out, t_emb)</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]</span></span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out B x C4 x H/4 x W/4</span></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mid <span class="kw">in</span> <span class="va">self</span>.mids:</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> mid(out, t_emb)</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out B x C3 x H/4 x W/4</span></span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> up <span class="kw">in</span> <span class="va">self</span>.ups:</span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>            down_out <span class="op">=</span> down_outs.pop()</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> up(out, down_out, t_emb)</span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>            <span class="co"># out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]</span></span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.norm_out(out)</span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.SiLU()(out)</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_out(out)</span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out B x C x H x W</span></span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="linear-noise-scheduler" class="level3">
<h3 class="anchored" data-anchor-id="linear-noise-scheduler">Linear Noise Scheduler</h3>
<div id="cell-27" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearNoiseScheduler:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Class for the linear noise scheduler that is used in DDPM.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_timesteps, beta_start, beta_end):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_timesteps <span class="op">=</span> num_timesteps</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta_start <span class="op">=</span> beta_start</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta_end <span class="op">=</span> beta_end</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mimicking how compvis repo creates schedule</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> (</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>                torch.linspace(beta_start <span class="op">**</span> <span class="fl">0.5</span>, beta_end <span class="op">**</span> <span class="fl">0.5</span>, num_timesteps) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> <span class="va">self</span>.betas</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha_cum_prod <span class="op">=</span> torch.cumprod(<span class="va">self</span>.alphas, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqrt_alpha_cum_prod <span class="op">=</span> torch.sqrt(<span class="va">self</span>.alpha_cum_prod)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqrt_one_minus_alpha_cum_prod <span class="op">=</span> torch.sqrt(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha_cum_prod)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_noise(<span class="va">self</span>, original, noise, t):</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward method for diffusion</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">        :param original: Image on which noise is to be applied</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">        :param noise: Random Noise Tensor (from normal dist)</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="co">        :param t: timestep of the forward process of shape -&gt; (B,)</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co">        :return:</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> original.shape</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> original_shape[<span class="dv">0</span>]</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        sqrt_alpha_cum_prod <span class="op">=</span> <span class="va">self</span>.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        sqrt_one_minus_alpha_cum_prod <span class="op">=</span> <span class="va">self</span>.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(original_shape) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>            sqrt_alpha_cum_prod <span class="op">=</span> sqrt_alpha_cum_prod.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(original_shape) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>            sqrt_one_minus_alpha_cum_prod <span class="op">=</span> sqrt_one_minus_alpha_cum_prod.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply and Return Forward process equation</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (sqrt_alpha_cum_prod.to(original.device) <span class="op">*</span> original</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>                <span class="op">+</span> sqrt_one_minus_alpha_cum_prod.to(original.device) <span class="op">*</span> noise)</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_prev_timestep(<span class="va">self</span>, xt, noise_pred, t):</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a><span class="co">            Use the noise prediction by model to get</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="co">            xt-1 using xt and the nosie predicted</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="co">        :param xt: current timestep sample</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="co">        :param noise_pred: model noise prediction</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="co">        :param t: current timestep we are at</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="co">        :return:</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> ((xt <span class="op">-</span> (<span class="va">self</span>.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] <span class="op">*</span> noise_pred)) <span class="op">/</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>              torch.sqrt(<span class="va">self</span>.alpha_cum_prod.to(xt.device)[t]))</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> torch.clamp(x0, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> xt <span class="op">-</span> ((<span class="va">self</span>.betas.to(xt.device)[t]) <span class="op">*</span> noise_pred) <span class="op">/</span> (<span class="va">self</span>.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> mean <span class="op">/</span> torch.sqrt(<span class="va">self</span>.alphas.to(xt.device)[t])</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> t <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> mean, x0</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>            variance <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha_cum_prod.to(xt.device)[t <span class="op">-</span> <span class="dv">1</span>]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> <span class="va">self</span>.alpha_cum_prod.to(xt.device)[t])</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>            variance <span class="op">=</span> variance <span class="op">*</span> <span class="va">self</span>.betas.to(xt.device)[t]</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> variance <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.randn(xt.shape).to(xt.device)</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> mean <span class="op">+</span> sigma <span class="op">*</span> z, x0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="latent-diffusion-model-training" class="level2">
<h2 class="anchored" data-anchor-id="latent-diffusion-model-training">Latent Diffusion Model Training</h2>
<div id="cell-29" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Parameters</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Diffusion Parameters</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>beta_start <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>beta_end <span class="op">=</span> <span class="fl">2e-2</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Parameters</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>nc <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>z_channels <span class="op">=</span> <span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="use-latents-for-setting-up-the-data" class="level2">
<h2 class="anchored" data-anchor-id="use-latents-for-setting-up-the-data">Use Latents for setting up the data</h2>
<div id="cell-31" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>im_dataset <span class="op">=</span> CelebDataset(split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                                im_path<span class="op">=</span>path,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                                im_size<span class="op">=</span>image_size,</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                                im_channels<span class="op">=</span>nc,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                                use_latents<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                                latent_path<span class="op">=</span><span class="st">"../vqvaelatents"</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                                )</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>celebALoader <span class="op">=</span> DataLoader(im_dataset,</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                            batch_size<span class="op">=</span>batch_size_ldm,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                            shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 30000/30000 [00:00&lt;00:00, 1944027.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 30000 images
Found 0 masks
Found 0 captions
Found 30000 latents</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> LinearNoiseScheduler(num_timesteps <span class="op">=</span> T, beta_start <span class="op">=</span> beta_start, beta_end <span class="op">=</span> beta_end)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Unet(im_channels <span class="op">=</span> z_channels).to(device)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> im_dataset.use_latents:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    vae <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    vae.<span class="bu">eval</span>()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    vae.load_state_dict(torch.load(<span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>, map_location <span class="op">=</span> device))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> im_dataset.use_latents:</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> vae.parameters():</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr <span class="op">=</span> lr)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch_idx <span class="kw">in</span> <span class="bu">range</span>(num_epochs_ldm):</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> im <span class="kw">in</span> tqdm(celebALoader):</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> im.<span class="bu">float</span>().to(device)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># THE MAIN PART -&gt; LATENT SPACE TRAINING</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> im_dataset.use_latents:</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>                im, _ <span class="op">=</span> vae.encode(im)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn_like(im).to(device)</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> torch.randint(<span class="dv">0</span>, T, (im.shape[<span class="dv">0</span>],)).to(device)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>        noisy_im <span class="op">=</span> scheduler.add_noise(im, noise, t)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        noise_pred <span class="op">=</span> model(noisy_im, t)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(noise_pred, noise)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span>epoch_idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs_ldm<span class="sc">}</span><span class="ss">] | Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(losses)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), <span class="st">"../ldmCeleb/denoiseLatentModelCeleb.pth"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:16&lt;00:00,  7.31it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:16&lt;00:00,  7.31it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:16&lt;00:00,  7.32it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|██████████| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|██████████| 1875/1875 [04:18&lt;00:00,  7.27it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/100] | Loss: 0.17239664629101753
Epoch [2/100] | Loss: 0.13481004771987598
Epoch [3/100] | Loss: 0.12825398214956124
Epoch [4/100] | Loss: 0.12480706822971503
Epoch [5/100] | Loss: 0.12244796647131442
Epoch [6/100] | Loss: 0.12263716047604879
Epoch [7/100] | Loss: 0.11959034487803777
Epoch [8/100] | Loss: 0.12007127964595954
Epoch [9/100] | Loss: 0.11737980163097382
Epoch [10/100] | Loss: 0.11774407567977906
Epoch [11/100] | Loss: 0.1164932386537393
Epoch [12/100] | Loss: 0.11754837945401668
Epoch [13/100] | Loss: 0.11633194713791212
Epoch [14/100] | Loss: 0.11751290398438771
Epoch [15/100] | Loss: 0.1151164310703675
Epoch [16/100] | Loss: 0.11360364832480749
Epoch [17/100] | Loss: 0.11501107030709584
Epoch [18/100] | Loss: 0.11372175222436587
Epoch [19/100] | Loss: 0.11383305485347907
Epoch [20/100] | Loss: 0.1133527055879434
Epoch [21/100] | Loss: 0.11380577364961306
Epoch [22/100] | Loss: 0.11273235224882762
Epoch [23/100] | Loss: 0.1135343521485726
Epoch [24/100] | Loss: 0.1123530367632707
Epoch [25/100] | Loss: 0.1115516833047072
Epoch [26/100] | Loss: 0.11164849996964137
Epoch [27/100] | Loss: 0.11084560413658619
Epoch [28/100] | Loss: 0.11147701257665953
Epoch [29/100] | Loss: 0.1116309017131726
Epoch [30/100] | Loss: 0.11129296476145585
Epoch [31/100] | Loss: 0.11189121434191862
Epoch [32/100] | Loss: 0.11070120003223419
Epoch [33/100] | Loss: 0.11032614000439644
Epoch [34/100] | Loss: 0.10985745530923208
Epoch [35/100] | Loss: 0.11104562021692593
Epoch [36/100] | Loss: 0.11127305963635445
Epoch [37/100] | Loss: 0.10923469912409782
Epoch [38/100] | Loss: 0.11093338783582052
Epoch [39/100] | Loss: 0.11022963825563589
Epoch [40/100] | Loss: 0.10861355056762695
Epoch [41/100] | Loss: 0.11006656314432621
Epoch [42/100] | Loss: 0.11026263686021169
Epoch [43/100] | Loss: 0.10791801128884157
Epoch [44/100] | Loss: 0.1098509761840105
Epoch [45/100] | Loss: 0.11083510490258534
Epoch [46/100] | Loss: 0.10826958584785461
Epoch [47/100] | Loss: 0.11006076967120171
Epoch [48/100] | Loss: 0.10849106203317642
Epoch [49/100] | Loss: 0.10978733394245306
Epoch [50/100] | Loss: 0.10951394220292568
Epoch [51/100] | Loss: 0.10842547579109668
Epoch [52/100] | Loss: 0.10871950341761112
Epoch [53/100] | Loss: 0.10760296205282212
Epoch [54/100] | Loss: 0.10936867837011814
Epoch [55/100] | Loss: 0.1076629061371088
Epoch [56/100] | Loss: 0.108080473741889
Epoch [57/100] | Loss: 0.10893807614843051
Epoch [58/100] | Loss: 0.10773360221982002
Epoch [59/100] | Loss: 0.10701240785022577
Epoch [60/100] | Loss: 0.10817974474231402
Epoch [61/100] | Loss: 0.1084976889014244
Epoch [62/100] | Loss: 0.10656732607583205
Epoch [63/100] | Loss: 0.10788566062649091
Epoch [64/100] | Loss: 0.10937032189965248
Epoch [65/100] | Loss: 0.10807198148171107
Epoch [66/100] | Loss: 0.10871618385712306
Epoch [67/100] | Loss: 0.10831890054742495
Epoch [68/100] | Loss: 0.1058956669057409
Epoch [69/100] | Loss: 0.10795067014495532
Epoch [70/100] | Loss: 0.10718891451358795
Epoch [71/100] | Loss: 0.10608673804104328
Epoch [72/100] | Loss: 0.10626225626369318
Epoch [73/100] | Loss: 0.10670563013056913
Epoch [74/100] | Loss: 0.10741446313162645
Epoch [75/100] | Loss: 0.10694800455967585
Epoch [76/100] | Loss: 0.10626805338263512
Epoch [77/100] | Loss: 0.10666371670762698
Epoch [78/100] | Loss: 0.10609417064587275
Epoch [79/100] | Loss: 0.10631756011446317
Epoch [80/100] | Loss: 0.106819432669878
Epoch [81/100] | Loss: 0.10773934854666392
Epoch [82/100] | Loss: 0.10610651188592116
Epoch [83/100] | Loss: 0.10720135339101156
Epoch [84/100] | Loss: 0.10536787053346634
Epoch [85/100] | Loss: 0.10579571547110875
Epoch [86/100] | Loss: 0.1059078903088967
Epoch [87/100] | Loss: 0.10642880010406176
Epoch [88/100] | Loss: 0.10592716257870197
Epoch [89/100] | Loss: 0.10538040651679038
Epoch [90/100] | Loss: 0.10482307714124521
Epoch [91/100] | Loss: 0.106354721408089
Epoch [92/100] | Loss: 0.10680986197392146
Epoch [93/100] | Loss: 0.10599970742166043
Epoch [94/100] | Loss: 0.1052045267522335
Epoch [95/100] | Loss: 0.10490048675835133
Epoch [96/100] | Loss: 0.10531028269926707
Epoch [97/100] | Loss: 0.1053869799733162
Epoch [98/100] | Loss: 0.1060175249983867
Epoch [99/100] | Loss: 0.10516799224118392
Epoch [100/100] | Loss: 0.10356067033906778</code></pre>
</div>
</div>
</section>
<section id="sampling" class="level2">
<h2 class="anchored" data-anchor-id="sampling">Sampling</h2>
<div id="cell-34" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>num_grid_rows <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> LinearNoiseScheduler(T, beta_start, beta_end)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Unet(im_channels <span class="op">=</span> z_channels).to(device)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(<span class="st">"../ldmCeleb/denoiseLatentModelCeleb.pth"</span>, map_location <span class="op">=</span> device))</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>vae <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>vae.<span class="bu">eval</span>()</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>vae.load_state_dict(torch.load(<span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>, map_location<span class="op">=</span>device), strict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    im_size <span class="op">=</span> image_size <span class="op">//</span> (<span class="dv">2</span> <span class="op">**</span> (<span class="bu">sum</span>(model.down_sample)))</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    xt <span class="op">=</span> torch.randn((num_samples, z_channels, im_size, im_size)).to(device)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T)):</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        noise_pred <span class="op">=</span> model(xt, torch.as_tensor(t).unsqueeze(<span class="dv">0</span>).to(device))</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        xt, x0_pred <span class="op">=</span> scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        ims_raw <span class="op">=</span> torch.clamp(xt, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>).detach().cpu()</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        ims_raw <span class="op">=</span> (ims_raw <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> vae.decode(xt)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> torch.clamp(ims, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>).detach().cpu()</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> (ims <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>        grid_latent <span class="op">=</span> vutils.make_grid(ims_raw, nrow <span class="op">=</span> num_grid_rows, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>        grid_reconstructed <span class="op">=</span> vutils.make_grid(ims, nrow <span class="op">=</span> num_grid_rows, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (t <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> t <span class="op">==</span> T <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>            plt.figure(figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">"off"</span>)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>            plt.imshow(np.transpose(grid_latent.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">"off"</span>)</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>            plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>        img_latent <span class="op">=</span> transforms.ToPILImage()(grid_latent)</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>        img_decode <span class="op">=</span> transforms.ToPILImage()(grid_reconstructed)</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">"./ldm/CelebLatent"</span>):</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>            os.makedirs(<span class="st">"./ldm/CelebLatent"</span>)</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">"../ldm/CelebDecode"</span>):</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>            os.makedirs(<span class="st">"../ldm/CelebDecode"</span>)</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>        img_latent.save(<span class="ss">f"../ldm/CelebLatent/x0_</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>        img_latent.close()</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>        img_decode.save(<span class="ss">f"../ldm/CelebDecode/x0_</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>        img_decode.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Website made with <a href="https://quarto.org/">Quarto</a>, by Guntas Singh Saran.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guntas-13/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/guntas.saran13/">
      <i class="bi bi-instagram" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/guntas-singh-saran-2b8811179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:guntassingh.saran@iitgn.ac.in">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>