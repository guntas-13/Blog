<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Guntas Singh Saran">
<meta name="dcterms.date" content="2024-07-04">

<title>Guntas Blog - Implementing Latent Diffusion Models over CelebAHQ</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../tabicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Guntas Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://guntas-13.github.io"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guntas-13"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/guntas-singh-saran-2b8811179/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/guntas.saran13/"> <i class="bi bi-instagram" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="mailto:guntassingh.saran@iitgn.ac.in"> <i class="bi bi-envelope" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Implementing Latent Diffusion Models over CelebAHQ</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Generative Models</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Guntas Singh Saran </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">July 4, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#final-generation" id="toc-final-generation" class="nav-link active" data-scroll-target="#final-generation">Final Generation</a>
  <ul class="collapse">
  <li><a href="#latent-diffusion-models" id="toc-latent-diffusion-models" class="nav-link" data-scroll-target="#latent-diffusion-models"><strong>Latent Diffusion Models</strong></a>
  <ul class="collapse">
  <li><a href="#the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image." id="toc-the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image." class="nav-link" data-scroll-target="#the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image.">The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.</a></li>
  <li><a href="#the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy" id="toc-the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy" class="nav-link" data-scroll-target="#the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy">The downside is that although the <span class="math inline">\(L1/L2\)</span> reconstruction loss might be low, the perceptual features in the reconstructed image still might be <strong>fuzzy</strong></a></li>
  </ul></li>
  <li><a href="#perceptual-retention-textlpips-as-the-metric" id="toc-perceptual-retention-textlpips-as-the-metric" class="nav-link" data-scroll-target="#perceptual-retention-textlpips-as-the-metric">Perceptual Retention &amp; <span class="math inline">\(\text{LPIPS}\)</span> as the metric</a></li>
  <li><a href="#discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes" id="toc-discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes" class="nav-link" data-scroll-target="#discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes">Discretizing the Latent Space using the <span class="math inline">\(\text{CodeBooks}\)</span> from <span class="math inline">\(\text{VQVAEs}\)</span></a>
  <ul class="collapse">
  <li><a href="#textvqvae-as-the-textautoencoder" id="toc-textvqvae-as-the-textautoencoder" class="nav-link" data-scroll-target="#textvqvae-as-the-textautoencoder"><span class="math inline">\(\text{VQVAE}\)</span> as the <span class="math inline">\(\text{AutoEncoder}\)</span></a></li>
  </ul></li>
  <li><a href="#the-autoencoder-architecture" id="toc-the-autoencoder-architecture" class="nav-link" data-scroll-target="#the-autoencoder-architecture">The AutoEncoder Architecture</a>
  <ul class="collapse">
  <li><a href="#the-model-blocks" id="toc-the-model-blocks" class="nav-link" data-scroll-target="#the-model-blocks">The Model Blocks</a></li>
  <li><a href="#vqvae-implementation" id="toc-vqvae-implementation" class="nav-link" data-scroll-target="#vqvae-implementation">VQVAE Implementation</a></li>
  <li><a href="#discriminator" id="toc-discriminator" class="nav-link" data-scroll-target="#discriminator">Discriminator</a></li>
  </ul></li>
  <li><a href="#loading-dataset---celebahq" id="toc-loading-dataset---celebahq" class="nav-link" data-scroll-target="#loading-dataset---celebahq">Loading Dataset - CelebAHQ</a>
  <ul class="collapse">
  <li><a href="#textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w" id="toc-textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w" class="nav-link" data-scroll-target="#textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w"><span class="math inline">\(\texttt{im\_dataset}\)</span> contains <span class="math inline">\(30000\)</span> images of shape <span class="math inline">\((3 \times 256 \times 256) \to (C \times H \times W)\)</span></a></li>
  <li><a href="#textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects" id="toc-textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects" class="nav-link" data-scroll-target="#textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects"><span class="math inline">\(\texttt{celebALoader}\)</span> creates a dataloader of <span class="math inline">\(\texttt{batch\_size} = 8\)</span> i.e.&nbsp;each object in this loader is of shape <span class="math inline">\((8 \times 3 \times 256 \times 256) \to (B \times C \times H \times W)\)</span> and there are <span class="math inline">\(30000 / 8 = 3750\)</span> such objects</a></li>
  </ul></li>
  <li><a href="#training-of-autoencoder" id="toc-training-of-autoencoder" class="nav-link" data-scroll-target="#training-of-autoencoder">Training of AutoEncoder</a></li>
  </ul></li>
  <li><a href="#infer-from-vqvae-to-get-latents" id="toc-infer-from-vqvae-to-get-latents" class="nav-link" data-scroll-target="#infer-from-vqvae-to-get-latents">Infer from VQVAE to get latents</a>
  <ul class="collapse">
  <li><a href="#latent-diffusion-model-architecture" id="toc-latent-diffusion-model-architecture" class="nav-link" data-scroll-target="#latent-diffusion-model-architecture">Latent Diffusion Model Architecture</a>
  <ul class="collapse">
  <li><a href="#unet" id="toc-unet" class="nav-link" data-scroll-target="#unet">UNet</a></li>
  <li><a href="#linear-noise-scheduler" id="toc-linear-noise-scheduler" class="nav-link" data-scroll-target="#linear-noise-scheduler">Linear Noise Scheduler</a></li>
  </ul></li>
  <li><a href="#latent-diffusion-model-training" id="toc-latent-diffusion-model-training" class="nav-link" data-scroll-target="#latent-diffusion-model-training">Latent Diffusion Model Training</a></li>
  <li><a href="#use-latents-for-setting-up-the-data" id="toc-use-latents-for-setting-up-the-data" class="nav-link" data-scroll-target="#use-latents-for-setting-up-the-data">Use Latents for setting up the data</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">Sampling</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="final-generation" class="level1">
<h1>Final Generation</h1>
<p><img src="./x0_999ldm.png" style="float: left; width: 50%;"> <img src="./x0_0ldm.png" style="width: 50%;"></p>
<div id="cell-2" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> random</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.parallel</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.utils <span class="im">as</span> vutils</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> datasets, transforms, models</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm <span class="im">import</span> tqdm</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data.dataset <span class="im">import</span> Dataset</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> glob</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> HTML</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>config InlineBackend.figure_format <span class="op">=</span> <span class="st">"retina"</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available():</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(device)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cuda</code></pre>
</div>
</div>
<section id="latent-diffusion-models" class="level2">
<h2 class="anchored" data-anchor-id="latent-diffusion-models"><strong>Latent Diffusion Models</strong></h2>
<section id="the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image." class="level3">
<h3 class="anchored" data-anchor-id="the-idea-is-to-train-the-diffusion-models-on-a-low-dimensional-latent-representation-rather-than-the-entire-big-pixel-space.-in-addition-to-that-also-train-an-encoder-decoder-model-that-takes-the-original-image-converts-it-into-the-latent-representation-using-the-encoder-and-reconverts-the-latent-representation-to-the-reconstructed-image.">The idea is to train the diffusion models on a low dimensional latent representation rather than the entire big pixel space. In addition to that, also train an Encoder-Decoder model that takes the original image converts it into the latent representation using the encoder and reconverts the latent representation to the reconstructed image.</h3>
<p align="center">
<img src="./Latent2.png" style="width:60%;border:0;" alt="image">
</p>
</section>
<section id="the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy" class="level3">
<h3 class="anchored" data-anchor-id="the-downside-is-that-although-the-l1l2-reconstruction-loss-might-be-low-the-perceptual-features-in-the-reconstructed-image-still-might-be-fuzzy">The downside is that although the <span class="math inline">\(L1/L2\)</span> reconstruction loss might be low, the perceptual features in the reconstructed image still might be <strong>fuzzy</strong></h3>
</section>
</section>
<section id="perceptual-retention-textlpips-as-the-metric" class="level2">
<h2 class="anchored" data-anchor-id="perceptual-retention-textlpips-as-the-metric">Perceptual Retention &amp; <span class="math inline">\(\text{LPIPS}\)</span> as the metric</h2>
<p align="center">
<img src="./Percept.png" style="width:80%;border:0;" alt="image">
</p>
<p>CLearly as said that although the <span class="math inline">\(L_1\)</span> or <span class="math inline">\(L_2\)</span> reconstruction loss might be low for the image, yet the perceptual features in the image perceived by a human are still <strong>blurry</strong>. Now in order to understand how a model would perceive the image, there is no better place to dig into pretrained classification <strong>CNNs</strong> <span class="math inline">\(\to\)</span> <strong>VGGs</strong>. The goal is to bring the feature map extracted at each VGG layer to be very similar to the original imageâ€™s feature maps at each VGG layer. This distance metric between the feature maps extracted from the layers of a pretrained VGG is called the <strong>perceptual loss</strong>.</p>
<p align="center">
<img src="./LPIPS1.png" style="width:80%;border:0;" alt="image">
</p>
<p>Check out the original implementation too at <a href="https://github.com/richzhang/PerceptualSimilarity">Perceptual Similarity</a>.</p>
<div id="cell-5" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># lpips.py implementation</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> namedtuple</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> spatial_average(in_tens, keepdim <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> in_tens.mean([<span class="dv">2</span>, <span class="dv">3</span>], keepdim <span class="op">=</span> keepdim)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> vgg16(nn.Module):</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, requires_grad <span class="op">=</span> <span class="va">False</span>, pretrained <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        vgg_pretrained_features <span class="op">=</span> models.vgg16(pretrained <span class="op">=</span> pretrained).features</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice1 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice2 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice3 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice4 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.slice5 <span class="op">=</span> nn.Sequential()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N_slices <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice1.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>, <span class="dv">9</span>):</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice2.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">9</span>, <span class="dv">16</span>):</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice3.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">16</span>, <span class="dv">23</span>):</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice4.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">23</span>, <span class="dv">30</span>):</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.slice5.add_module(<span class="bu">str</span>(x), vgg_pretrained_features[x])</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Freeze the model</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> requires_grad <span class="op">==</span> <span class="va">False</span>:</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>                param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X):</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice1(X)</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        h_relu1_2 <span class="op">=</span> h</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice2(h)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>        h_relu2_2 <span class="op">=</span> h</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice3(h)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>        h_relu3_3 <span class="op">=</span> h</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice4(h)</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>        h_relu4_3 <span class="op">=</span> h</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>        h <span class="op">=</span> <span class="va">self</span>.slice5(h)</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a>        h_relu5_3 <span class="op">=</span> h</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a>        vgg_outputs <span class="op">=</span> namedtuple(<span class="st">"VggOutputs"</span>, [<span class="st">"relu1_2"</span>, <span class="st">"relu2_2"</span>, <span class="st">"relu3_3"</span>, <span class="st">"relu4_3"</span>, <span class="st">"relu5_3"</span>])</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)</span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ScalingLayer(nn.Module):</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mean = [0.485, 0.456, 0.406]</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># std = [0.229, 0.224, 0.225]</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"shift"</span>, torch.tensor([<span class="op">-</span><span class="fl">.030</span>, <span class="op">-</span><span class="fl">.088</span>, <span class="op">-</span><span class="fl">.188</span>])[<span class="va">None</span>, :, <span class="va">None</span>, <span class="va">None</span>])</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">"scale"</span>, torch.tensor([<span class="fl">.458</span>, <span class="fl">.448</span>, <span class="fl">.450</span>])[<span class="va">None</span>, :, <span class="va">None</span>, <span class="va">None</span>])</span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inp):</span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (inp <span class="op">-</span> <span class="va">self</span>.shift) <span class="op">/</span> <span class="va">self</span>.scale</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NetLinLayer(nn.Module):</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, chn_in, chn_out <span class="op">=</span> <span class="dv">1</span>, use_dropout <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> [nn.Dropout(), ] <span class="cf">if</span> (use_dropout) <span class="cf">else</span> []</span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">+=</span> [nn.Conv2d(chn_in, chn_out, kernel_size <span class="op">=</span> <span class="dv">1</span>, stride <span class="op">=</span> <span class="dv">1</span>, padding <span class="op">=</span> <span class="dv">0</span>, bias <span class="op">=</span> <span class="va">False</span>), ]</span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> nn.Sequential(<span class="op">*</span>layers)</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.model(x)</span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LPIPS(nn.Module):</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, use_dropout <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.scaling_layer <span class="op">=</span> ScalingLayer()</span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.chns <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">512</span>, <span class="dv">512</span>]</span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.L <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.chns)</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net <span class="op">=</span> vgg16(pretrained <span class="op">=</span> <span class="va">True</span>, requires_grad <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin0 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">0</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin1 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">1</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin2 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">2</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin3 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">3</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lin4 <span class="op">=</span> NetLinLayer(<span class="va">self</span>.chns[<span class="dv">4</span>], use_dropout <span class="op">=</span> use_dropout)</span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lins <span class="op">=</span> [<span class="va">self</span>.lin0, <span class="va">self</span>.lin1, <span class="va">self</span>.lin2, <span class="va">self</span>.lin3, <span class="va">self</span>.lin4]</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lins <span class="op">=</span> nn.ModuleList(<span class="va">self</span>.lins)</span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.net.load_state_dict(torch.load(<span class="st">"./vgg.pth"</span>, map_location <span class="op">=</span> device), strict <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">eval</span>()</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> param <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a>            param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, in0, in1, normalize <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> normalize:</span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a>            in0 <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> in0 <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a>            in1 <span class="op">=</span> <span class="dv">2</span> <span class="op">*</span> in1 <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a>        in0_input, in1_input <span class="op">=</span> <span class="va">self</span>.scaling_layer(in0), <span class="va">self</span>.scaling_layer(in1)</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a>        outs0, outs1 <span class="op">=</span> <span class="va">self</span>.net.forward(in0_input), <span class="va">self</span>.net.forward(in1_input)</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a>        feats0, feats1, diffs <span class="op">=</span> {}, {}, {}</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L):</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a>            feats0[kk], feats1[kk] <span class="op">=</span> F.normalize(outs0[kk], dim <span class="op">=</span> <span class="dv">1</span>), F.normalize(outs1[kk])</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a>            diffs[kk] <span class="op">=</span> (feats0[kk] <span class="op">-</span> feats1[kk]) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a>        res <span class="op">=</span> [spatial_average(<span class="va">self</span>.lins[kk](diffs[kk]), keepdim <span class="op">=</span> <span class="va">True</span>) <span class="cf">for</span> kk <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L)]</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a>        val <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.L):</span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a>            val <span class="op">+=</span> res[l]</span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> val</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes" class="level2">
<h2 class="anchored" data-anchor-id="discretizing-the-latent-space-using-the-textcodebooks-from-textvqvaes">Discretizing the Latent Space using the <span class="math inline">\(\text{CodeBooks}\)</span> from <span class="math inline">\(\text{VQVAEs}\)</span></h2>
<section id="textvqvae-as-the-textautoencoder" class="level3">
<h3 class="anchored" data-anchor-id="textvqvae-as-the-textautoencoder"><span class="math inline">\(\text{VQVAE}\)</span> as the <span class="math inline">\(\text{AutoEncoder}\)</span></h3>
<p><span class="math inline">\(k\)</span> vectors, each of <span class="math inline">\(d\)</span> dimensions <span class="math inline">\((k \times d)\)</span> help us encode the data.</p>
<p align="center">
<img src="./VQVAE1.png" style="width:70%;border:0;" alt="image">
</p>
The encoder generates a feature map of <span class="math inline">\(H \times W\)</span> features each of <span class="math inline">\(d\)</span> dimension.
<p align="center">
<img src="./VQVAE2.png" style="width:70%;border:0;" alt="image">
</p>
<p>For each of the features, we find the nearest <span class="math inline">\(d\)</span> dimensional encoding to it and replace it with that.</p>
<p><span class="math display">\[ z_q(x) = e_k \]</span> <span class="math display">\[ k = \argmin_j || z_e(x) - e_j ||_2 \]</span></p>
<p align="center">
<img src="./VQVAE3.png" style="width:70%;border:0;" alt="image">
</p>
The decoder then discards off the feature map given by the encoder and only uses the nearest codeblock feature map to reconstruct the output image.
<p align="center">
<img src="./VQVAE4.png" style="width:70%;border:0;" alt="image">
</p>
The issue is we have to define the gradients for the <span class="math inline">\(\argmin\)</span> step separately for the gradients to flow back. We approximate the gradient similar to the straight-through estimator and just copy gradients from decoder input <span class="math inline">\(z_q(x)\)</span> to encoder output <span class="math inline">\(z_e(x)\)</span>
<p align="center">
<img src="./VQVAE5.png" style="width:70%;border:0;" alt="image">
</p>
<p><span class="math display">\[ L = \log p(x | z_q(x)) + || \text{sg}[z_e(x)] - e ||_2^2 + \beta || z_e(x) - \text{sg}[e] ||_2^2 \]</span></p>
</section>
</section>
<section id="the-autoencoder-architecture" class="level2">
<h2 class="anchored" data-anchor-id="the-autoencoder-architecture">The AutoEncoder Architecture</h2>
<p align="center">
<img src="./Latent3.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./Latent4.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc1.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc2.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc3.png" style="width:80%;border:0;" alt="image">
</p>
<p align="center">
<img src="./AutoEnc4.png" style="width:80%;border:0;" alt="image">
</p>
<section id="the-model-blocks" class="level3">
<h3 class="anchored" data-anchor-id="the-model-blocks">The Model Blocks</h3>
<p>Adapted from <a href="https://github.com/explainingai-code/StableDiffusion-PyTorch/blob/main/models/blocks.py">ExplainingAI</a></p>
<div id="cell-9" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Time Embedding</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_time_embedding(T, d_model):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    factor <span class="op">=</span> <span class="dv">10000</span> <span class="op">**</span> ((torch.arange(start <span class="op">=</span> <span class="dv">0</span>, end <span class="op">=</span> d_model <span class="op">//</span> <span class="dv">2</span>, dtype <span class="op">=</span> torch.float32, device <span class="op">=</span> T.device)) <span class="op">/</span> (d_model <span class="op">//</span> <span class="dv">2</span>))</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    t_emb <span class="op">=</span> T[:, <span class="va">None</span>].repeat(<span class="dv">1</span>, d_model <span class="op">//</span> <span class="dv">2</span>) <span class="op">/</span> factor</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    t_emb <span class="op">=</span> torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim <span class="op">=</span> <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> t_emb</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Blocks</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DownBlock(nn.Module):</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co">    Down conv block with attention.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following block</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Resnet block with time embedding</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Attention block</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Downsample</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim,</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>                 down_sample, num_heads, num_layers, attn, norm_channels, cross_attn<span class="op">=</span><span class="va">False</span>, context_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> down_sample</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> attn</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dim <span class="op">=</span> context_dim</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> cross_attn</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels,</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>                              kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(<span class="va">self</span>.t_emb_dim, out_channels)</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels,</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>                              kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> context_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"Context Dimension must be passed for cross attention"</span></span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_proj <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>                [nn.Linear(context_dim, out_channels)</span>
<span id="cb4-84"><a href="#cb4-84" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-85"><a href="#cb4-85" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-86"><a href="#cb4-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-87"><a href="#cb4-87" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-88"><a href="#cb4-88" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-89"><a href="#cb4-89" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-90"><a href="#cb4-90" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-91"><a href="#cb4-91" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-92"><a href="#cb4-92" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-93"><a href="#cb4-93" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample_conv <span class="op">=</span> nn.Conv2d(out_channels, out_channels,</span>
<span id="cb4-94"><a href="#cb4-94" aria-hidden="true" tabindex="-1"></a>                                          <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="cf">if</span> <span class="va">self</span>.down_sample <span class="cf">else</span> nn.Identity()</span>
<span id="cb4-95"><a href="#cb4-95" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-96"><a href="#cb4-96" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t_emb<span class="op">=</span><span class="va">None</span>, context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-97"><a href="#cb4-97" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-98"><a href="#cb4-98" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-99"><a href="#cb4-99" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet block of Unet</span></span>
<span id="cb4-100"><a href="#cb4-100" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-101"><a href="#cb4-101" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i](out)</span>
<span id="cb4-102"><a href="#cb4-102" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-103"><a href="#cb4-103" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-104"><a href="#cb4-104" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i](out)</span>
<span id="cb4-105"><a href="#cb4-105" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i](resnet_input)</span>
<span id="cb4-106"><a href="#cb4-106" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-107"><a href="#cb4-107" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-108"><a href="#cb4-108" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Attention block of Unet</span></span>
<span id="cb4-109"><a href="#cb4-109" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-110"><a href="#cb4-110" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-111"><a href="#cb4-111" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-112"><a href="#cb4-112" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-113"><a href="#cb4-113" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-114"><a href="#cb4-114" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-115"><a href="#cb4-115" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-116"><a href="#cb4-116" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-117"><a href="#cb4-117" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-118"><a href="#cb4-118" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"context cannot be None if cross attention layers are used"</span></span>
<span id="cb4-119"><a href="#cb4-119" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-120"><a href="#cb4-120" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-121"><a href="#cb4-121" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.cross_attention_norms[i](in_attn)</span>
<span id="cb4-122"><a href="#cb4-122" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-123"><a href="#cb4-123" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context.shape[<span class="dv">0</span>] <span class="op">==</span> x.shape[<span class="dv">0</span>] <span class="kw">and</span> context.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.context_dim</span>
<span id="cb4-124"><a href="#cb4-124" aria-hidden="true" tabindex="-1"></a>                context_proj <span class="op">=</span> <span class="va">self</span>.context_proj[i](context)</span>
<span id="cb4-125"><a href="#cb4-125" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.cross_attentions[i](in_attn, context_proj, context_proj)</span>
<span id="cb4-126"><a href="#cb4-126" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-127"><a href="#cb4-127" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-128"><a href="#cb4-128" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-129"><a href="#cb4-129" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Downsample</span></span>
<span id="cb4-130"><a href="#cb4-130" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.down_sample_conv(out)</span>
<span id="cb4-131"><a href="#cb4-131" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-132"><a href="#cb4-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-133"><a href="#cb4-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-134"><a href="#cb4-134" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MidBlock(nn.Module):</span>
<span id="cb4-135"><a href="#cb4-135" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-136"><a href="#cb4-136" aria-hidden="true" tabindex="-1"></a><span class="co">    Mid conv block with attention.</span></span>
<span id="cb4-137"><a href="#cb4-137" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following blocks</span></span>
<span id="cb4-138"><a href="#cb4-138" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Resnet block with time embedding</span></span>
<span id="cb4-139"><a href="#cb4-139" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Attention block</span></span>
<span id="cb4-140"><a href="#cb4-140" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Resnet block with time embedding</span></span>
<span id="cb4-141"><a href="#cb4-141" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-142"><a href="#cb4-142" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-143"><a href="#cb4-143" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn<span class="op">=</span><span class="va">None</span>, context_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-144"><a href="#cb4-144" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-145"><a href="#cb4-145" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-146"><a href="#cb4-146" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-147"><a href="#cb4-147" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dim <span class="op">=</span> context_dim</span>
<span id="cb4-148"><a href="#cb4-148" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> cross_attn</span>
<span id="cb4-149"><a href="#cb4-149" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-150"><a href="#cb4-150" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-151"><a href="#cb4-151" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-152"><a href="#cb4-152" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-153"><a href="#cb4-153" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-154"><a href="#cb4-154" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-155"><a href="#cb4-155" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-156"><a href="#cb4-156" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-157"><a href="#cb4-157" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-158"><a href="#cb4-158" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-159"><a href="#cb4-159" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-160"><a href="#cb4-160" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-161"><a href="#cb4-161" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-162"><a href="#cb4-162" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-163"><a href="#cb4-163" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-164"><a href="#cb4-164" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-165"><a href="#cb4-165" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(t_emb_dim, out_channels)</span>
<span id="cb4-166"><a href="#cb4-166" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-167"><a href="#cb4-167" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-168"><a href="#cb4-168" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-169"><a href="#cb4-169" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-170"><a href="#cb4-170" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-171"><a href="#cb4-171" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-172"><a href="#cb4-172" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-173"><a href="#cb4-173" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-174"><a href="#cb4-174" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-175"><a href="#cb4-175" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-176"><a href="#cb4-176" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-177"><a href="#cb4-177" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-178"><a href="#cb4-178" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-179"><a href="#cb4-179" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-180"><a href="#cb4-180" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-181"><a href="#cb4-181" aria-hidden="true" tabindex="-1"></a>            [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-182"><a href="#cb4-182" aria-hidden="true" tabindex="-1"></a>             <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-183"><a href="#cb4-183" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-184"><a href="#cb4-184" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-185"><a href="#cb4-185" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-186"><a href="#cb4-186" aria-hidden="true" tabindex="-1"></a>            [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-187"><a href="#cb4-187" aria-hidden="true" tabindex="-1"></a>             <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-188"><a href="#cb4-188" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-189"><a href="#cb4-189" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-190"><a href="#cb4-190" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> context_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"Context Dimension must be passed for cross attention"</span></span>
<span id="cb4-191"><a href="#cb4-191" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-192"><a href="#cb4-192" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-193"><a href="#cb4-193" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-194"><a href="#cb4-194" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-195"><a href="#cb4-195" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-196"><a href="#cb4-196" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-197"><a href="#cb4-197" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-198"><a href="#cb4-198" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-199"><a href="#cb4-199" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_proj <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-200"><a href="#cb4-200" aria-hidden="true" tabindex="-1"></a>                [nn.Linear(context_dim, out_channels)</span>
<span id="cb4-201"><a href="#cb4-201" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-202"><a href="#cb4-202" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-203"><a href="#cb4-203" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-204"><a href="#cb4-204" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-205"><a href="#cb4-205" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-206"><a href="#cb4-206" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb4-207"><a href="#cb4-207" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-208"><a href="#cb4-208" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-209"><a href="#cb4-209" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-210"><a href="#cb4-210" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t_emb<span class="op">=</span><span class="va">None</span>, context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-211"><a href="#cb4-211" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-212"><a href="#cb4-212" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-213"><a href="#cb4-213" aria-hidden="true" tabindex="-1"></a>        <span class="co"># First resnet block</span></span>
<span id="cb4-214"><a href="#cb4-214" aria-hidden="true" tabindex="-1"></a>        resnet_input <span class="op">=</span> out</span>
<span id="cb4-215"><a href="#cb4-215" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[<span class="dv">0</span>](out)</span>
<span id="cb4-216"><a href="#cb4-216" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-217"><a href="#cb4-217" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[<span class="dv">0</span>](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-218"><a href="#cb4-218" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[<span class="dv">0</span>](out)</span>
<span id="cb4-219"><a href="#cb4-219" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[<span class="dv">0</span>](resnet_input)</span>
<span id="cb4-220"><a href="#cb4-220" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-221"><a href="#cb4-221" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-222"><a href="#cb4-222" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Attention Block</span></span>
<span id="cb4-223"><a href="#cb4-223" aria-hidden="true" tabindex="-1"></a>            batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-224"><a href="#cb4-224" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-225"><a href="#cb4-225" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-226"><a href="#cb4-226" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-227"><a href="#cb4-227" aria-hidden="true" tabindex="-1"></a>            out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-228"><a href="#cb4-228" aria-hidden="true" tabindex="-1"></a>            out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-229"><a href="#cb4-229" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-230"><a href="#cb4-230" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-231"><a href="#cb4-231" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-232"><a href="#cb4-232" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"context cannot be None if cross attention layers are used"</span></span>
<span id="cb4-233"><a href="#cb4-233" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-234"><a href="#cb4-234" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-235"><a href="#cb4-235" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.cross_attention_norms[i](in_attn)</span>
<span id="cb4-236"><a href="#cb4-236" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-237"><a href="#cb4-237" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context.shape[<span class="dv">0</span>] <span class="op">==</span> x.shape[<span class="dv">0</span>] <span class="kw">and</span> context.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.context_dim</span>
<span id="cb4-238"><a href="#cb4-238" aria-hidden="true" tabindex="-1"></a>                context_proj <span class="op">=</span> <span class="va">self</span>.context_proj[i](context)</span>
<span id="cb4-239"><a href="#cb4-239" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.cross_attentions[i](in_attn, context_proj, context_proj)</span>
<span id="cb4-240"><a href="#cb4-240" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-241"><a href="#cb4-241" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-242"><a href="#cb4-242" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb4-243"><a href="#cb4-243" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-244"><a href="#cb4-244" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet Block</span></span>
<span id="cb4-245"><a href="#cb4-245" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-246"><a href="#cb4-246" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i <span class="op">+</span> <span class="dv">1</span>](out)</span>
<span id="cb4-247"><a href="#cb4-247" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-248"><a href="#cb4-248" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i <span class="op">+</span> <span class="dv">1</span>](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-249"><a href="#cb4-249" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i <span class="op">+</span> <span class="dv">1</span>](out)</span>
<span id="cb4-250"><a href="#cb4-250" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i <span class="op">+</span> <span class="dv">1</span>](resnet_input)</span>
<span id="cb4-251"><a href="#cb4-251" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-252"><a href="#cb4-252" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-253"><a href="#cb4-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-254"><a href="#cb4-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-255"><a href="#cb4-255" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UpBlock(nn.Module):</span>
<span id="cb4-256"><a href="#cb4-256" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-257"><a href="#cb4-257" aria-hidden="true" tabindex="-1"></a><span class="co">    Up conv block with attention.</span></span>
<span id="cb4-258"><a href="#cb4-258" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following blocks</span></span>
<span id="cb4-259"><a href="#cb4-259" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Upsample</span></span>
<span id="cb4-260"><a href="#cb4-260" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Concatenate Down block output</span></span>
<span id="cb4-261"><a href="#cb4-261" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Resnet block with time embedding</span></span>
<span id="cb4-262"><a href="#cb4-262" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Attention Block</span></span>
<span id="cb4-263"><a href="#cb4-263" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-264"><a href="#cb4-264" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-265"><a href="#cb4-265" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim,</span>
<span id="cb4-266"><a href="#cb4-266" aria-hidden="true" tabindex="-1"></a>                 up_sample, num_heads, num_layers, attn, norm_channels):</span>
<span id="cb4-267"><a href="#cb4-267" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-268"><a href="#cb4-268" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-269"><a href="#cb4-269" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> up_sample</span>
<span id="cb4-270"><a href="#cb4-270" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-271"><a href="#cb4-271" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> attn</span>
<span id="cb4-272"><a href="#cb4-272" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-273"><a href="#cb4-273" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-274"><a href="#cb4-274" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-275"><a href="#cb4-275" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-276"><a href="#cb4-276" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-277"><a href="#cb4-277" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-278"><a href="#cb4-278" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-279"><a href="#cb4-279" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-280"><a href="#cb4-280" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-281"><a href="#cb4-281" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-282"><a href="#cb4-282" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-283"><a href="#cb4-283" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-284"><a href="#cb4-284" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-285"><a href="#cb4-285" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-286"><a href="#cb4-286" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-287"><a href="#cb4-287" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-288"><a href="#cb4-288" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(t_emb_dim, out_channels)</span>
<span id="cb4-289"><a href="#cb4-289" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-290"><a href="#cb4-290" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-291"><a href="#cb4-291" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-292"><a href="#cb4-292" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-293"><a href="#cb4-293" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-294"><a href="#cb4-294" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-295"><a href="#cb4-295" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-296"><a href="#cb4-296" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-297"><a href="#cb4-297" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-298"><a href="#cb4-298" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-299"><a href="#cb4-299" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-300"><a href="#cb4-300" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-301"><a href="#cb4-301" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-302"><a href="#cb4-302" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-303"><a href="#cb4-303" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-304"><a href="#cb4-304" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-305"><a href="#cb4-305" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb4-306"><a href="#cb4-306" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-307"><a href="#cb4-307" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-308"><a href="#cb4-308" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb4-309"><a href="#cb4-309" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-310"><a href="#cb4-310" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-311"><a href="#cb4-311" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-312"><a href="#cb4-312" aria-hidden="true" tabindex="-1"></a>                [</span>
<span id="cb4-313"><a href="#cb4-313" aria-hidden="true" tabindex="-1"></a>                    nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-314"><a href="#cb4-314" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-315"><a href="#cb4-315" aria-hidden="true" tabindex="-1"></a>                ]</span>
<span id="cb4-316"><a href="#cb4-316" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-317"><a href="#cb4-317" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-318"><a href="#cb4-318" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-319"><a href="#cb4-319" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-320"><a href="#cb4-320" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-321"><a href="#cb4-321" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-322"><a href="#cb4-322" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-323"><a href="#cb4-323" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-324"><a href="#cb4-324" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample_conv <span class="op">=</span> nn.ConvTranspose2d(in_channels, in_channels,</span>
<span id="cb4-325"><a href="#cb4-325" aria-hidden="true" tabindex="-1"></a>                                                 <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">\</span></span>
<span id="cb4-326"><a href="#cb4-326" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.up_sample <span class="cf">else</span> nn.Identity()</span>
<span id="cb4-327"><a href="#cb4-327" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-328"><a href="#cb4-328" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, out_down<span class="op">=</span><span class="va">None</span>, t_emb<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-329"><a href="#cb4-329" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Upsample</span></span>
<span id="cb4-330"><a href="#cb4-330" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up_sample_conv(x)</span>
<span id="cb4-331"><a href="#cb4-331" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-332"><a href="#cb4-332" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Concat with Downblock output</span></span>
<span id="cb4-333"><a href="#cb4-333" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> out_down <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-334"><a href="#cb4-334" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, out_down], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-335"><a href="#cb4-335" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-336"><a href="#cb4-336" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-337"><a href="#cb4-337" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-338"><a href="#cb4-338" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet Block</span></span>
<span id="cb4-339"><a href="#cb4-339" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-340"><a href="#cb4-340" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i](out)</span>
<span id="cb4-341"><a href="#cb4-341" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-342"><a href="#cb4-342" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-343"><a href="#cb4-343" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i](out)</span>
<span id="cb4-344"><a href="#cb4-344" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i](resnet_input)</span>
<span id="cb4-345"><a href="#cb4-345" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-346"><a href="#cb4-346" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Self Attention</span></span>
<span id="cb4-347"><a href="#cb4-347" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.attn:</span>
<span id="cb4-348"><a href="#cb4-348" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-349"><a href="#cb4-349" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-350"><a href="#cb4-350" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-351"><a href="#cb4-351" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-352"><a href="#cb4-352" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-353"><a href="#cb4-353" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-354"><a href="#cb4-354" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-355"><a href="#cb4-355" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb4-356"><a href="#cb4-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-357"><a href="#cb4-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-358"><a href="#cb4-358" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UpBlockUnet(nn.Module):</span>
<span id="cb4-359"><a href="#cb4-359" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb4-360"><a href="#cb4-360" aria-hidden="true" tabindex="-1"></a><span class="co">    Up conv block with attention.</span></span>
<span id="cb4-361"><a href="#cb4-361" aria-hidden="true" tabindex="-1"></a><span class="co">    Sequence of following blocks</span></span>
<span id="cb4-362"><a href="#cb4-362" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Upsample</span></span>
<span id="cb4-363"><a href="#cb4-363" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Concatenate Down block output</span></span>
<span id="cb4-364"><a href="#cb4-364" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Resnet block with time embedding</span></span>
<span id="cb4-365"><a href="#cb4-365" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Attention Block</span></span>
<span id="cb4-366"><a href="#cb4-366" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-367"><a href="#cb4-367" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-368"><a href="#cb4-368" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_channels, out_channels, t_emb_dim, up_sample,</span>
<span id="cb4-369"><a href="#cb4-369" aria-hidden="true" tabindex="-1"></a>                 num_heads, num_layers, norm_channels, cross_attn<span class="op">=</span><span class="va">False</span>, context_dim<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-370"><a href="#cb4-370" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-371"><a href="#cb4-371" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_layers <span class="op">=</span> num_layers</span>
<span id="cb4-372"><a href="#cb4-372" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> up_sample</span>
<span id="cb4-373"><a href="#cb4-373" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> t_emb_dim</span>
<span id="cb4-374"><a href="#cb4-374" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cross_attn <span class="op">=</span> cross_attn</span>
<span id="cb4-375"><a href="#cb4-375" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.context_dim <span class="op">=</span> context_dim</span>
<span id="cb4-376"><a href="#cb4-376" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_first <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-377"><a href="#cb4-377" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-378"><a href="#cb4-378" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-379"><a href="#cb4-379" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels),</span>
<span id="cb4-380"><a href="#cb4-380" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-381"><a href="#cb4-381" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-382"><a href="#cb4-382" aria-hidden="true" tabindex="-1"></a>                              padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-383"><a href="#cb4-383" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-384"><a href="#cb4-384" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-385"><a href="#cb4-385" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-386"><a href="#cb4-386" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-387"><a href="#cb4-387" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-388"><a href="#cb4-388" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-389"><a href="#cb4-389" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.t_emb_layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-390"><a href="#cb4-390" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-391"><a href="#cb4-391" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-392"><a href="#cb4-392" aria-hidden="true" tabindex="-1"></a>                    nn.Linear(t_emb_dim, out_channels)</span>
<span id="cb4-393"><a href="#cb4-393" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-394"><a href="#cb4-394" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-395"><a href="#cb4-395" aria-hidden="true" tabindex="-1"></a>            ])</span>
<span id="cb4-396"><a href="#cb4-396" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-397"><a href="#cb4-397" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resnet_conv_second <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-398"><a href="#cb4-398" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-399"><a href="#cb4-399" aria-hidden="true" tabindex="-1"></a>                nn.Sequential(</span>
<span id="cb4-400"><a href="#cb4-400" aria-hidden="true" tabindex="-1"></a>                    nn.GroupNorm(norm_channels, out_channels),</span>
<span id="cb4-401"><a href="#cb4-401" aria-hidden="true" tabindex="-1"></a>                    nn.SiLU(),</span>
<span id="cb4-402"><a href="#cb4-402" aria-hidden="true" tabindex="-1"></a>                    nn.Conv2d(out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-403"><a href="#cb4-403" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb4-404"><a href="#cb4-404" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-405"><a href="#cb4-405" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-406"><a href="#cb4-406" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-407"><a href="#cb4-407" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-408"><a href="#cb4-408" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-409"><a href="#cb4-409" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-410"><a href="#cb4-410" aria-hidden="true" tabindex="-1"></a>                nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-411"><a href="#cb4-411" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-412"><a href="#cb4-412" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-413"><a href="#cb4-413" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-414"><a href="#cb4-414" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-415"><a href="#cb4-415" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-416"><a href="#cb4-416" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-417"><a href="#cb4-417" aria-hidden="true" tabindex="-1"></a>                nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-418"><a href="#cb4-418" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-419"><a href="#cb4-419" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-420"><a href="#cb4-420" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-421"><a href="#cb4-421" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-422"><a href="#cb4-422" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-423"><a href="#cb4-423" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> context_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"Context Dimension must be passed for cross attention"</span></span>
<span id="cb4-424"><a href="#cb4-424" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attention_norms <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-425"><a href="#cb4-425" aria-hidden="true" tabindex="-1"></a>                [nn.GroupNorm(norm_channels, out_channels)</span>
<span id="cb4-426"><a href="#cb4-426" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-427"><a href="#cb4-427" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-428"><a href="#cb4-428" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cross_attentions <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-429"><a href="#cb4-429" aria-hidden="true" tabindex="-1"></a>                [nn.MultiheadAttention(out_channels, num_heads, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-430"><a href="#cb4-430" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-431"><a href="#cb4-431" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-432"><a href="#cb4-432" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.context_proj <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-433"><a href="#cb4-433" aria-hidden="true" tabindex="-1"></a>                [nn.Linear(context_dim, out_channels)</span>
<span id="cb4-434"><a href="#cb4-434" aria-hidden="true" tabindex="-1"></a>                 <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_layers)]</span>
<span id="cb4-435"><a href="#cb4-435" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb4-436"><a href="#cb4-436" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residual_input_conv <span class="op">=</span> nn.ModuleList(</span>
<span id="cb4-437"><a href="#cb4-437" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb4-438"><a href="#cb4-438" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(in_channels <span class="cf">if</span> i <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> out_channels, out_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-439"><a href="#cb4-439" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_layers)</span>
<span id="cb4-440"><a href="#cb4-440" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb4-441"><a href="#cb4-441" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-442"><a href="#cb4-442" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample_conv <span class="op">=</span> nn.ConvTranspose2d(in_channels <span class="op">//</span> <span class="dv">2</span>, in_channels <span class="op">//</span> <span class="dv">2</span>,</span>
<span id="cb4-443"><a href="#cb4-443" aria-hidden="true" tabindex="-1"></a>                                                 <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>) <span class="op">\</span></span>
<span id="cb4-444"><a href="#cb4-444" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.up_sample <span class="cf">else</span> nn.Identity()</span>
<span id="cb4-445"><a href="#cb4-445" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-446"><a href="#cb4-446" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, out_down<span class="op">=</span><span class="va">None</span>, t_emb<span class="op">=</span><span class="va">None</span>, context<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb4-447"><a href="#cb4-447" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.up_sample_conv(x)</span>
<span id="cb4-448"><a href="#cb4-448" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> out_down <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-449"><a href="#cb4-449" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> torch.cat([x, out_down], dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb4-450"><a href="#cb4-450" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-451"><a href="#cb4-451" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb4-452"><a href="#cb4-452" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.num_layers):</span>
<span id="cb4-453"><a href="#cb4-453" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Resnet</span></span>
<span id="cb4-454"><a href="#cb4-454" aria-hidden="true" tabindex="-1"></a>            resnet_input <span class="op">=</span> out</span>
<span id="cb4-455"><a href="#cb4-455" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_first[i](out)</span>
<span id="cb4-456"><a href="#cb4-456" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.t_emb_dim <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb4-457"><a href="#cb4-457" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.t_emb_layers[i](t_emb)[:, :, <span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb4-458"><a href="#cb4-458" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> <span class="va">self</span>.resnet_conv_second[i](out)</span>
<span id="cb4-459"><a href="#cb4-459" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> <span class="va">self</span>.residual_input_conv[i](resnet_input)</span>
<span id="cb4-460"><a href="#cb4-460" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Self Attention</span></span>
<span id="cb4-461"><a href="#cb4-461" aria-hidden="true" tabindex="-1"></a>            batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-462"><a href="#cb4-462" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-463"><a href="#cb4-463" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> <span class="va">self</span>.attention_norms[i](in_attn)</span>
<span id="cb4-464"><a href="#cb4-464" aria-hidden="true" tabindex="-1"></a>            in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-465"><a href="#cb4-465" aria-hidden="true" tabindex="-1"></a>            out_attn, _ <span class="op">=</span> <span class="va">self</span>.attentions[i](in_attn, in_attn, in_attn)</span>
<span id="cb4-466"><a href="#cb4-466" aria-hidden="true" tabindex="-1"></a>            out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-467"><a href="#cb4-467" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-468"><a href="#cb4-468" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Cross Attention</span></span>
<span id="cb4-469"><a href="#cb4-469" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="va">self</span>.cross_attn:</span>
<span id="cb4-470"><a href="#cb4-470" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>, <span class="st">"context cannot be None if cross attention layers are used"</span></span>
<span id="cb4-471"><a href="#cb4-471" aria-hidden="true" tabindex="-1"></a>                batch_size, channels, h, w <span class="op">=</span> out.shape</span>
<span id="cb4-472"><a href="#cb4-472" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> out.reshape(batch_size, channels, h <span class="op">*</span> w)</span>
<span id="cb4-473"><a href="#cb4-473" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> <span class="va">self</span>.cross_attention_norms[i](in_attn)</span>
<span id="cb4-474"><a href="#cb4-474" aria-hidden="true" tabindex="-1"></a>                in_attn <span class="op">=</span> in_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb4-475"><a href="#cb4-475" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> <span class="bu">len</span>(context.shape) <span class="op">==</span> <span class="dv">3</span>, <span class="op">\</span></span>
<span id="cb4-476"><a href="#cb4-476" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Context shape does not match B,_,CONTEXT_DIM"</span></span>
<span id="cb4-477"><a href="#cb4-477" aria-hidden="true" tabindex="-1"></a>                <span class="cf">assert</span> context.shape[<span class="dv">0</span>] <span class="op">==</span> x.shape[<span class="dv">0</span>] <span class="kw">and</span> context.shape[<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> <span class="va">self</span>.context_dim,<span class="op">\</span></span>
<span id="cb4-478"><a href="#cb4-478" aria-hidden="true" tabindex="-1"></a>                    <span class="st">"Context shape does not match B,_,CONTEXT_DIM"</span></span>
<span id="cb4-479"><a href="#cb4-479" aria-hidden="true" tabindex="-1"></a>                context_proj <span class="op">=</span> <span class="va">self</span>.context_proj[i](context)</span>
<span id="cb4-480"><a href="#cb4-480" aria-hidden="true" tabindex="-1"></a>                out_attn, _ <span class="op">=</span> <span class="va">self</span>.cross_attentions[i](in_attn, context_proj, context_proj)</span>
<span id="cb4-481"><a href="#cb4-481" aria-hidden="true" tabindex="-1"></a>                out_attn <span class="op">=</span> out_attn.transpose(<span class="dv">1</span>, <span class="dv">2</span>).reshape(batch_size, channels, h, w)</span>
<span id="cb4-482"><a href="#cb4-482" aria-hidden="true" tabindex="-1"></a>                out <span class="op">=</span> out <span class="op">+</span> out_attn</span>
<span id="cb4-483"><a href="#cb4-483" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-484"><a href="#cb4-484" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="vqvae-implementation" class="level3">
<h3 class="anchored" data-anchor-id="vqvae-implementation">VQVAE Implementation</h3>
<div id="cell-11" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> VQVAE(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        im_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_channels <span class="op">=</span> [<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>, <span class="dv">256</span>]</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mid_channels <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">256</span>]</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>]</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_down_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_mid_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_up_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_channels <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># To disable attention in the DownBlock of Encoder and UpBlock of Decoder</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attns <span class="op">=</span> [<span class="va">False</span>, <span class="va">False</span>, <span class="va">False</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Latent Dimension</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.z_channels <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.codebook_size <span class="op">=</span> <span class="dv">8192</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Wherever we use downsampling in encoder correspondingly use</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># upsampling in decoder</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> <span class="bu">list</span>(<span class="bu">reversed</span>(<span class="va">self</span>.down_sample))</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        <span class="co">##################### Encoder ######################</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_conv_in <span class="op">=</span> nn.Conv2d(im_channels, <span class="va">self</span>.down_channels[<span class="dv">0</span>], kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Downblock + Midblock</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_layers <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.down_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder_layers.append(DownBlock(<span class="va">self</span>.down_channels[i], <span class="va">self</span>.down_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>                                                 t_emb_dim<span class="op">=</span><span class="va">None</span>, down_sample<span class="op">=</span><span class="va">self</span>.down_sample[i],</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>                                                 num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>                                                 num_layers<span class="op">=</span><span class="va">self</span>.num_down_layers,</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>                                                 attn<span class="op">=</span><span class="va">self</span>.attns[i],</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>                                                 norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_mids <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.mid_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.encoder_mids.append(MidBlock(<span class="va">self</span>.mid_channels[i], <span class="va">self</span>.mid_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>                                              t_emb_dim<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>                                              num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>                                              num_layers<span class="op">=</span><span class="va">self</span>.num_mid_layers,</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>                                              norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_norm_out <span class="op">=</span> nn.GroupNorm(<span class="va">self</span>.norm_channels, <span class="va">self</span>.down_channels[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.encoder_conv_out <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.down_channels[<span class="op">-</span><span class="dv">1</span>], <span class="va">self</span>.z_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pre Quantization Convolution</span></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pre_quant_conv <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.z_channels, <span class="va">self</span>.z_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Codebook</span></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding <span class="op">=</span> nn.Embedding(<span class="va">self</span>.codebook_size, <span class="va">self</span>.z_channels)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">####################################################</span></span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>        <span class="co">##################### Decoder ######################</span></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Post Quantization Convolution</span></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.post_quant_conv <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.z_channels, <span class="va">self</span>.z_channels, kernel_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_conv_in <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.z_channels, <span class="va">self</span>.mid_channels[<span class="op">-</span><span class="dv">1</span>], kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Midblock + Upblock</span></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_mids <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(<span class="va">self</span>.mid_channels))):</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_mids.append(MidBlock(<span class="va">self</span>.mid_channels[i], <span class="va">self</span>.mid_channels[i <span class="op">-</span> <span class="dv">1</span>],</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>                                              t_emb_dim<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>                                              num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>                                              num_layers<span class="op">=</span><span class="va">self</span>.num_mid_layers,</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>                                              norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_layers <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">len</span>(<span class="va">self</span>.down_channels))):</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.decoder_layers.append(UpBlock(<span class="va">self</span>.down_channels[i], <span class="va">self</span>.down_channels[i <span class="op">-</span> <span class="dv">1</span>],</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>                                               t_emb_dim<span class="op">=</span><span class="va">None</span>, up_sample<span class="op">=</span><span class="va">self</span>.down_sample[i <span class="op">-</span> <span class="dv">1</span>],</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>                                               num_heads<span class="op">=</span><span class="va">self</span>.num_heads,</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>                                               num_layers<span class="op">=</span><span class="va">self</span>.num_up_layers,</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>                                               attn<span class="op">=</span><span class="va">self</span>.attns[i<span class="op">-</span><span class="dv">1</span>],</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>                                               norm_channels<span class="op">=</span><span class="va">self</span>.norm_channels))</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_norm_out <span class="op">=</span> nn.GroupNorm(<span class="va">self</span>.norm_channels, <span class="va">self</span>.down_channels[<span class="dv">0</span>])</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decoder_conv_out <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.down_channels[<span class="dv">0</span>], im_channels, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> quantize(<span class="va">self</span>, x):</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a>        B, C, H, W <span class="op">=</span> x.shape</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B, C, H, W -&gt; B, H, W, C</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B, H, W, C -&gt; B, H*W, C</span></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(x.size(<span class="dv">0</span>), <span class="op">-</span><span class="dv">1</span>, x.size(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Find nearest embedding/codebook vector</span></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        <span class="co"># dist between (B, H*W, C) and (B, K, C) -&gt; (B, H*W, K)</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        dist <span class="op">=</span> torch.cdist(x, <span class="va">self</span>.embedding.weight[<span class="va">None</span>, :].repeat((x.size(<span class="dv">0</span>), <span class="dv">1</span>, <span class="dv">1</span>)))</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>        <span class="co"># (B, H*W)</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>        min_encoding_indices <span class="op">=</span> torch.argmin(dist, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Replace encoder output with nearest codebook</span></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>        <span class="co"># quant_out -&gt; B*H*W, C</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>        quant_out <span class="op">=</span> torch.index_select(<span class="va">self</span>.embedding.weight, <span class="dv">0</span>, min_encoding_indices.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x -&gt; B*H*W, C</span></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape((<span class="op">-</span><span class="dv">1</span>, x.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a>        commmitment_loss <span class="op">=</span> torch.mean((quant_out.detach() <span class="op">-</span> x) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>        codebook_loss <span class="op">=</span> torch.mean((quant_out <span class="op">-</span> x.detach()) <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a>        quantize_losses <span class="op">=</span> {</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a>            <span class="st">'codebook_loss'</span>: codebook_loss,</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a>            <span class="st">'commitment_loss'</span>: commmitment_loss</span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Straight through estimation</span></span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a>        quant_out <span class="op">=</span> x <span class="op">+</span> (quant_out <span class="op">-</span> x).detach()</span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a>        <span class="co"># quant_out -&gt; B, C, H, W</span></span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a>        quant_out <span class="op">=</span> quant_out.reshape((B, H, W, C)).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a>        min_encoding_indices <span class="op">=</span> min_encoding_indices.reshape((<span class="op">-</span><span class="dv">1</span>, quant_out.size(<span class="op">-</span><span class="dv">2</span>), quant_out.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> quant_out, quantize_losses, min_encoding_indices</span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> encode(<span class="va">self</span>, x):</span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.encoder_conv_in(x)</span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, down <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.encoder_layers):</span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> down(out)</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mid <span class="kw">in</span> <span class="va">self</span>.encoder_mids:</span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> mid(out)</span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.encoder_norm_out(out)</span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.SiLU()(out)</span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.encoder_conv_out(out)</span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.pre_quant_conv(out)</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a>        out, quant_losses, _ <span class="op">=</span> <span class="va">self</span>.quantize(out)</span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, quant_losses</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> decode(<span class="va">self</span>, z):</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> z</span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.post_quant_conv(out)</span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decoder_conv_in(out)</span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mid <span class="kw">in</span> <span class="va">self</span>.decoder_mids:</span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> mid(out)</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, up <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.decoder_layers):</span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> up(out)</span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decoder_norm_out(out)</span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.SiLU()(out)</span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decoder_conv_out(out)</span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a>        z, quant_losses <span class="op">=</span> <span class="va">self</span>.encode(x)</span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.decode(z)</span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, z, quant_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="discriminator" class="level3">
<h3 class="anchored" data-anchor-id="discriminator">Discriminator</h3>
<div id="cell-13" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Discriminator(nn.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">    PatchGAN Discriminator.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Rather than taking IMG_CHANNELSxIMG_HxIMG_W all the way to</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co">    1 scalar value , we instead predict grid of values.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Where each grid is prediction of how likely</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    the discriminator thinks that the image patch corresponding</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    to the grid cell is real</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, im_channels<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>                 conv_channels<span class="op">=</span>[<span class="dv">64</span>, <span class="dv">128</span>, <span class="dv">256</span>],</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>                 kernels<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>],</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>                 strides<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">1</span>],</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>                 paddings<span class="op">=</span>[<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>]):</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_channels <span class="op">=</span> im_channels</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        activation <span class="op">=</span> nn.LeakyReLU(<span class="fl">0.2</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        layers_dim <span class="op">=</span> [<span class="va">self</span>.im_channels] <span class="op">+</span> conv_channels <span class="op">+</span> [<span class="dv">1</span>]</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>            nn.Sequential(</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>                nn.Conv2d(layers_dim[i], layers_dim[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>                          kernel_size<span class="op">=</span>kernels[i],</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>                          stride<span class="op">=</span>strides[i],</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>                          padding<span class="op">=</span>paddings[i],</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>                          bias<span class="op">=</span><span class="va">False</span> <span class="cf">if</span> i <span class="op">!=</span><span class="dv">0</span> <span class="cf">else</span> <span class="va">True</span>),</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>                nn.BatchNorm2d(layers_dim[i <span class="op">+</span> <span class="dv">1</span>]) <span class="cf">if</span> i <span class="op">!=</span> <span class="bu">len</span>(layers_dim) <span class="op">-</span> <span class="dv">2</span> <span class="kw">and</span> i <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> nn.Identity(),</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>                activation <span class="cf">if</span> i <span class="op">!=</span> <span class="bu">len</span>(layers_dim) <span class="op">-</span> <span class="dv">2</span> <span class="cf">else</span> nn.Identity()</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(layers_dim) <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> x</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> layer(out)</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="loading-dataset---celebahq" class="level2">
<h2 class="anchored" data-anchor-id="loading-dataset---celebahq">Loading Dataset - CelebAHQ</h2>
<div id="cell-15" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_latents(latent_path):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Simple utility to save latents to speed up ldm training</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param latent_path:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :return:</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    latent_maps <span class="op">=</span> {}</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> fname <span class="kw">in</span> glob.glob(os.path.join(latent_path, <span class="st">'*.pkl'</span>)):</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        s <span class="op">=</span> pickle.load(<span class="bu">open</span>(fname, <span class="st">'rb'</span>))</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k, v <span class="kw">in</span> s.items():</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>            latent_maps[k] <span class="op">=</span> v[<span class="dv">0</span>]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> latent_maps</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CelebDataset(Dataset):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Celeb dataset will by default centre crop and resize the images.</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    This can be replaced by any other dataset. As long as all the images</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">    are under one directory.</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, split, im_path, im_size<span class="op">=</span><span class="dv">256</span>, im_channels<span class="op">=</span><span class="dv">3</span>, im_ext<span class="op">=</span><span class="st">'jpg'</span>,</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>                 use_latents<span class="op">=</span><span class="va">False</span>, latent_path<span class="op">=</span><span class="va">None</span>, condition_config<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.split <span class="op">=</span> split</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_size <span class="op">=</span> im_size</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_channels <span class="op">=</span> im_channels</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_ext <span class="op">=</span> im_ext</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.im_path <span class="op">=</span> im_path</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.latent_maps <span class="op">=</span> <span class="va">None</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.use_latents <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.condition_types <span class="op">=</span> [] <span class="cf">if</span> condition_config <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> condition_config[<span class="st">'condition_types'</span>]</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.idx_to_cls_map <span class="op">=</span> {}</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_to_idx_map <span class="op">=</span>{}</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mask_channels <span class="op">=</span> condition_config[<span class="st">'image_condition_config'</span>][<span class="st">'image_condition_input_channels'</span>]</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mask_h <span class="op">=</span> condition_config[<span class="st">'image_condition_config'</span>][<span class="st">'image_condition_h'</span>]</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mask_w <span class="op">=</span> condition_config[<span class="st">'image_condition_config'</span>][<span class="st">'image_condition_w'</span>]</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.images, <span class="va">self</span>.texts, <span class="va">self</span>.masks <span class="op">=</span> <span class="va">self</span>.load_images(im_path)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Whether to load images or to load latents</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> use_latents <span class="kw">and</span> latent_path <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>            latent_maps <span class="op">=</span> load_latents(latent_path)</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(latent_maps) <span class="op">==</span> <span class="bu">len</span>(<span class="va">self</span>.images):</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.use_latents <span class="op">=</span> <span class="va">True</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.latent_maps <span class="op">=</span> latent_maps</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> latents'</span>.<span class="bu">format</span>(<span class="bu">len</span>(<span class="va">self</span>.latent_maps)))</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">'Latents not found'</span>)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> load_images(<span class="va">self</span>, im_path):</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="co">        Gets all images from the path specified</span></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="co">        and stacks them all up</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> os.path.exists(im_path), <span class="st">"images path </span><span class="sc">{}</span><span class="st"> does not exist"</span>.<span class="bu">format</span>(im_path)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> []</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>        fnames <span class="op">=</span> glob.glob(os.path.join(im_path, <span class="st">'CelebA-HQ-img/*.</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'png'</span>)))</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        fnames <span class="op">+=</span> glob.glob(os.path.join(im_path, <span class="st">'CelebA-HQ-img/*.</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'jpg'</span>)))</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>        fnames <span class="op">+=</span> glob.glob(os.path.join(im_path, <span class="st">'CelebA-HQ-img/*.</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(<span class="st">'jpeg'</span>)))</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>        texts <span class="op">=</span> []</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        masks <span class="op">=</span> []</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>            label_list <span class="op">=</span> [<span class="st">'skin'</span>, <span class="st">'nose'</span>, <span class="st">'eye_g'</span>, <span class="st">'l_eye'</span>, <span class="st">'r_eye'</span>, <span class="st">'l_brow'</span>, <span class="st">'r_brow'</span>, <span class="st">'l_ear'</span>, <span class="st">'r_ear'</span>, <span class="st">'mouth'</span>,</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>                          <span class="st">'u_lip'</span>, <span class="st">'l_lip'</span>, <span class="st">'hair'</span>, <span class="st">'hat'</span>, <span class="st">'ear_r'</span>, <span class="st">'neck_l'</span>, <span class="st">'neck'</span>, <span class="st">'cloth'</span>]</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.idx_to_cls_map <span class="op">=</span> {idx: label_list[idx] <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(label_list))}</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.cls_to_idx_map <span class="op">=</span> {label_list[idx]: idx <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(label_list))}</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> fname <span class="kw">in</span> tqdm(fnames):</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>            ims.append(fname)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>                im_name <span class="op">=</span> os.path.split(fname)[<span class="dv">1</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>]</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>                captions_im <span class="op">=</span> []</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>                <span class="cf">with</span> <span class="bu">open</span>(os.path.join(im_path, <span class="st">'celeba-caption/</span><span class="sc">{}</span><span class="st">.txt'</span>.<span class="bu">format</span>(im_name))) <span class="im">as</span> f:</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> line <span class="kw">in</span> f.readlines():</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>                        captions_im.append(line.strip())</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>                texts.append(captions_im)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>                im_name <span class="op">=</span> <span class="bu">int</span>(os.path.split(fname)[<span class="dv">1</span>].split(<span class="st">'.'</span>)[<span class="dv">0</span>])</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>                masks.append(os.path.join(im_path, <span class="st">'CelebAMask-HQ-mask'</span>, <span class="st">'</span><span class="sc">{}</span><span class="st">.png'</span>.<span class="bu">format</span>(im_name)))</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="bu">len</span>(texts) <span class="op">==</span> <span class="bu">len</span>(ims), <span class="st">"Condition Type Text but could not find captions for all images"</span></span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>            <span class="cf">assert</span> <span class="bu">len</span>(masks) <span class="op">==</span> <span class="bu">len</span>(ims), <span class="st">"Condition Type Image but could not find masks for all images"</span></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> images'</span>.<span class="bu">format</span>(<span class="bu">len</span>(ims)))</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> masks'</span>.<span class="bu">format</span>(<span class="bu">len</span>(masks)))</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Found </span><span class="sc">{}</span><span class="st"> captions'</span>.<span class="bu">format</span>(<span class="bu">len</span>(texts)))</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> ims, texts, masks</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_mask(<span class="va">self</span>, index):</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a><span class="co">        Method to get the mask of WxH</span></span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a><span class="co">        for given index and convert it into</span></span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a><span class="co">        Classes x W x H mask image</span></span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a><span class="co">        :param index:</span></span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a><span class="co">        :return:</span></span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>        mask_im <span class="op">=</span> Image.<span class="bu">open</span>(<span class="va">self</span>.masks[index])</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>        mask_im <span class="op">=</span> np.array(mask_im)</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>        im_base <span class="op">=</span> np.zeros((<span class="va">self</span>.mask_h, <span class="va">self</span>.mask_w, <span class="va">self</span>.mask_channels))</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> orig_idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.idx_to_cls_map)):</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>            im_base[mask_im <span class="op">==</span> (orig_idx<span class="op">+</span><span class="dv">1</span>), orig_idx] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> torch.from_numpy(im_base).permute(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mask</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.images)</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, index):</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a>        <span class="co">######## Set Conditioning Info ########</span></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>        cond_inputs <span class="op">=</span> {}</span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'text'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a>            cond_inputs[<span class="st">'text'</span>] <span class="op">=</span> random.sample(<span class="va">self</span>.texts[index], k<span class="op">=</span><span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="st">'image'</span> <span class="kw">in</span> <span class="va">self</span>.condition_types:</span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>            mask <span class="op">=</span> <span class="va">self</span>.get_mask(index)</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>            cond_inputs[<span class="st">'image'</span>] <span class="op">=</span> mask</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>        <span class="co">#######################################</span></span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.use_latents:</span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>            latent <span class="op">=</span> <span class="va">self</span>.latent_maps[<span class="va">self</span>.images[index]]</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.condition_types) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> latent</span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> latent, cond_inputs</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a>            im <span class="op">=</span> Image.<span class="bu">open</span>(<span class="va">self</span>.images[index])</span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>            im_tensor <span class="op">=</span> transforms.Compose([</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>                transforms.Resize(<span class="va">self</span>.im_size),</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a>                transforms.CenterCrop(<span class="va">self</span>.im_size),</span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>                transforms.ToTensor(),</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a>            ])(im)</span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>            im.close()</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convert input to -1 to 1 range.</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a>            im_tensor <span class="op">=</span> (<span class="dv">2</span> <span class="op">*</span> im_tensor) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">len</span>(<span class="va">self</span>.condition_types) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> im_tensor</span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> im_tensor, cond_inputs</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-16" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Parameters</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>batch_size_autoenc <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>num_epochs_autoenc <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>batch_size_ldm <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>num_epochs_ldm <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Parameters</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>nc <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">256</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w" class="level3">
<h3 class="anchored" data-anchor-id="textttim_dataset-contains-30000-images-of-shape-3-times-256-times-256-to-c-times-h-times-w"><span class="math inline">\(\texttt{im\_dataset}\)</span> contains <span class="math inline">\(30000\)</span> images of shape <span class="math inline">\((3 \times 256 \times 256) \to (C \times H \times W)\)</span></h3>
</section>
<section id="textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects" class="level3">
<h3 class="anchored" data-anchor-id="textttcelebaloader-creates-a-dataloader-of-textttbatch_size-8-i.e.-each-object-in-this-loader-is-of-shape-8-times-3-times-256-times-256-to-b-times-c-times-h-times-w-and-there-are-30000-8-3750-such-objects"><span class="math inline">\(\texttt{celebALoader}\)</span> creates a dataloader of <span class="math inline">\(\texttt{batch\_size} = 8\)</span> i.e.&nbsp;each object in this loader is of shape <span class="math inline">\((8 \times 3 \times 256 \times 256) \to (B \times C \times H \times W)\)</span> and there are <span class="math inline">\(30000 / 8 = 3750\)</span> such objects</h3>
<div id="cell-18" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>path <span class="op">=</span> <span class="st">"../CelebAMask-HQ"</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>im_dataset <span class="op">=</span> CelebDataset(split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                                im_path<span class="op">=</span>path,</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>                                im_size<span class="op">=</span>image_size,</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                                im_channels<span class="op">=</span>nc)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>celebALoader <span class="op">=</span> DataLoader(im_dataset,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>                             batch_size<span class="op">=</span>batch_size_autoenc,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>                             shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:00&lt;00:00, 3045603.78it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 30000 images
Found 0 masks
Found 0 captions</code></pre>
</div>
</div>
<div id="cell-19" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>indices <span class="op">=</span> np.random.choice(<span class="dv">30000</span>, <span class="dv">16</span>, replace <span class="op">=</span> <span class="va">False</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> torch.stack([im_dataset[i] <span class="cf">for</span> i <span class="kw">in</span> indices], dim <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> vutils.make_grid(images, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.imshow(np.transpose(grid, (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-10-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="training-of-autoencoder" class="level2">
<h2 class="anchored" data-anchor-id="training-of-autoencoder">Training of AutoEncoder</h2>
<div id="cell-21" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>lpips_model <span class="op">=</span> LPIPS().<span class="bu">eval</span>().to(device)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>discriminator <span class="op">=</span> Discriminator().to(device)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>recon_criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>disc_criterion <span class="op">=</span> nn.BCEWithLogitsLoss()</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>optimizer_d <span class="op">=</span> torch.optim.Adam(discriminator.parameters(), lr <span class="op">=</span> <span class="fl">1e-5</span>, betas <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>optimizer_g <span class="op">=</span> torch.optim.Adam(model.parameters(), lr <span class="op">=</span> <span class="fl">1e-5</span>, betas <span class="op">=</span> (<span class="fl">0.5</span>, <span class="fl">0.999</span>))</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>disc_step_start <span class="op">=</span> <span class="dv">15000</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>step_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>acc_steps <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>real_images <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>reconstructed_images <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>quantized_images <span class="op">=</span> [<span class="va">None</span>, <span class="va">None</span>]</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch_idx <span class="kw">in</span> <span class="bu">range</span>(num_epochs_autoenc):</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    recon_losses <span class="op">=</span> []</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a>    codebook_losses <span class="op">=</span> []</span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    perceptual_losses <span class="op">=</span> []</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>    disc_losses <span class="op">=</span> []</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>    gen_losses <span class="op">=</span> []</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    optimizer_d.zero_grad()</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>    optimizer_g.zero_grad()</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_idx, im <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(celebALoader)):</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        step_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> im.<span class="bu">float</span>().to(device)</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Generator</span></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>        model_output <span class="op">=</span> model(im)</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        output, z, quantize_losses <span class="op">=</span> model_output</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        recon_loss <span class="op">=</span> recon_criterion(output, im)</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a>        recon_losses.append(recon_loss.item())</span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>        recon_loss <span class="op">=</span> recon_loss <span class="op">/</span> acc_steps</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a>        g_loss <span class="op">=</span> recon_loss <span class="op">+</span> (<span class="dv">1</span> <span class="op">*</span> quantize_losses[<span class="st">"codebook_loss"</span>] <span class="op">/</span> acc_steps) <span class="op">+</span> (<span class="fl">0.2</span> <span class="op">*</span> quantize_losses[<span class="st">"commitment_loss"</span>] <span class="op">/</span> acc_steps)</span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>        codebook_losses.append(quantize_losses[<span class="st">"codebook_loss"</span>].item())</span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Adversarial Loss</span></span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step_count <span class="op">&gt;</span> disc_step_start:</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>            disc_fake_pred <span class="op">=</span> discriminator(output)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>            disc_fake_loss <span class="op">=</span> disc_criterion(disc_fake_pred, torch.ones(disc_fake_pred.shape, device <span class="op">=</span> disc_fake_pred.device))</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>            gen_losses.append(<span class="fl">0.5</span> <span class="op">*</span> disc_fake_loss.item())</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>            g_loss <span class="op">+=</span> <span class="fl">0.5</span> <span class="op">*</span> disc_fake_loss.item()</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        lpips_loss <span class="op">=</span> torch.mean(lpips_model(output, im)) <span class="op">/</span> acc_steps</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a>        perceptual_losses.append(lpips_loss.item())</span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>        g_loss <span class="op">+=</span> lpips_loss <span class="op">/</span> acc_steps</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>        losses.append(g_loss.item())</span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        g_loss.backward()</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        optimizer_g.step()</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Discriminator</span></span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step_count <span class="op">&gt;</span> disc_step_start:</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>            fake <span class="op">=</span> output</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>            disc_fake_pred <span class="op">=</span> discriminator(fake.detach())</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>            disc_real_pred <span class="op">=</span> discriminator(im)</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a>            disc_fake_loss <span class="op">=</span> disc_criterion(disc_fake_pred, torch.zeros(disc_fake_pred.shape, device <span class="op">=</span> disc_fake_pred.device))</span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a>            disc_real_loss <span class="op">=</span> disc_criterion(disc_real_pred, torch.ones(disc_real_pred.shape, device <span class="op">=</span> disc_real_pred.device))</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>            disc_loss <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (disc_fake_loss <span class="op">+</span> disc_real_loss) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>            disc_losses.append(disc_loss.item())</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>            disc_loss <span class="op">=</span> disc_loss <span class="op">/</span> acc_steps</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a>            disc_loss.backward()</span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> step_count <span class="op">%</span> acc_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>                optimizer_d.step()</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>                optimizer_d.zero_grad()</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> step_count <span class="op">%</span> acc_steps <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a>            optimizer_g.step()</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>            optimizer_g.zero_grad()</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        idx <span class="op">=</span> batch_idx <span class="op">%</span> <span class="dv">2</span></span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        real_images[idx] <span class="op">=</span> im</span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>        reconstructed_images[idx] <span class="op">=</span> output</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>        quantized_images[idx] <span class="op">=</span> z</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>    optimizer_d.step()</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>    optimizer_d.zero_grad()</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>    optimizer_g.step()</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>    optimizer_g.zero_grad()</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(disc_losses) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Finished epoch: </span><span class="sc">{}</span><span class="st"> | Recon Loss : </span><span class="sc">{:.4f}</span><span class="st"> | Perceptual Loss : </span><span class="sc">{:.4f}</span><span class="st"> | '</span></span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Codebook : </span><span class="sc">{:.4f}</span><span class="st"> | G Loss : </span><span class="sc">{:.4f}</span><span class="st"> | D Loss </span><span class="sc">{:.4f}</span><span class="st">'</span>.</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>            <span class="bu">format</span>(epoch_idx <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>                    np.mean(recon_losses),</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>                    np.mean(perceptual_losses),</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>                    np.mean(codebook_losses),</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>                    np.mean(gen_losses),</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>                    np.mean(disc_losses)))</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">'Finished epoch: </span><span class="sc">{}</span><span class="st"> | Recon Loss : </span><span class="sc">{:.4f}</span><span class="st"> | Perceptual Loss : </span><span class="sc">{:.4f}</span><span class="st"> | Codebook : </span><span class="sc">{:.4f}</span><span class="st">'</span>.</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>                <span class="bu">format</span>(epoch_idx <span class="op">+</span> <span class="dv">1</span>,</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>                        np.mean(recon_losses),</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a>                        np.mean(perceptual_losses),</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a>                        np.mean(codebook_losses)))</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), <span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>)</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>    torch.save(discriminator.state_dict(), <span class="st">"../vqvaeCeleb/vqvae_discriminator.pth"</span>)</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>        real_images1 <span class="op">=</span> torch.cat(real_images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>        reconstructed_images1 <span class="op">=</span> torch.cat(reconstructed_images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>        quantized_images1 <span class="op">=</span> torch.cat(quantized_images, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Each of these lists have 2 tensors of shape (8, 3, 256, 256)</span></span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># So stacking them gives (16, 3, 256, 256)</span></span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and we plot it in a grid of 4x4</span></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>        grid_real <span class="op">=</span> vutils.make_grid(real_images1, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a>        grid_reconstructed <span class="op">=</span> vutils.make_grid(reconstructed_images1, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-122"><a href="#cb13-122" aria-hidden="true" tabindex="-1"></a>        grid_quantized <span class="op">=</span> vutils.make_grid(quantized_images1, nrow <span class="op">=</span> <span class="dv">4</span>, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb13-123"><a href="#cb13-123" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-124"><a href="#cb13-124" aria-hidden="true" tabindex="-1"></a>        plt.figure(figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb13-125"><a href="#cb13-125" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">1</span>)</span>
<span id="cb13-126"><a href="#cb13-126" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb13-127"><a href="#cb13-127" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Real Images"</span>)</span>
<span id="cb13-128"><a href="#cb13-128" aria-hidden="true" tabindex="-1"></a>        plt.imshow(np.transpose(grid_real.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb13-129"><a href="#cb13-129" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-130"><a href="#cb13-130" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)</span>
<span id="cb13-131"><a href="#cb13-131" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb13-132"><a href="#cb13-132" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Reconstructed Images"</span>)</span>
<span id="cb13-133"><a href="#cb13-133" aria-hidden="true" tabindex="-1"></a>        plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb13-134"><a href="#cb13-134" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-135"><a href="#cb13-135" aria-hidden="true" tabindex="-1"></a>        plt.subplot(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">3</span>)</span>
<span id="cb13-136"><a href="#cb13-136" aria-hidden="true" tabindex="-1"></a>        plt.axis(<span class="st">"off"</span>)</span>
<span id="cb13-137"><a href="#cb13-137" aria-hidden="true" tabindex="-1"></a>        plt.title(<span class="st">"Quantized Images"</span>)</span>
<span id="cb13-138"><a href="#cb13-138" aria-hidden="true" tabindex="-1"></a>        plt.imshow(np.transpose(grid_quantized.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb13-139"><a href="#cb13-139" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb13-140"><a href="#cb13-140" aria-hidden="true" tabindex="-1"></a>        plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/home/cvig/miniconda3/envs/PyTorchG/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/cvig/miniconda3/envs/PyTorchG/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:43&lt;00:00,  3.74it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:46&lt;00:00,  3.72it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:39&lt;00:00,  3.75it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [16:41&lt;00:00,  3.74it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [23:59&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:01&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:04&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:01&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:04&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:01&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:03&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:02&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:00&lt;00:00,  2.60it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3750/3750 [24:03&lt;00:00,  2.60it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Finished epoch: 1 | Recon Loss : 0.0444 | Perceptual Loss : -0.0079 | Codebook : 0.0024
Finished epoch: 2 | Recon Loss : 0.0274 | Perceptual Loss : -0.0099 | Codebook : 0.0012
Finished epoch: 3 | Recon Loss : 0.0187 | Perceptual Loss : -0.0102 | Codebook : 0.0016
Finished epoch: 4 | Recon Loss : 0.0153 | Perceptual Loss : -0.0104 | Codebook : 0.0013
Finished epoch: 5 | Recon Loss : 0.0135 | Perceptual Loss : -0.0107 | Codebook : 0.0011 | G Loss : 0.9127 | D Loss 0.1442
Finished epoch: 6 | Recon Loss : 0.0126 | Perceptual Loss : -0.0110 | Codebook : 0.0009 | G Loss : 1.9146 | D Loss 0.0166
Finished epoch: 7 | Recon Loss : 0.0119 | Perceptual Loss : -0.0113 | Codebook : 0.0008 | G Loss : 2.4890 | D Loss 0.0052
Finished epoch: 8 | Recon Loss : 0.0117 | Perceptual Loss : -0.0118 | Codebook : 0.0008 | G Loss : 2.8359 | D Loss 0.0040
Finished epoch: 9 | Recon Loss : 0.0111 | Perceptual Loss : -0.0124 | Codebook : 0.0008 | G Loss : 3.0382 | D Loss 0.0018
Finished epoch: 10 | Recon Loss : 0.0108 | Perceptual Loss : -0.0131 | Codebook : 0.0007 | G Loss : 3.1862 | D Loss 0.0014
Finished epoch: 11 | Recon Loss : 0.0105 | Perceptual Loss : -0.0139 | Codebook : 0.0007 | G Loss : 3.3179 | D Loss 0.0012
Finished epoch: 12 | Recon Loss : 0.0102 | Perceptual Loss : -0.0146 | Codebook : 0.0007 | G Loss : 3.5116 | D Loss 0.0008
Finished epoch: 13 | Recon Loss : 0.0100 | Perceptual Loss : -0.0150 | Codebook : 0.0007 | G Loss : 3.7793 | D Loss 0.0005
Finished epoch: 14 | Recon Loss : 0.0098 | Perceptual Loss : -0.0153 | Codebook : 0.0007 | G Loss : 3.9513 | D Loss 0.0041
Finished epoch: 15 | Recon Loss : 0.0096 | Perceptual Loss : -0.0156 | Codebook : 0.0006 | G Loss : 3.6160 | D Loss 0.0009
Finished epoch: 16 | Recon Loss : 0.0095 | Perceptual Loss : -0.0158 | Codebook : 0.0006 | G Loss : 3.9615 | D Loss 0.0004
Finished epoch: 17 | Recon Loss : 0.0094 | Perceptual Loss : -0.0160 | Codebook : 0.0006 | G Loss : 4.0736 | D Loss 0.0003
Finished epoch: 18 | Recon Loss : 0.0093 | Perceptual Loss : -0.0161 | Codebook : 0.0006 | G Loss : 4.1628 | D Loss 0.0003
Finished epoch: 19 | Recon Loss : 0.0091 | Perceptual Loss : -0.0162 | Codebook : 0.0006 | G Loss : 4.2656 | D Loss 0.0002
Finished epoch: 20 | Recon Loss : 0.0090 | Perceptual Loss : -0.0164 | Codebook : 0.0006 | G Loss : 4.3928 | D Loss 0.0002</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-12.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-13.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-14.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-15.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-16.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-17.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-18.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-19.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-20.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-21.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-11-output-22.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="infer-from-vqvae-to-get-latents" class="level1">
<h1>Infer from VQVAE to get latents</h1>
<div id="cell-23" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>im_dataset <span class="op">=</span> CelebDataset(split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>                                im_path<span class="op">=</span>path,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                                im_size<span class="op">=</span>image_size,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>                                im_channels<span class="op">=</span>nc)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(im_dataset,</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>                            batch_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                            shuffle<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>num_images <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>ngrid <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>idxs <span class="op">=</span> torch.randint(<span class="dv">0</span>, <span class="bu">len</span>(im_dataset) <span class="op">-</span> <span class="dv">1</span>, (num_images,))</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>ims <span class="op">=</span> torch.cat([im_dataset[idx][<span class="va">None</span>, :] <span class="cf">for</span> idx <span class="kw">in</span> idxs]).<span class="bu">float</span>()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>ims <span class="op">=</span> ims.to(device)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(<span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>, map_location <span class="op">=</span> device))</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    encoded_output, _ <span class="op">=</span> model.encode(ims)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    decoded_output <span class="op">=</span> model.decode(encoded_output)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    encoded_output <span class="op">=</span> torch.clamp(encoded_output, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    encoded_output <span class="op">=</span> (encoded_output <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    decoded_output <span class="op">=</span> torch.clamp(decoded_output, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>    decoded_output <span class="op">=</span> (decoded_output <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>    ims <span class="op">=</span> (ims <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    encoder_grid <span class="op">=</span> vutils.make_grid(encoded_output.cpu(), nrow<span class="op">=</span>ngrid)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a>    decoder_grid <span class="op">=</span> vutils.make_grid(decoded_output.cpu(), nrow<span class="op">=</span>ngrid)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    input_grid <span class="op">=</span> vutils.make_grid(ims.cpu(), nrow<span class="op">=</span>ngrid)</span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    encoder_grid <span class="op">=</span> transforms.ToPILImage()(encoder_grid)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    decoder_grid <span class="op">=</span> transforms.ToPILImage()(decoder_grid)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    input_grid <span class="op">=</span> transforms.ToPILImage()(input_grid)</span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>    input_grid.save(<span class="st">'../CelebAHQ/input_samples.png'</span>)</span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>    encoder_grid.save(<span class="st">'../CelebAHQ/encoded_samples.png'</span>)</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    decoder_grid.save(<span class="st">"../CelebAHQ/reconstructed_samples.png"</span>)</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    latent_path <span class="op">=</span> <span class="st">"../vqvaelatents"</span></span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    latent_fnames <span class="op">=</span> glob.glob(os.path.join(<span class="st">"../vqvaelatents"</span>, <span class="st">'*.pkl'</span>))</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(latent_fnames) <span class="op">==</span> <span class="dv">0</span>, <span class="st">'Latents already present. Delete all latent files and re-run'</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> os.path.exists(latent_path):</span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        os.mkdir(latent_path)</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    fname_latent_map <span class="op">=</span> {}</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    part_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>    count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> idx, im <span class="kw">in</span> <span class="bu">enumerate</span>(tqdm(data_loader)):</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        encoded_output, _ <span class="op">=</span> model.encode(im.<span class="bu">float</span>().to(device))</span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        fname_latent_map[im_dataset.images[idx]] <span class="op">=</span> encoded_output.cpu()</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save latents every 1000 images</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (count<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> <span class="dv">1000</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>            pickle.dump(fname_latent_map, <span class="bu">open</span>(os.path.join(latent_path,</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>                                                            <span class="st">'</span><span class="sc">{}</span><span class="st">.pkl'</span>.<span class="bu">format</span>(part_count)), <span class="st">'wb'</span>))</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>            part_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>            fname_latent_map <span class="op">=</span> {}</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>        count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(fname_latent_map) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>        pickle.dump(fname_latent_map, <span class="bu">open</span>(os.path.join(latent_path,</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>                                            <span class="st">'</span><span class="sc">{}</span><span class="st">.pkl'</span>.<span class="bu">format</span>(part_count)), <span class="st">'wb'</span>))</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Done saving latents"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:00&lt;00:00, 6612145.03it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [10:07&lt;00:00, 49.35it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 30000 images
Found 0 masks
Found 0 captions
Done saving latents</code></pre>
</div>
</div>
<section id="latent-diffusion-model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="latent-diffusion-model-architecture">Latent Diffusion Model Architecture</h2>
<section id="unet" class="level3">
<h3 class="anchored" data-anchor-id="unet">UNet</h3>
<div id="cell-25" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Unet(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, im_channels <span class="op">=</span> <span class="dv">3</span>):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_channels <span class="op">=</span> [<span class="dv">256</span>, <span class="dv">384</span>, <span class="dv">512</span>, <span class="dv">768</span>]</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mid_channels <span class="op">=</span> [<span class="dv">768</span>, <span class="dv">512</span>]</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_emb_dim <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down_sample <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>]</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_down_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_mid_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_up_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attns <span class="op">=</span> [<span class="va">True</span>, <span class="va">True</span>, <span class="va">True</span>]</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_channels <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_out_channels <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initial projection from sinusoidal time embedding</span></span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.t_proj <span class="op">=</span> nn.Sequential(</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.t_emb_dim, <span class="va">self</span>.t_emb_dim),</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>            nn.SiLU(),</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="va">self</span>.t_emb_dim, <span class="va">self</span>.t_emb_dim)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.up_sample <span class="op">=</span> <span class="bu">list</span>(<span class="bu">reversed</span>(<span class="va">self</span>.down_sample))</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_in <span class="op">=</span> nn.Conv2d(im_channels, <span class="va">self</span>.down_channels[<span class="dv">0</span>], kernel_size <span class="op">=</span> <span class="dv">3</span>, padding <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downs <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.down_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.downs.append(DownBlock(<span class="va">self</span>.down_channels[i], <span class="va">self</span>.down_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>                                       t_emb_dim <span class="op">=</span> <span class="va">self</span>.t_emb_dim, down_sample <span class="op">=</span> <span class="va">self</span>.down_sample[i],</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>                                       num_heads <span class="op">=</span> <span class="va">self</span>.num_heads, num_layers <span class="op">=</span> <span class="va">self</span>.num_down_layers,</span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a>                                       attn <span class="op">=</span> <span class="va">self</span>.attns[i], norm_channels <span class="op">=</span> <span class="va">self</span>.norm_channels))</span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-34"><a href="#cb19-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mids <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb19-35"><a href="#cb19-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.mid_channels) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb19-36"><a href="#cb19-36" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.mids.append(MidBlock(<span class="va">self</span>.mid_channels[i], <span class="va">self</span>.mid_channels[i <span class="op">+</span> <span class="dv">1</span>],</span>
<span id="cb19-37"><a href="#cb19-37" aria-hidden="true" tabindex="-1"></a>                                    t_emb_dim <span class="op">=</span> <span class="va">self</span>.t_emb_dim, num_heads <span class="op">=</span> <span class="va">self</span>.num_heads,</span>
<span id="cb19-38"><a href="#cb19-38" aria-hidden="true" tabindex="-1"></a>                                    num_layers <span class="op">=</span> <span class="va">self</span>.num_mid_layers, norm_channels <span class="op">=</span> <span class="va">self</span>.norm_channels))</span>
<span id="cb19-39"><a href="#cb19-39" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-40"><a href="#cb19-40" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ups <span class="op">=</span> nn.ModuleList([])</span>
<span id="cb19-41"><a href="#cb19-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(<span class="bu">len</span>(<span class="va">self</span>.down_channels) <span class="op">-</span> <span class="dv">1</span>)):</span>
<span id="cb19-42"><a href="#cb19-42" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.ups.append(UpBlockUnet(<span class="va">self</span>.down_channels[i] <span class="op">*</span> <span class="dv">2</span>, <span class="va">self</span>.down_channels[i <span class="op">-</span> <span class="dv">1</span>] <span class="cf">if</span> i <span class="op">!=</span> <span class="dv">0</span> <span class="cf">else</span> <span class="va">self</span>.conv_out_channels,</span>
<span id="cb19-43"><a href="#cb19-43" aria-hidden="true" tabindex="-1"></a>                                    <span class="va">self</span>.t_emb_dim, up_sample <span class="op">=</span> <span class="va">self</span>.down_sample[i],</span>
<span id="cb19-44"><a href="#cb19-44" aria-hidden="true" tabindex="-1"></a>                                        num_heads <span class="op">=</span> <span class="va">self</span>.num_heads,</span>
<span id="cb19-45"><a href="#cb19-45" aria-hidden="true" tabindex="-1"></a>                                        num_layers <span class="op">=</span> <span class="va">self</span>.num_up_layers,</span>
<span id="cb19-46"><a href="#cb19-46" aria-hidden="true" tabindex="-1"></a>                                        norm_channels <span class="op">=</span> <span class="va">self</span>.norm_channels))</span>
<span id="cb19-47"><a href="#cb19-47" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-48"><a href="#cb19-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm_out <span class="op">=</span> nn.GroupNorm(<span class="va">self</span>.norm_channels, <span class="va">self</span>.conv_out_channels)</span>
<span id="cb19-49"><a href="#cb19-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_out <span class="op">=</span> nn.Conv2d(<span class="va">self</span>.conv_out_channels, im_channels, kernel_size <span class="op">=</span> <span class="dv">3</span>, padding <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb19-50"><a href="#cb19-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-51"><a href="#cb19-51" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, t):</span>
<span id="cb19-52"><a href="#cb19-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shapes assuming downblocks are [C1, C2, C3, C4]</span></span>
<span id="cb19-53"><a href="#cb19-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shapes assuming midblocks are [C4, C4, C3]</span></span>
<span id="cb19-54"><a href="#cb19-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shapes assuming downsamples are [True, True, False]</span></span>
<span id="cb19-55"><a href="#cb19-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B x C x H x W</span></span>
<span id="cb19-56"><a href="#cb19-56" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_in(x)</span>
<span id="cb19-57"><a href="#cb19-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># B x C1 x H x W</span></span>
<span id="cb19-58"><a href="#cb19-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-59"><a href="#cb19-59" aria-hidden="true" tabindex="-1"></a>        <span class="co"># t_emb -&gt; B x t_emb_dim</span></span>
<span id="cb19-60"><a href="#cb19-60" aria-hidden="true" tabindex="-1"></a>        t_emb <span class="op">=</span> get_time_embedding(torch.as_tensor(t).<span class="bu">long</span>(), <span class="va">self</span>.t_emb_dim)</span>
<span id="cb19-61"><a href="#cb19-61" aria-hidden="true" tabindex="-1"></a>        t_emb <span class="op">=</span> <span class="va">self</span>.t_proj(t_emb)</span>
<span id="cb19-62"><a href="#cb19-62" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-63"><a href="#cb19-63" aria-hidden="true" tabindex="-1"></a>        down_outs <span class="op">=</span> []</span>
<span id="cb19-64"><a href="#cb19-64" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-65"><a href="#cb19-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, down <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="va">self</span>.downs):</span>
<span id="cb19-66"><a href="#cb19-66" aria-hidden="true" tabindex="-1"></a>            down_outs.append(out)</span>
<span id="cb19-67"><a href="#cb19-67" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> down(out, t_emb)</span>
<span id="cb19-68"><a href="#cb19-68" aria-hidden="true" tabindex="-1"></a>        <span class="co"># down_outs  [B x C1 x H x W, B x C2 x H/2 x W/2, B x C3 x H/4 x W/4]</span></span>
<span id="cb19-69"><a href="#cb19-69" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out B x C4 x H/4 x W/4</span></span>
<span id="cb19-70"><a href="#cb19-70" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-71"><a href="#cb19-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mid <span class="kw">in</span> <span class="va">self</span>.mids:</span>
<span id="cb19-72"><a href="#cb19-72" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> mid(out, t_emb)</span>
<span id="cb19-73"><a href="#cb19-73" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out B x C3 x H/4 x W/4</span></span>
<span id="cb19-74"><a href="#cb19-74" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-75"><a href="#cb19-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> up <span class="kw">in</span> <span class="va">self</span>.ups:</span>
<span id="cb19-76"><a href="#cb19-76" aria-hidden="true" tabindex="-1"></a>            down_out <span class="op">=</span> down_outs.pop()</span>
<span id="cb19-77"><a href="#cb19-77" aria-hidden="true" tabindex="-1"></a>            out <span class="op">=</span> up(out, down_out, t_emb)</span>
<span id="cb19-78"><a href="#cb19-78" aria-hidden="true" tabindex="-1"></a>            <span class="co"># out [B x C2 x H/4 x W/4, B x C1 x H/2 x W/2, B x 16 x H x W]</span></span>
<span id="cb19-79"><a href="#cb19-79" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.norm_out(out)</span>
<span id="cb19-80"><a href="#cb19-80" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> nn.SiLU()(out)</span>
<span id="cb19-81"><a href="#cb19-81" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.conv_out(out)</span>
<span id="cb19-82"><a href="#cb19-82" aria-hidden="true" tabindex="-1"></a>        <span class="co"># out B x C x H x W</span></span>
<span id="cb19-83"><a href="#cb19-83" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="linear-noise-scheduler" class="level3">
<h3 class="anchored" data-anchor-id="linear-noise-scheduler">Linear Noise Scheduler</h3>
<div id="cell-27" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinearNoiseScheduler:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">r"""</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Class for the linear noise scheduler that is used in DDPM.</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_timesteps, beta_start, beta_end):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_timesteps <span class="op">=</span> num_timesteps</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta_start <span class="op">=</span> beta_start</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta_end <span class="op">=</span> beta_end</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Mimicking how compvis repo creates schedule</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> (</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>                torch.linspace(beta_start <span class="op">**</span> <span class="fl">0.5</span>, beta_end <span class="op">**</span> <span class="fl">0.5</span>, num_timesteps) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> <span class="fl">1.</span> <span class="op">-</span> <span class="va">self</span>.betas</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha_cum_prod <span class="op">=</span> torch.cumprod(<span class="va">self</span>.alphas, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqrt_alpha_cum_prod <span class="op">=</span> torch.sqrt(<span class="va">self</span>.alpha_cum_prod)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sqrt_one_minus_alpha_cum_prod <span class="op">=</span> torch.sqrt(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha_cum_prod)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> add_noise(<span class="va">self</span>, original, noise, t):</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="co">        Forward method for diffusion</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co">        :param original: Image on which noise is to be applied</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co">        :param noise: Random Noise Tensor (from normal dist)</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a><span class="co">        :param t: timestep of the forward process of shape -&gt; (B,)</span></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co">        :return:</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        original_shape <span class="op">=</span> original.shape</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        batch_size <span class="op">=</span> original_shape[<span class="dv">0</span>]</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        sqrt_alpha_cum_prod <span class="op">=</span> <span class="va">self</span>.sqrt_alpha_cum_prod.to(original.device)[t].reshape(batch_size)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        sqrt_one_minus_alpha_cum_prod <span class="op">=</span> <span class="va">self</span>.sqrt_one_minus_alpha_cum_prod.to(original.device)[t].reshape(batch_size)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Reshape till (B,) becomes (B,1,1,1) if image is (B,C,H,W)</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(original_shape) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>            sqrt_alpha_cum_prod <span class="op">=</span> sqrt_alpha_cum_prod.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(original_shape) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>            sqrt_one_minus_alpha_cum_prod <span class="op">=</span> sqrt_one_minus_alpha_cum_prod.unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Apply and Return Forward process equation</span></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (sqrt_alpha_cum_prod.to(original.device) <span class="op">*</span> original</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>                <span class="op">+</span> sqrt_one_minus_alpha_cum_prod.to(original.device) <span class="op">*</span> noise)</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample_prev_timestep(<span class="va">self</span>, xt, noise_pred, t):</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        <span class="co">r"""</span></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a><span class="co">            Use the noise prediction by model to get</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="co">            xt-1 using xt and the nosie predicted</span></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="co">        :param xt: current timestep sample</span></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a><span class="co">        :param noise_pred: model noise prediction</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a><span class="co">        :param t: current timestep we are at</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a><span class="co">        :return:</span></span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> ((xt <span class="op">-</span> (<span class="va">self</span>.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t] <span class="op">*</span> noise_pred)) <span class="op">/</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>              torch.sqrt(<span class="va">self</span>.alpha_cum_prod.to(xt.device)[t]))</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>        x0 <span class="op">=</span> torch.clamp(x0, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> xt <span class="op">-</span> ((<span class="va">self</span>.betas.to(xt.device)[t]) <span class="op">*</span> noise_pred) <span class="op">/</span> (<span class="va">self</span>.sqrt_one_minus_alpha_cum_prod.to(xt.device)[t])</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>        mean <span class="op">=</span> mean <span class="op">/</span> torch.sqrt(<span class="va">self</span>.alphas.to(xt.device)[t])</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> t <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> mean, x0</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>            variance <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha_cum_prod.to(xt.device)[t <span class="op">-</span> <span class="dv">1</span>]) <span class="op">/</span> (<span class="fl">1.0</span> <span class="op">-</span> <span class="va">self</span>.alpha_cum_prod.to(xt.device)[t])</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>            variance <span class="op">=</span> variance <span class="op">*</span> <span class="va">self</span>.betas.to(xt.device)[t]</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>            sigma <span class="op">=</span> variance <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>            z <span class="op">=</span> torch.randn(xt.shape).to(xt.device)</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> mean <span class="op">+</span> sigma <span class="op">*</span> z, x0</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="latent-diffusion-model-training" class="level2">
<h2 class="anchored" data-anchor-id="latent-diffusion-model-training">Latent Diffusion Model Training</h2>
<div id="cell-29" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Parameters</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Diffusion Parameters</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>beta_start <span class="op">=</span> <span class="fl">1e-4</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>beta_end <span class="op">=</span> <span class="fl">2e-2</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Model Parameters</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>nc <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>image_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>z_channels <span class="op">=</span> <span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="use-latents-for-setting-up-the-data" class="level2">
<h2 class="anchored" data-anchor-id="use-latents-for-setting-up-the-data">Use Latents for setting up the data</h2>
<div id="cell-31" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>im_dataset <span class="op">=</span> CelebDataset(split<span class="op">=</span><span class="st">'train'</span>,</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>                                im_path<span class="op">=</span>path,</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>                                im_size<span class="op">=</span>image_size,</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>                                im_channels<span class="op">=</span>nc,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                                use_latents<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>                                latent_path<span class="op">=</span><span class="st">"../vqvaelatents"</span></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>                                )</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>celebALoader <span class="op">=</span> DataLoader(im_dataset,</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                            batch_size<span class="op">=</span>batch_size_ldm,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                            shuffle<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30000/30000 [00:00&lt;00:00, 1944027.44it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Found 30000 images
Found 0 masks
Found 0 captions
Found 30000 latents</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> LinearNoiseScheduler(num_timesteps <span class="op">=</span> T, beta_start <span class="op">=</span> beta_start, beta_end <span class="op">=</span> beta_end)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Unet(im_channels <span class="op">=</span> z_channels).to(device)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>model.train()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> im_dataset.use_latents:</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    vae <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    vae.<span class="bu">eval</span>()</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    vae.load_state_dict(torch.load(<span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>, map_location <span class="op">=</span> device))</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="kw">not</span> im_dataset.use_latents:</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param <span class="kw">in</span> vae.parameters():</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>        param.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters(), lr <span class="op">=</span> lr)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.MSELoss()</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch_idx <span class="kw">in</span> <span class="bu">range</span>(num_epochs_ldm):</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> im <span class="kw">in</span> tqdm(celebALoader):</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>        im <span class="op">=</span> im.<span class="bu">float</span>().to(device)</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-24"><a href="#cb25-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># THE MAIN PART -&gt; LATENT SPACE TRAINING</span></span>
<span id="cb25-25"><a href="#cb25-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> im_dataset.use_latents:</span>
<span id="cb25-26"><a href="#cb25-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb25-27"><a href="#cb25-27" aria-hidden="true" tabindex="-1"></a>                im, _ <span class="op">=</span> vae.encode(im)</span>
<span id="cb25-28"><a href="#cb25-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-29"><a href="#cb25-29" aria-hidden="true" tabindex="-1"></a>        noise <span class="op">=</span> torch.randn_like(im).to(device)</span>
<span id="cb25-30"><a href="#cb25-30" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> torch.randint(<span class="dv">0</span>, T, (im.shape[<span class="dv">0</span>],)).to(device)</span>
<span id="cb25-31"><a href="#cb25-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-32"><a href="#cb25-32" aria-hidden="true" tabindex="-1"></a>        noisy_im <span class="op">=</span> scheduler.add_noise(im, noise, t)</span>
<span id="cb25-33"><a href="#cb25-33" aria-hidden="true" tabindex="-1"></a>        noise_pred <span class="op">=</span> model(noisy_im, t)</span>
<span id="cb25-34"><a href="#cb25-34" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-35"><a href="#cb25-35" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(noise_pred, noise)</span>
<span id="cb25-36"><a href="#cb25-36" aria-hidden="true" tabindex="-1"></a>        losses.append(loss.item())</span>
<span id="cb25-37"><a href="#cb25-37" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb25-38"><a href="#cb25-38" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb25-39"><a href="#cb25-39" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-40"><a href="#cb25-40" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epoch [</span><span class="sc">{</span>epoch_idx <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>num_epochs_ldm<span class="sc">}</span><span class="ss">] | Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(losses)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-41"><a href="#cb25-41" aria-hidden="true" tabindex="-1"></a>    torch.save(model.state_dict(), <span class="st">"../ldmCeleb/denoiseLatentModelCeleb.pth"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:16&lt;00:00,  7.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:16&lt;00:00,  7.31it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:16&lt;00:00,  7.32it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.26it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:17&lt;00:00,  7.27it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1875/1875 [04:18&lt;00:00,  7.27it/s]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch [1/100] | Loss: 0.17239664629101753
Epoch [2/100] | Loss: 0.13481004771987598
Epoch [3/100] | Loss: 0.12825398214956124
Epoch [4/100] | Loss: 0.12480706822971503
Epoch [5/100] | Loss: 0.12244796647131442
Epoch [6/100] | Loss: 0.12263716047604879
Epoch [7/100] | Loss: 0.11959034487803777
Epoch [8/100] | Loss: 0.12007127964595954
Epoch [9/100] | Loss: 0.11737980163097382
Epoch [10/100] | Loss: 0.11774407567977906
Epoch [11/100] | Loss: 0.1164932386537393
Epoch [12/100] | Loss: 0.11754837945401668
Epoch [13/100] | Loss: 0.11633194713791212
Epoch [14/100] | Loss: 0.11751290398438771
Epoch [15/100] | Loss: 0.1151164310703675
Epoch [16/100] | Loss: 0.11360364832480749
Epoch [17/100] | Loss: 0.11501107030709584
Epoch [18/100] | Loss: 0.11372175222436587
Epoch [19/100] | Loss: 0.11383305485347907
Epoch [20/100] | Loss: 0.1133527055879434
Epoch [21/100] | Loss: 0.11380577364961306
Epoch [22/100] | Loss: 0.11273235224882762
Epoch [23/100] | Loss: 0.1135343521485726
Epoch [24/100] | Loss: 0.1123530367632707
Epoch [25/100] | Loss: 0.1115516833047072
Epoch [26/100] | Loss: 0.11164849996964137
Epoch [27/100] | Loss: 0.11084560413658619
Epoch [28/100] | Loss: 0.11147701257665953
Epoch [29/100] | Loss: 0.1116309017131726
Epoch [30/100] | Loss: 0.11129296476145585
Epoch [31/100] | Loss: 0.11189121434191862
Epoch [32/100] | Loss: 0.11070120003223419
Epoch [33/100] | Loss: 0.11032614000439644
Epoch [34/100] | Loss: 0.10985745530923208
Epoch [35/100] | Loss: 0.11104562021692593
Epoch [36/100] | Loss: 0.11127305963635445
Epoch [37/100] | Loss: 0.10923469912409782
Epoch [38/100] | Loss: 0.11093338783582052
Epoch [39/100] | Loss: 0.11022963825563589
Epoch [40/100] | Loss: 0.10861355056762695
Epoch [41/100] | Loss: 0.11006656314432621
Epoch [42/100] | Loss: 0.11026263686021169
Epoch [43/100] | Loss: 0.10791801128884157
Epoch [44/100] | Loss: 0.1098509761840105
Epoch [45/100] | Loss: 0.11083510490258534
Epoch [46/100] | Loss: 0.10826958584785461
Epoch [47/100] | Loss: 0.11006076967120171
Epoch [48/100] | Loss: 0.10849106203317642
Epoch [49/100] | Loss: 0.10978733394245306
Epoch [50/100] | Loss: 0.10951394220292568
Epoch [51/100] | Loss: 0.10842547579109668
Epoch [52/100] | Loss: 0.10871950341761112
Epoch [53/100] | Loss: 0.10760296205282212
Epoch [54/100] | Loss: 0.10936867837011814
Epoch [55/100] | Loss: 0.1076629061371088
Epoch [56/100] | Loss: 0.108080473741889
Epoch [57/100] | Loss: 0.10893807614843051
Epoch [58/100] | Loss: 0.10773360221982002
Epoch [59/100] | Loss: 0.10701240785022577
Epoch [60/100] | Loss: 0.10817974474231402
Epoch [61/100] | Loss: 0.1084976889014244
Epoch [62/100] | Loss: 0.10656732607583205
Epoch [63/100] | Loss: 0.10788566062649091
Epoch [64/100] | Loss: 0.10937032189965248
Epoch [65/100] | Loss: 0.10807198148171107
Epoch [66/100] | Loss: 0.10871618385712306
Epoch [67/100] | Loss: 0.10831890054742495
Epoch [68/100] | Loss: 0.1058956669057409
Epoch [69/100] | Loss: 0.10795067014495532
Epoch [70/100] | Loss: 0.10718891451358795
Epoch [71/100] | Loss: 0.10608673804104328
Epoch [72/100] | Loss: 0.10626225626369318
Epoch [73/100] | Loss: 0.10670563013056913
Epoch [74/100] | Loss: 0.10741446313162645
Epoch [75/100] | Loss: 0.10694800455967585
Epoch [76/100] | Loss: 0.10626805338263512
Epoch [77/100] | Loss: 0.10666371670762698
Epoch [78/100] | Loss: 0.10609417064587275
Epoch [79/100] | Loss: 0.10631756011446317
Epoch [80/100] | Loss: 0.106819432669878
Epoch [81/100] | Loss: 0.10773934854666392
Epoch [82/100] | Loss: 0.10610651188592116
Epoch [83/100] | Loss: 0.10720135339101156
Epoch [84/100] | Loss: 0.10536787053346634
Epoch [85/100] | Loss: 0.10579571547110875
Epoch [86/100] | Loss: 0.1059078903088967
Epoch [87/100] | Loss: 0.10642880010406176
Epoch [88/100] | Loss: 0.10592716257870197
Epoch [89/100] | Loss: 0.10538040651679038
Epoch [90/100] | Loss: 0.10482307714124521
Epoch [91/100] | Loss: 0.106354721408089
Epoch [92/100] | Loss: 0.10680986197392146
Epoch [93/100] | Loss: 0.10599970742166043
Epoch [94/100] | Loss: 0.1052045267522335
Epoch [95/100] | Loss: 0.10490048675835133
Epoch [96/100] | Loss: 0.10531028269926707
Epoch [97/100] | Loss: 0.1053869799733162
Epoch [98/100] | Loss: 0.1060175249983867
Epoch [99/100] | Loss: 0.10516799224118392
Epoch [100/100] | Loss: 0.10356067033906778</code></pre>
</div>
</div>
</section>
<section id="sampling" class="level2">
<h2 class="anchored" data-anchor-id="sampling">Sampling</h2>
<div id="cell-34" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>num_grid_rows <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">16</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-35" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> LinearNoiseScheduler(T, beta_start, beta_end)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Unet(im_channels <span class="op">=</span> z_channels).to(device)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>model.load_state_dict(torch.load(<span class="st">"../ldmCeleb/denoiseLatentModelCeleb.pth"</span>, map_location <span class="op">=</span> device))</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>vae <span class="op">=</span> VQVAE().to(device)</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>vae.<span class="bu">eval</span>()</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>vae.load_state_dict(torch.load(<span class="st">"../vqvaeCeleb/vqvae_autoencoder.pth"</span>, map_location<span class="op">=</span>device), strict<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    im_size <span class="op">=</span> image_size <span class="op">//</span> (<span class="dv">2</span> <span class="op">**</span> (<span class="bu">sum</span>(model.down_sample)))</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    xt <span class="op">=</span> torch.randn((num_samples, z_channels, im_size, im_size)).to(device)</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(T)):</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        noise_pred <span class="op">=</span> model(xt, torch.as_tensor(t).unsqueeze(<span class="dv">0</span>).to(device))</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        xt, x0_pred <span class="op">=</span> scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(t).to(device))</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        ims_raw <span class="op">=</span> torch.clamp(xt, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>).detach().cpu()</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        ims_raw <span class="op">=</span> (ims_raw <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> vae.decode(xt)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> torch.clamp(ims, <span class="op">-</span><span class="fl">1.</span>, <span class="fl">1.</span>).detach().cpu()</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        ims <span class="op">=</span> (ims <span class="op">+</span> <span class="dv">1</span>) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>        grid_latent <span class="op">=</span> vutils.make_grid(ims_raw, nrow <span class="op">=</span> num_grid_rows, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>        grid_reconstructed <span class="op">=</span> vutils.make_grid(ims, nrow <span class="op">=</span> num_grid_rows, normalize <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (t <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> t <span class="op">==</span> T <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>            plt.figure(figsize <span class="op">=</span> (<span class="dv">15</span>, <span class="dv">15</span>))</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">"off"</span>)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>            plt.imshow(np.transpose(grid_latent.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>            plt.subplot(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>            plt.axis(<span class="st">"off"</span>)</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>            plt.imshow(np.transpose(grid_reconstructed.cpu().detach().numpy(), (<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>)))</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>        img_latent <span class="op">=</span> transforms.ToPILImage()(grid_latent)</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>        img_decode <span class="op">=</span> transforms.ToPILImage()(grid_reconstructed)</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">"./ldm/CelebLatent"</span>):</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>            os.makedirs(<span class="st">"./ldm/CelebLatent"</span>)</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> os.path.exists(<span class="st">"../ldm/CelebDecode"</span>):</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>            os.makedirs(<span class="st">"../ldm/CelebDecode"</span>)</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>        img_latent.save(<span class="ss">f"../ldm/CelebLatent/x0_</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>        img_latent.close()</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>        img_decode.save(<span class="ss">f"../ldm/CelebDecode/x0_</span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">.png"</span>)</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>        img_decode.close()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-4.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-6.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-7.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-9.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-10.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="LatentDiff_files/figure-html/cell-19-output-11.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>Website made with <a href="https://quarto.org/">Quarto</a>, by Guntas Singh Saran.</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/guntas-13/">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.instagram.com/guntas.saran13/">
      <i class="bi bi-instagram" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/guntas-singh-saran-2b8811179/">
      <i class="bi bi-linkedin" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="mailto:guntassingh.saran@iitgn.ac.in">
      <i class="bi bi-envelope" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




<script src="../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>